{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1cb77-5344-418c-94b3-7bacb800c746",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 6\n",
    "\n",
    "## Preamble: Data\n",
    "In this lab we are using a dataset on beer purchases. Our goal is to predict if light beer purchased in the US is BUD light. To achieve this goal, we will use the information provided by the following socioeconomic characteristics:\n",
    "* market           - where the beer is bought\n",
    "* buyertype        - who is the buyer () \n",
    "* income           - ranges of income\n",
    "* childrenUnder6   - does the buyer have children under 6 years old\n",
    "* children6to17    - does the buyer have children between 6 and 17\n",
    "* age              - bracketed age groups\n",
    "* employment       - fully employed, partially employed, no employment.\n",
    "* degree           - level of occupation\n",
    "* occuptation      - which sector you are employed in\n",
    "* ethnic           - white, asian, hispanic, black or other\n",
    "* microwave        - own a microwave\n",
    "* dishwasher       - own a dishwasher\n",
    "* tvcable          - what type cable tv subscription you have\n",
    "* singlefamilyhome - are you in a single family home\n",
    "* npeople          - number of people you live with 1,2,3,4, +5\n",
    "\n",
    "First, we load the dataset and create an output variable that indicates purchases of Bud Light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620cb558-0655-4ff6-80d8-51cc89e3c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73115, 24)\n",
      "(73115, 123)\n",
      "(73115,)\n",
      "   household   upc_description  quantity beer_brand  beer_spend  beer_floz  \\\n",
      "0    2000946  BUD LT BR CN 12P         1  BUD LIGHT        8.14      144.0   \n",
      "1    2003036  BUD LT BR CN 24P         1  BUD LIGHT       17.48      288.0   \n",
      "2    2003036  BUD LT BR CN 24P         2  BUD LIGHT       33.92      576.0   \n",
      "3    2003036  BUD LT BR CN 30P         2  BUD LIGHT       34.74      720.0   \n",
      "4    2003036  BUD LT BR CN 36P         2  BUD LIGHT       42.96      864.0   \n",
      "\n",
      "   price_floz container_descr  promotion          market  ...  age employment  \\\n",
      "0    0.056528             CAN      False  RURAL ILLINOIS  ...  50+       none   \n",
      "1    0.060694             CAN      False         ATLANTA  ...  50+       full   \n",
      "2    0.058889             CAN      False         ATLANTA  ...  50+       full   \n",
      "3    0.048250             CAN      False         ATLANTA  ...  50+       full   \n",
      "4    0.049722             CAN      False         ATLANTA  ...  50+       full   \n",
      "\n",
      "    degree              occupation ethnic microwave dishwasher  tvcable  \\\n",
      "0     Grad    none/retired/student  white      True       True  premium   \n",
      "1  College  clerical/sales/service  white      True       True    basic   \n",
      "2  College  clerical/sales/service  white      True       True    basic   \n",
      "3  College  clerical/sales/service  white      True       True    basic   \n",
      "4  College  clerical/sales/service  white      True       True    basic   \n",
      "\n",
      "  singlefamilyhome  npeople  \n",
      "0            False        1  \n",
      "1             True        2  \n",
      "2             True        2  \n",
      "3             True        2  \n",
      "4             True        2  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "       household       upc_description  quantity     beer_brand  beer_spend  \\\n",
      "73110    9062536  NATURAL LT BR CN 30P         1  NATURAL LIGHT       16.59   \n",
      "73111    9062536  NATURAL LT BR CN 30P         1  NATURAL LIGHT       16.59   \n",
      "73112    9103985  NATURAL LT BR CN 30P         1  NATURAL LIGHT       13.88   \n",
      "73113    9158319  NATURAL LT BR CN 24P         1  NATURAL LIGHT        9.98   \n",
      "73114    9164033  NATURAL LT BR CN 30P         1  NATURAL LIGHT       12.98   \n",
      "\n",
      "       beer_floz  price_floz container_descr  promotion          market  ...  \\\n",
      "73110      360.0    0.046083             CAN      False           TAMPA  ...   \n",
      "73111      360.0    0.046083             CAN      False           TAMPA  ...   \n",
      "73112      360.0    0.038556             CAN      False         DETROIT  ...   \n",
      "73113      288.0    0.034653             CAN      False  RURAL VIRGINIA  ...   \n",
      "73114      360.0    0.036056             CAN      False      SACRAMENTO  ...   \n",
      "\n",
      "       age employment   degree              occupation    ethnic microwave  \\\n",
      "73110  50+       part       HS  clerical/sales/service     white      True   \n",
      "73111  50+       part       HS  clerical/sales/service     white      True   \n",
      "73112  50+       full       HS                    prof     white      True   \n",
      "73113  50+       none       HS    none/retired/student     black      True   \n",
      "73114  50+       full  College                    prof  hispanic      True   \n",
      "\n",
      "      dishwasher  tvcable singlefamilyhome  npeople  \n",
      "73110      False     none             True        2  \n",
      "73111      False     none             True        2  \n",
      "73112       True     none             True        2  \n",
      "73113       True  premium             True        2  \n",
      "73114       True    basic             True    5plus  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "np.random.seed(2023)\n",
    "tf.random.set_seed(2023)\n",
    "random.seed(2023)\n",
    "\n",
    "\n",
    "# os.chdir(\"??\") Change working directory if needed \n",
    "\n",
    "lb    = pd.read_csv(\"LightBeer2.csv\")\n",
    "y     = np.zeros(shape=lb.shape[0])\n",
    "y[lb['beer_brand'] == \"BUD LIGHT\"]     = 1\n",
    "demog = lb.iloc[:,9:]\n",
    "demog = pd.get_dummies(demog, drop_first=True)\n",
    "\n",
    "# inspect the data\n",
    "print(lb.shape)\n",
    "print(demog.shape)\n",
    "print(y.shape)\n",
    "# inspect first 5 and last 5\n",
    "print(lb.head())\n",
    "print(lb.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174fd7-d779-4207-a30e-224c89bdcbc9",
   "metadata": {},
   "source": [
    "We also split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5c287b-7069-426c-b3c1-15857ff123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(demog, y, train_size=0.75, shuffle=False)\n",
    "\n",
    "stdz_X = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = stdz_X.transform(X_train)\n",
    "X_test  = stdz_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983d48-57e5-4342-9935-16676fd3a106",
   "metadata": {},
   "source": [
    "## Part 1: Specifying and training neural networks\n",
    "We will now start building a neural network to predict the purchase of Bud Light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7cab-178e-4fee-b942-8ab68a140e23",
   "metadata": {},
   "source": [
    "### Task 1a) \n",
    "We start with specifying the architecture of our very first and very small neural net `model1`.\n",
    "Add three layers to `model1`, two hidden layers with $30$ and $15$ hidden units, respectively, and an output layer.\n",
    "For the two hidden layers you should use the ReLU activation function. Additionally, choose a suitable activation for the output layer, given that we have a classification problem. See [the Keras documentation](https://keras.io/api/layers/activations/) for activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d9388d-cbd6-4d10-90ff-2c3d974a8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize a first model\n",
    "with tf.device('/cpu:0'):\n",
    "    model1 = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "    model1.add(layers.Dense(30, input_dim=X_train.shape[1], activation='ReLU'))\n",
    "    model1.add(layers.Dense(15, activation = 'ReLU'))\n",
    "    model1.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd214-22bf-42fa-a973-2c27f0643431",
   "metadata": {},
   "source": [
    "### Task 1b) \n",
    "\n",
    "Next, we compile our model specification. From [https://keras.io/api/losses/probabilistic_losses/](losses) select a suitable loss function for our classification problem. As optimization algorithm use *Adam* with learning rate $0.00003$. Lastly, use `accuracy` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a88ddd5-3e85-4238-96a3-81b914f4fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2904463-9a53-4ea3-b175-adfc9ff0f599",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "Now train the model using $250$ epochs, a batch_size of $2^8$, and use $25\\%$ of the data for validation.\n",
    "Use the string variable `loss_valloss_difference_1c` to describe and explain the observed difference between validation loss and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f00f52b-92c7-4100-bfe0-05cdd6f0fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7756 - accuracy: 0.4681 - val_loss: 0.8387 - val_accuracy: 0.3946\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7330 - accuracy: 0.5154 - val_loss: 0.7501 - val_accuracy: 0.4872\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7061 - accuracy: 0.5498 - val_loss: 0.6868 - val_accuracy: 0.5748\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6888 - accuracy: 0.5680 - val_loss: 0.6428 - val_accuracy: 0.6204\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 961us/step - loss: 0.6769 - accuracy: 0.5858 - val_loss: 0.6139 - val_accuracy: 0.6582\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 951us/step - loss: 0.6679 - accuracy: 0.5995 - val_loss: 0.5943 - val_accuracy: 0.6874\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 961us/step - loss: 0.6604 - accuracy: 0.6117 - val_loss: 0.5814 - val_accuracy: 0.7081\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 982us/step - loss: 0.6540 - accuracy: 0.6195 - val_loss: 0.5721 - val_accuracy: 0.7199\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6483 - accuracy: 0.6227 - val_loss: 0.5657 - val_accuracy: 0.7270\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6432 - accuracy: 0.6323 - val_loss: 0.5610 - val_accuracy: 0.7473\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6385 - accuracy: 0.6408 - val_loss: 0.5584 - val_accuracy: 0.7542\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 958us/step - loss: 0.6342 - accuracy: 0.6452 - val_loss: 0.5544 - val_accuracy: 0.7585\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 959us/step - loss: 0.6302 - accuracy: 0.6484 - val_loss: 0.5525 - val_accuracy: 0.7621\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 957us/step - loss: 0.6266 - accuracy: 0.6517 - val_loss: 0.5506 - val_accuracy: 0.7676\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 955us/step - loss: 0.6232 - accuracy: 0.6557 - val_loss: 0.5497 - val_accuracy: 0.7677\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.6199 - accuracy: 0.6591 - val_loss: 0.5480 - val_accuracy: 0.7637\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 960us/step - loss: 0.6168 - accuracy: 0.6623 - val_loss: 0.5475 - val_accuracy: 0.7664\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.6140 - accuracy: 0.6655 - val_loss: 0.5463 - val_accuracy: 0.7680\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.6112 - accuracy: 0.6680 - val_loss: 0.5460 - val_accuracy: 0.7799\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.6086 - accuracy: 0.6693 - val_loss: 0.5463 - val_accuracy: 0.7850\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.6062 - accuracy: 0.6713 - val_loss: 0.5469 - val_accuracy: 0.7784\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.6038 - accuracy: 0.6743 - val_loss: 0.5466 - val_accuracy: 0.7748\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.6014 - accuracy: 0.6785 - val_loss: 0.5468 - val_accuracy: 0.7781\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 965us/step - loss: 0.5992 - accuracy: 0.6805 - val_loss: 0.5466 - val_accuracy: 0.7797\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5970 - accuracy: 0.6818 - val_loss: 0.5463 - val_accuracy: 0.7790\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.5950 - accuracy: 0.6818 - val_loss: 0.5472 - val_accuracy: 0.7782\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.5930 - accuracy: 0.6836 - val_loss: 0.5459 - val_accuracy: 0.7774\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.5911 - accuracy: 0.6840 - val_loss: 0.5470 - val_accuracy: 0.7768\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.5892 - accuracy: 0.6895 - val_loss: 0.5479 - val_accuracy: 0.7738\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.5874 - accuracy: 0.6934 - val_loss: 0.5458 - val_accuracy: 0.7745\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.5857 - accuracy: 0.6946 - val_loss: 0.5463 - val_accuracy: 0.7709\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.5840 - accuracy: 0.6955 - val_loss: 0.5482 - val_accuracy: 0.7689\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.5823 - accuracy: 0.6971 - val_loss: 0.5478 - val_accuracy: 0.7702\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.5807 - accuracy: 0.6985 - val_loss: 0.5494 - val_accuracy: 0.7676\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.5791 - accuracy: 0.7010 - val_loss: 0.5496 - val_accuracy: 0.7659\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.5776 - accuracy: 0.7013 - val_loss: 0.5494 - val_accuracy: 0.7656\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.5761 - accuracy: 0.7024 - val_loss: 0.5480 - val_accuracy: 0.7621\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5746 - accuracy: 0.7033 - val_loss: 0.5493 - val_accuracy: 0.7591\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.5732 - accuracy: 0.7045 - val_loss: 0.5502 - val_accuracy: 0.7501\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.5718 - accuracy: 0.7044 - val_loss: 0.5512 - val_accuracy: 0.7508\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.5704 - accuracy: 0.7063 - val_loss: 0.5515 - val_accuracy: 0.7510\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.5691 - accuracy: 0.7086 - val_loss: 0.5509 - val_accuracy: 0.7507\n",
      "Epoch 43/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.5677 - accuracy: 0.7098 - val_loss: 0.5519 - val_accuracy: 0.7516\n",
      "Epoch 44/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.5664 - accuracy: 0.7106 - val_loss: 0.5518 - val_accuracy: 0.7520\n",
      "Epoch 45/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.5651 - accuracy: 0.7119 - val_loss: 0.5514 - val_accuracy: 0.7528\n",
      "Epoch 46/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.5639 - accuracy: 0.7121 - val_loss: 0.5550 - val_accuracy: 0.7470\n",
      "Epoch 47/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.5626 - accuracy: 0.7148 - val_loss: 0.5543 - val_accuracy: 0.7489\n",
      "Epoch 48/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.5614 - accuracy: 0.7163 - val_loss: 0.5531 - val_accuracy: 0.7622\n",
      "Epoch 49/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.5603 - accuracy: 0.7167 - val_loss: 0.5538 - val_accuracy: 0.7581\n",
      "Epoch 50/250\n",
      "161/161 [==============================] - 0s 924us/step - loss: 0.5591 - accuracy: 0.7183 - val_loss: 0.5565 - val_accuracy: 0.7529\n",
      "Epoch 51/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7236 - val_loss: 0.5547 - val_accuracy: 0.7529\n",
      "Epoch 52/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.5568 - accuracy: 0.7252 - val_loss: 0.5574 - val_accuracy: 0.7489\n",
      "Epoch 53/250\n",
      "161/161 [==============================] - 0s 952us/step - loss: 0.5557 - accuracy: 0.7256 - val_loss: 0.5591 - val_accuracy: 0.7482\n",
      "Epoch 54/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5546 - accuracy: 0.7274 - val_loss: 0.5570 - val_accuracy: 0.7491\n",
      "Epoch 55/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.7282 - val_loss: 0.5584 - val_accuracy: 0.7476\n",
      "Epoch 56/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5525 - accuracy: 0.7300 - val_loss: 0.5565 - val_accuracy: 0.7518\n",
      "Epoch 57/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.7295 - val_loss: 0.5580 - val_accuracy: 0.7511\n",
      "Epoch 58/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.7301 - val_loss: 0.5591 - val_accuracy: 0.7571\n",
      "Epoch 59/250\n",
      "161/161 [==============================] - 0s 982us/step - loss: 0.5493 - accuracy: 0.7315 - val_loss: 0.5590 - val_accuracy: 0.7555\n",
      "Epoch 60/250\n",
      "161/161 [==============================] - 0s 995us/step - loss: 0.5482 - accuracy: 0.7324 - val_loss: 0.5601 - val_accuracy: 0.7493\n",
      "Epoch 61/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.7327 - val_loss: 0.5597 - val_accuracy: 0.7563\n",
      "Epoch 62/250\n",
      "161/161 [==============================] - 0s 976us/step - loss: 0.5461 - accuracy: 0.7325 - val_loss: 0.5607 - val_accuracy: 0.7542\n",
      "Epoch 63/250\n",
      "161/161 [==============================] - 0s 953us/step - loss: 0.5450 - accuracy: 0.7337 - val_loss: 0.5597 - val_accuracy: 0.7542\n",
      "Epoch 64/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7340 - val_loss: 0.5611 - val_accuracy: 0.7524\n",
      "Epoch 65/250\n",
      "161/161 [==============================] - 0s 975us/step - loss: 0.5430 - accuracy: 0.7340 - val_loss: 0.5599 - val_accuracy: 0.7507\n",
      "Epoch 66/250\n",
      "161/161 [==============================] - 0s 963us/step - loss: 0.5420 - accuracy: 0.7349 - val_loss: 0.5641 - val_accuracy: 0.7497\n",
      "Epoch 67/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.5410 - accuracy: 0.7352 - val_loss: 0.5626 - val_accuracy: 0.7497\n",
      "Epoch 68/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.5400 - accuracy: 0.7362 - val_loss: 0.5631 - val_accuracy: 0.7505\n",
      "Epoch 69/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.5390 - accuracy: 0.7361 - val_loss: 0.5651 - val_accuracy: 0.7488\n",
      "Epoch 70/250\n",
      "161/161 [==============================] - 0s 951us/step - loss: 0.5380 - accuracy: 0.7368 - val_loss: 0.5640 - val_accuracy: 0.7441\n",
      "Epoch 71/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.5370 - accuracy: 0.7374 - val_loss: 0.5653 - val_accuracy: 0.7432\n",
      "Epoch 72/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.5361 - accuracy: 0.7374 - val_loss: 0.5639 - val_accuracy: 0.7434\n",
      "Epoch 73/250\n",
      "161/161 [==============================] - 0s 951us/step - loss: 0.5351 - accuracy: 0.7378 - val_loss: 0.5647 - val_accuracy: 0.7425\n",
      "Epoch 74/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5341 - accuracy: 0.7386 - val_loss: 0.5672 - val_accuracy: 0.7400\n",
      "Epoch 75/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7399 - val_loss: 0.5666 - val_accuracy: 0.7397\n",
      "Epoch 76/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.7402 - val_loss: 0.5668 - val_accuracy: 0.7394\n",
      "Epoch 77/250\n",
      "161/161 [==============================] - 0s 996us/step - loss: 0.5313 - accuracy: 0.7406 - val_loss: 0.5680 - val_accuracy: 0.7348\n",
      "Epoch 78/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.5304 - accuracy: 0.7412 - val_loss: 0.5675 - val_accuracy: 0.7349\n",
      "Epoch 79/250\n",
      "161/161 [==============================] - 0s 981us/step - loss: 0.5294 - accuracy: 0.7416 - val_loss: 0.5696 - val_accuracy: 0.7346\n",
      "Epoch 80/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7419 - val_loss: 0.5707 - val_accuracy: 0.7324\n",
      "Epoch 81/250\n",
      "161/161 [==============================] - 0s 972us/step - loss: 0.5276 - accuracy: 0.7421 - val_loss: 0.5681 - val_accuracy: 0.7312\n",
      "Epoch 82/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7422 - val_loss: 0.5705 - val_accuracy: 0.7280\n",
      "Epoch 83/250\n",
      "161/161 [==============================] - 0s 960us/step - loss: 0.5258 - accuracy: 0.7433 - val_loss: 0.5694 - val_accuracy: 0.7281\n",
      "Epoch 84/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.5249 - accuracy: 0.7447 - val_loss: 0.5741 - val_accuracy: 0.7259\n",
      "Epoch 85/250\n",
      "161/161 [==============================] - 0s 965us/step - loss: 0.5241 - accuracy: 0.7450 - val_loss: 0.5717 - val_accuracy: 0.7275\n",
      "Epoch 86/250\n",
      "161/161 [==============================] - 0s 991us/step - loss: 0.5232 - accuracy: 0.7451 - val_loss: 0.5737 - val_accuracy: 0.7250\n",
      "Epoch 87/250\n",
      "161/161 [==============================] - 0s 988us/step - loss: 0.5223 - accuracy: 0.7455 - val_loss: 0.5739 - val_accuracy: 0.7248\n",
      "Epoch 88/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7464 - val_loss: 0.5745 - val_accuracy: 0.7246\n",
      "Epoch 89/250\n",
      "161/161 [==============================] - 0s 1000us/step - loss: 0.5206 - accuracy: 0.7464 - val_loss: 0.5756 - val_accuracy: 0.7228\n",
      "Epoch 90/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7471 - val_loss: 0.5769 - val_accuracy: 0.7227\n",
      "Epoch 91/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.7482 - val_loss: 0.5711 - val_accuracy: 0.7298\n",
      "Epoch 92/250\n",
      "161/161 [==============================] - 0s 984us/step - loss: 0.5180 - accuracy: 0.7491 - val_loss: 0.5758 - val_accuracy: 0.7263\n",
      "Epoch 93/250\n",
      "161/161 [==============================] - 0s 994us/step - loss: 0.5172 - accuracy: 0.7487 - val_loss: 0.5758 - val_accuracy: 0.7242\n",
      "Epoch 94/250\n",
      "161/161 [==============================] - 0s 998us/step - loss: 0.5163 - accuracy: 0.7498 - val_loss: 0.5758 - val_accuracy: 0.7260\n",
      "Epoch 95/250\n",
      "161/161 [==============================] - 0s 999us/step - loss: 0.5154 - accuracy: 0.7499 - val_loss: 0.5764 - val_accuracy: 0.7261\n",
      "Epoch 96/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5146 - accuracy: 0.7507 - val_loss: 0.5787 - val_accuracy: 0.7240\n",
      "Epoch 97/250\n",
      "161/161 [==============================] - 0s 983us/step - loss: 0.5138 - accuracy: 0.7508 - val_loss: 0.5771 - val_accuracy: 0.7243\n",
      "Epoch 98/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7518 - val_loss: 0.5791 - val_accuracy: 0.7227\n",
      "Epoch 99/250\n",
      "161/161 [==============================] - 0s 976us/step - loss: 0.5121 - accuracy: 0.7535 - val_loss: 0.5768 - val_accuracy: 0.7246\n",
      "Epoch 100/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.5113 - accuracy: 0.7543 - val_loss: 0.5773 - val_accuracy: 0.7236\n",
      "Epoch 101/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.5105 - accuracy: 0.7555 - val_loss: 0.5787 - val_accuracy: 0.7232\n",
      "Epoch 102/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.5096 - accuracy: 0.7565 - val_loss: 0.5792 - val_accuracy: 0.7230\n",
      "Epoch 103/250\n",
      "161/161 [==============================] - 0s 946us/step - loss: 0.5088 - accuracy: 0.7572 - val_loss: 0.5802 - val_accuracy: 0.7239\n",
      "Epoch 104/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.5080 - accuracy: 0.7589 - val_loss: 0.5799 - val_accuracy: 0.7215\n",
      "Epoch 105/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.5072 - accuracy: 0.7590 - val_loss: 0.5824 - val_accuracy: 0.7211\n",
      "Epoch 106/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.5064 - accuracy: 0.7596 - val_loss: 0.5835 - val_accuracy: 0.7203\n",
      "Epoch 107/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.5056 - accuracy: 0.7607 - val_loss: 0.5832 - val_accuracy: 0.7219\n",
      "Epoch 108/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.5048 - accuracy: 0.7612 - val_loss: 0.5843 - val_accuracy: 0.7236\n",
      "Epoch 109/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.5040 - accuracy: 0.7613 - val_loss: 0.5838 - val_accuracy: 0.7237\n",
      "Epoch 110/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7620 - val_loss: 0.5834 - val_accuracy: 0.7211\n",
      "Epoch 111/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.5024 - accuracy: 0.7621 - val_loss: 0.5863 - val_accuracy: 0.7205\n",
      "Epoch 112/250\n",
      "161/161 [==============================] - 0s 953us/step - loss: 0.5016 - accuracy: 0.7623 - val_loss: 0.5853 - val_accuracy: 0.7205\n",
      "Epoch 113/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.5008 - accuracy: 0.7638 - val_loss: 0.5843 - val_accuracy: 0.7208\n",
      "Epoch 114/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.5001 - accuracy: 0.7647 - val_loss: 0.5857 - val_accuracy: 0.7135\n",
      "Epoch 115/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.4993 - accuracy: 0.7650 - val_loss: 0.5894 - val_accuracy: 0.7095\n",
      "Epoch 116/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.4986 - accuracy: 0.7675 - val_loss: 0.5873 - val_accuracy: 0.7124\n",
      "Epoch 117/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.4978 - accuracy: 0.7685 - val_loss: 0.5878 - val_accuracy: 0.7076\n",
      "Epoch 118/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.4970 - accuracy: 0.7696 - val_loss: 0.5870 - val_accuracy: 0.7081\n",
      "Epoch 119/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.4963 - accuracy: 0.7704 - val_loss: 0.5880 - val_accuracy: 0.7066\n",
      "Epoch 120/250\n",
      "161/161 [==============================] - 0s 952us/step - loss: 0.4955 - accuracy: 0.7711 - val_loss: 0.5894 - val_accuracy: 0.7006\n",
      "Epoch 121/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7717 - val_loss: 0.5893 - val_accuracy: 0.7080\n",
      "Epoch 122/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.4940 - accuracy: 0.7725 - val_loss: 0.5914 - val_accuracy: 0.7006\n",
      "Epoch 123/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.4933 - accuracy: 0.7732 - val_loss: 0.5877 - val_accuracy: 0.7040\n",
      "Epoch 124/250\n",
      "161/161 [==============================] - 0s 957us/step - loss: 0.4925 - accuracy: 0.7734 - val_loss: 0.5921 - val_accuracy: 0.6995\n",
      "Epoch 125/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.4918 - accuracy: 0.7742 - val_loss: 0.5926 - val_accuracy: 0.6995\n",
      "Epoch 126/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.4911 - accuracy: 0.7747 - val_loss: 0.5953 - val_accuracy: 0.6979\n",
      "Epoch 127/250\n",
      "161/161 [==============================] - 0s 953us/step - loss: 0.4903 - accuracy: 0.7751 - val_loss: 0.5939 - val_accuracy: 0.6982\n",
      "Epoch 128/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.4896 - accuracy: 0.7748 - val_loss: 0.5960 - val_accuracy: 0.6976\n",
      "Epoch 129/250\n",
      "161/161 [==============================] - 0s 930us/step - loss: 0.4889 - accuracy: 0.7760 - val_loss: 0.5939 - val_accuracy: 0.6982\n",
      "Epoch 130/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.4882 - accuracy: 0.7765 - val_loss: 0.5943 - val_accuracy: 0.6980\n",
      "Epoch 131/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.4875 - accuracy: 0.7771 - val_loss: 0.5963 - val_accuracy: 0.6982\n",
      "Epoch 132/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7772 - val_loss: 0.5968 - val_accuracy: 0.6982\n",
      "Epoch 133/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.4861 - accuracy: 0.7775 - val_loss: 0.5962 - val_accuracy: 0.6983\n",
      "Epoch 134/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.4855 - accuracy: 0.7785 - val_loss: 0.5976 - val_accuracy: 0.6982\n",
      "Epoch 135/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.4848 - accuracy: 0.7789 - val_loss: 0.5974 - val_accuracy: 0.6965\n",
      "Epoch 136/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.4841 - accuracy: 0.7789 - val_loss: 0.5994 - val_accuracy: 0.6965\n",
      "Epoch 137/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.4834 - accuracy: 0.7790 - val_loss: 0.6014 - val_accuracy: 0.6963\n",
      "Epoch 138/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.4827 - accuracy: 0.7804 - val_loss: 0.5995 - val_accuracy: 0.6979\n",
      "Epoch 139/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.4820 - accuracy: 0.7806 - val_loss: 0.6006 - val_accuracy: 0.7024\n",
      "Epoch 140/250\n",
      "161/161 [==============================] - 0s 946us/step - loss: 0.4814 - accuracy: 0.7806 - val_loss: 0.5992 - val_accuracy: 0.7013\n",
      "Epoch 141/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.4807 - accuracy: 0.7810 - val_loss: 0.6001 - val_accuracy: 0.7006\n",
      "Epoch 142/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.4800 - accuracy: 0.7813 - val_loss: 0.6009 - val_accuracy: 0.7001\n",
      "Epoch 143/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7811 - val_loss: 0.6017 - val_accuracy: 0.7009\n",
      "Epoch 144/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.4787 - accuracy: 0.7816 - val_loss: 0.6012 - val_accuracy: 0.6957\n",
      "Epoch 145/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4780 - accuracy: 0.7822 - val_loss: 0.6044 - val_accuracy: 0.6946\n",
      "Epoch 146/250\n",
      "161/161 [==============================] - 0s 974us/step - loss: 0.4774 - accuracy: 0.7832 - val_loss: 0.6036 - val_accuracy: 0.6934\n",
      "Epoch 147/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.4767 - accuracy: 0.7828 - val_loss: 0.6050 - val_accuracy: 0.6928\n",
      "Epoch 148/250\n",
      "161/161 [==============================] - 0s 968us/step - loss: 0.4760 - accuracy: 0.7833 - val_loss: 0.6070 - val_accuracy: 0.6925\n",
      "Epoch 149/250\n",
      "161/161 [==============================] - 0s 956us/step - loss: 0.4754 - accuracy: 0.7832 - val_loss: 0.6065 - val_accuracy: 0.6922\n",
      "Epoch 150/250\n",
      "161/161 [==============================] - 0s 951us/step - loss: 0.4748 - accuracy: 0.7840 - val_loss: 0.6105 - val_accuracy: 0.6895\n",
      "Epoch 151/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.4741 - accuracy: 0.7842 - val_loss: 0.6082 - val_accuracy: 0.6920\n",
      "Epoch 152/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.4735 - accuracy: 0.7848 - val_loss: 0.6066 - val_accuracy: 0.6934\n",
      "Epoch 153/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.7847 - val_loss: 0.6079 - val_accuracy: 0.6921\n",
      "Epoch 154/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.4722 - accuracy: 0.7853 - val_loss: 0.6085 - val_accuracy: 0.6925\n",
      "Epoch 155/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.4716 - accuracy: 0.7859 - val_loss: 0.6112 - val_accuracy: 0.6906\n",
      "Epoch 156/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.4710 - accuracy: 0.7857 - val_loss: 0.6144 - val_accuracy: 0.6886\n",
      "Epoch 157/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.4704 - accuracy: 0.7857 - val_loss: 0.6116 - val_accuracy: 0.6899\n",
      "Epoch 158/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.4698 - accuracy: 0.7858 - val_loss: 0.6122 - val_accuracy: 0.6900\n",
      "Epoch 159/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.4692 - accuracy: 0.7862 - val_loss: 0.6141 - val_accuracy: 0.6902\n",
      "Epoch 160/250\n",
      "161/161 [==============================] - 0s 916us/step - loss: 0.4686 - accuracy: 0.7859 - val_loss: 0.6127 - val_accuracy: 0.6898\n",
      "Epoch 161/250\n",
      "161/161 [==============================] - 0s 919us/step - loss: 0.4680 - accuracy: 0.7863 - val_loss: 0.6109 - val_accuracy: 0.6903\n",
      "Epoch 162/250\n",
      "161/161 [==============================] - 0s 912us/step - loss: 0.4674 - accuracy: 0.7870 - val_loss: 0.6123 - val_accuracy: 0.6905\n",
      "Epoch 163/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.4669 - accuracy: 0.7875 - val_loss: 0.6126 - val_accuracy: 0.6892\n",
      "Epoch 164/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.4663 - accuracy: 0.7872 - val_loss: 0.6120 - val_accuracy: 0.6914\n",
      "Epoch 165/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7873 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 166/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.4652 - accuracy: 0.7883 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 167/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.4646 - accuracy: 0.7880 - val_loss: 0.6155 - val_accuracy: 0.6887\n",
      "Epoch 168/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7879 - val_loss: 0.6168 - val_accuracy: 0.6895\n",
      "Epoch 169/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.7888 - val_loss: 0.6184 - val_accuracy: 0.6897\n",
      "Epoch 170/250\n",
      "161/161 [==============================] - 0s 971us/step - loss: 0.4629 - accuracy: 0.7903 - val_loss: 0.6205 - val_accuracy: 0.6889\n",
      "Epoch 171/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.4624 - accuracy: 0.7899 - val_loss: 0.6178 - val_accuracy: 0.6916\n",
      "Epoch 172/250\n",
      "161/161 [==============================] - 0s 925us/step - loss: 0.4618 - accuracy: 0.7904 - val_loss: 0.6195 - val_accuracy: 0.6893\n",
      "Epoch 173/250\n",
      "161/161 [==============================] - 0s 920us/step - loss: 0.4612 - accuracy: 0.7908 - val_loss: 0.6217 - val_accuracy: 0.6904\n",
      "Epoch 174/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.4607 - accuracy: 0.7911 - val_loss: 0.6218 - val_accuracy: 0.6903\n",
      "Epoch 175/250\n",
      "161/161 [==============================] - 0s 918us/step - loss: 0.4601 - accuracy: 0.7916 - val_loss: 0.6204 - val_accuracy: 0.6913\n",
      "Epoch 176/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7916 - val_loss: 0.6244 - val_accuracy: 0.6897\n",
      "Epoch 177/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.4590 - accuracy: 0.7928 - val_loss: 0.6219 - val_accuracy: 0.6904\n",
      "Epoch 178/250\n",
      "161/161 [==============================] - 0s 925us/step - loss: 0.4585 - accuracy: 0.7927 - val_loss: 0.6233 - val_accuracy: 0.6909\n",
      "Epoch 179/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.4579 - accuracy: 0.7929 - val_loss: 0.6218 - val_accuracy: 0.6908\n",
      "Epoch 180/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.4574 - accuracy: 0.7929 - val_loss: 0.6274 - val_accuracy: 0.6874\n",
      "Epoch 181/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.4568 - accuracy: 0.7931 - val_loss: 0.6252 - val_accuracy: 0.6913\n",
      "Epoch 182/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.4563 - accuracy: 0.7935 - val_loss: 0.6225 - val_accuracy: 0.6921\n",
      "Epoch 183/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.4558 - accuracy: 0.7936 - val_loss: 0.6261 - val_accuracy: 0.6902\n",
      "Epoch 184/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.4553 - accuracy: 0.7935 - val_loss: 0.6274 - val_accuracy: 0.6904\n",
      "Epoch 185/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.4548 - accuracy: 0.7941 - val_loss: 0.6278 - val_accuracy: 0.6900\n",
      "Epoch 186/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.7940 - val_loss: 0.6304 - val_accuracy: 0.6845\n",
      "Epoch 187/250\n",
      "161/161 [==============================] - 0s 918us/step - loss: 0.4537 - accuracy: 0.7944 - val_loss: 0.6304 - val_accuracy: 0.6836\n",
      "Epoch 188/250\n",
      "161/161 [==============================] - 0s 907us/step - loss: 0.4532 - accuracy: 0.7948 - val_loss: 0.6260 - val_accuracy: 0.6875\n",
      "Epoch 189/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.4527 - accuracy: 0.7952 - val_loss: 0.6292 - val_accuracy: 0.6836\n",
      "Epoch 190/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.4522 - accuracy: 0.7957 - val_loss: 0.6284 - val_accuracy: 0.6836\n",
      "Epoch 191/250\n",
      "161/161 [==============================] - 0s 932us/step - loss: 0.4516 - accuracy: 0.7966 - val_loss: 0.6251 - val_accuracy: 0.6882\n",
      "Epoch 192/250\n",
      "161/161 [==============================] - 0s 919us/step - loss: 0.4511 - accuracy: 0.7962 - val_loss: 0.6290 - val_accuracy: 0.6847\n",
      "Epoch 193/250\n",
      "161/161 [==============================] - 0s 920us/step - loss: 0.4506 - accuracy: 0.7973 - val_loss: 0.6308 - val_accuracy: 0.6845\n",
      "Epoch 194/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.4501 - accuracy: 0.7971 - val_loss: 0.6319 - val_accuracy: 0.6834\n",
      "Epoch 195/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.4496 - accuracy: 0.7974 - val_loss: 0.6327 - val_accuracy: 0.6835\n",
      "Epoch 196/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7981 - val_loss: 0.6310 - val_accuracy: 0.6850\n",
      "Epoch 197/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.4486 - accuracy: 0.7985 - val_loss: 0.6320 - val_accuracy: 0.6848\n",
      "Epoch 198/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.4481 - accuracy: 0.7983 - val_loss: 0.6344 - val_accuracy: 0.6835\n",
      "Epoch 199/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.4476 - accuracy: 0.7991 - val_loss: 0.6321 - val_accuracy: 0.6848\n",
      "Epoch 200/250\n",
      "161/161 [==============================] - 0s 912us/step - loss: 0.4472 - accuracy: 0.7991 - val_loss: 0.6366 - val_accuracy: 0.6830\n",
      "Epoch 201/250\n",
      "161/161 [==============================] - 0s 910us/step - loss: 0.4467 - accuracy: 0.7998 - val_loss: 0.6352 - val_accuracy: 0.6816\n",
      "Epoch 202/250\n",
      "161/161 [==============================] - 0s 917us/step - loss: 0.4462 - accuracy: 0.8000 - val_loss: 0.6349 - val_accuracy: 0.6816\n",
      "Epoch 203/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.4458 - accuracy: 0.7997 - val_loss: 0.6344 - val_accuracy: 0.6816\n",
      "Epoch 204/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.4453 - accuracy: 0.7998 - val_loss: 0.6376 - val_accuracy: 0.6809\n",
      "Epoch 205/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.4448 - accuracy: 0.7996 - val_loss: 0.6363 - val_accuracy: 0.6812\n",
      "Epoch 206/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.8000 - val_loss: 0.6353 - val_accuracy: 0.6820\n",
      "Epoch 207/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.4439 - accuracy: 0.8007 - val_loss: 0.6418 - val_accuracy: 0.6800\n",
      "Epoch 208/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.4435 - accuracy: 0.8010 - val_loss: 0.6419 - val_accuracy: 0.6805\n",
      "Epoch 209/250\n",
      "161/161 [==============================] - 0s 962us/step - loss: 0.4430 - accuracy: 0.8010 - val_loss: 0.6388 - val_accuracy: 0.6825\n",
      "Epoch 210/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.4426 - accuracy: 0.8010 - val_loss: 0.6398 - val_accuracy: 0.6807\n",
      "Epoch 211/250\n",
      "161/161 [==============================] - 0s 925us/step - loss: 0.4421 - accuracy: 0.8014 - val_loss: 0.6421 - val_accuracy: 0.6801\n",
      "Epoch 212/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.4417 - accuracy: 0.8012 - val_loss: 0.6402 - val_accuracy: 0.6828\n",
      "Epoch 213/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.4412 - accuracy: 0.8019 - val_loss: 0.6408 - val_accuracy: 0.6850\n",
      "Epoch 214/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.4408 - accuracy: 0.8015 - val_loss: 0.6414 - val_accuracy: 0.6828\n",
      "Epoch 215/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.4404 - accuracy: 0.8027 - val_loss: 0.6424 - val_accuracy: 0.6823\n",
      "Epoch 216/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.8028 - val_loss: 0.6449 - val_accuracy: 0.6788\n",
      "Epoch 217/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4395 - accuracy: 0.8033 - val_loss: 0.6449 - val_accuracy: 0.6736\n",
      "Epoch 218/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.6442 - val_accuracy: 0.6820\n",
      "Epoch 219/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.4387 - accuracy: 0.8048 - val_loss: 0.6434 - val_accuracy: 0.6758\n",
      "Epoch 220/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.4383 - accuracy: 0.8045 - val_loss: 0.6451 - val_accuracy: 0.6744\n",
      "Epoch 221/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.4378 - accuracy: 0.8055 - val_loss: 0.6479 - val_accuracy: 0.6732\n",
      "Epoch 222/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.4374 - accuracy: 0.8056 - val_loss: 0.6473 - val_accuracy: 0.6733\n",
      "Epoch 223/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.4370 - accuracy: 0.8054 - val_loss: 0.6471 - val_accuracy: 0.6649\n",
      "Epoch 224/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8059 - val_loss: 0.6461 - val_accuracy: 0.6818\n",
      "Epoch 225/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.4362 - accuracy: 0.8061 - val_loss: 0.6503 - val_accuracy: 0.6610\n",
      "Epoch 226/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.4358 - accuracy: 0.8067 - val_loss: 0.6511 - val_accuracy: 0.6606\n",
      "Epoch 227/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.4354 - accuracy: 0.8070 - val_loss: 0.6489 - val_accuracy: 0.6801\n",
      "Epoch 228/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.4349 - accuracy: 0.8075 - val_loss: 0.6483 - val_accuracy: 0.6718\n",
      "Epoch 229/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.4345 - accuracy: 0.8081 - val_loss: 0.6524 - val_accuracy: 0.6675\n",
      "Epoch 230/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.4341 - accuracy: 0.8079 - val_loss: 0.6502 - val_accuracy: 0.6715\n",
      "Epoch 231/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.4337 - accuracy: 0.8086 - val_loss: 0.6510 - val_accuracy: 0.6731\n",
      "Epoch 232/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.4333 - accuracy: 0.8082 - val_loss: 0.6519 - val_accuracy: 0.6729\n",
      "Epoch 233/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8084 - val_loss: 0.6545 - val_accuracy: 0.6683\n",
      "Epoch 234/250\n",
      "161/161 [==============================] - 0s 914us/step - loss: 0.4325 - accuracy: 0.8086 - val_loss: 0.6568 - val_accuracy: 0.6655\n",
      "Epoch 235/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.4321 - accuracy: 0.8090 - val_loss: 0.6548 - val_accuracy: 0.6682\n",
      "Epoch 236/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.4317 - accuracy: 0.8089 - val_loss: 0.6520 - val_accuracy: 0.6718\n",
      "Epoch 237/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.4313 - accuracy: 0.8091 - val_loss: 0.6552 - val_accuracy: 0.6681\n",
      "Epoch 238/250\n",
      "161/161 [==============================] - 0s 925us/step - loss: 0.4309 - accuracy: 0.8092 - val_loss: 0.6574 - val_accuracy: 0.6622\n",
      "Epoch 239/250\n",
      "161/161 [==============================] - 0s 917us/step - loss: 0.4305 - accuracy: 0.8097 - val_loss: 0.6568 - val_accuracy: 0.6669\n",
      "Epoch 240/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.4301 - accuracy: 0.8091 - val_loss: 0.6615 - val_accuracy: 0.6617\n",
      "Epoch 241/250\n",
      "161/161 [==============================] - 0s 917us/step - loss: 0.4298 - accuracy: 0.8097 - val_loss: 0.6610 - val_accuracy: 0.6618\n",
      "Epoch 242/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.4294 - accuracy: 0.8092 - val_loss: 0.6586 - val_accuracy: 0.6645\n",
      "Epoch 243/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.8098 - val_loss: 0.6614 - val_accuracy: 0.6627\n",
      "Epoch 244/250\n",
      "161/161 [==============================] - 0s 930us/step - loss: 0.4287 - accuracy: 0.8099 - val_loss: 0.6610 - val_accuracy: 0.6624\n",
      "Epoch 245/250\n",
      "161/161 [==============================] - 0s 917us/step - loss: 0.4283 - accuracy: 0.8104 - val_loss: 0.6612 - val_accuracy: 0.6626\n",
      "Epoch 246/250\n",
      "161/161 [==============================] - 0s 954us/step - loss: 0.4279 - accuracy: 0.8100 - val_loss: 0.6594 - val_accuracy: 0.6651\n",
      "Epoch 247/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.4275 - accuracy: 0.8101 - val_loss: 0.6622 - val_accuracy: 0.6630\n",
      "Epoch 248/250\n",
      "161/161 [==============================] - 0s 932us/step - loss: 0.4272 - accuracy: 0.8104 - val_loss: 0.6655 - val_accuracy: 0.6601\n",
      "Epoch 249/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.4268 - accuracy: 0.8102 - val_loss: 0.6628 - val_accuracy: 0.6604\n",
      "Epoch 250/250\n",
      "161/161 [==============================] - 0s 923us/step - loss: 0.4264 - accuracy: 0.8104 - val_loss: 0.6660 - val_accuracy: 0.6599\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train, y_train, epochs=250, batch_size=2**8, validation_split=0.25)\n",
    "\n",
    "loss_valloss_difference_1c = \"Training loss indicates how well the model is fitting the training data, whereas the validation loss indicates how well the model is generalizing to new data. At some point the validation loss actually increases, which indicates that the model is overfitting the training data. Ideally we would want to use less epochs so that the model does not overfit the training data, because this in turn also affects the accuracy of the model.  \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189128-f787-4b8c-8c96-6b782065dad7",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "In Lecture 9 we used early stopping to avoid overfitting. Apply this here, with `patience` argument set to 20, a new model `model1b` which otherwise should have a setup identical to `model1`. In which epoch did the model training procedure stop? Figure this out by counting the number of elements in `model1b_fit.history['loss']` and write your answer into the string variable `when_earlystop_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f13f2cb-b29b-4288-9b8d-e69f331faff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7777 - accuracy: 0.4800 - val_loss: 0.8446 - val_accuracy: 0.3018\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7399 - accuracy: 0.4967 - val_loss: 0.7632 - val_accuracy: 0.4357\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7144 - accuracy: 0.5259 - val_loss: 0.7054 - val_accuracy: 0.5207\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6968 - accuracy: 0.5480 - val_loss: 0.6647 - val_accuracy: 0.5753\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5724 - val_loss: 0.6366 - val_accuracy: 0.6321\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6733 - accuracy: 0.5979 - val_loss: 0.6164 - val_accuracy: 0.6610\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.6058 - val_loss: 0.6021 - val_accuracy: 0.6817\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6573 - accuracy: 0.6105 - val_loss: 0.5914 - val_accuracy: 0.6880\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6507 - accuracy: 0.6197 - val_loss: 0.5833 - val_accuracy: 0.7119\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6448 - accuracy: 0.6282 - val_loss: 0.5765 - val_accuracy: 0.7303\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6395 - accuracy: 0.6349 - val_loss: 0.5725 - val_accuracy: 0.7410\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6346 - accuracy: 0.6398 - val_loss: 0.5676 - val_accuracy: 0.7534\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6303 - accuracy: 0.6444 - val_loss: 0.5636 - val_accuracy: 0.7559\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.6439 - val_loss: 0.5604 - val_accuracy: 0.7553\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6225 - accuracy: 0.6482 - val_loss: 0.5581 - val_accuracy: 0.7507\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6190 - accuracy: 0.6583 - val_loss: 0.5560 - val_accuracy: 0.7496\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.6618 - val_loss: 0.5542 - val_accuracy: 0.7523\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6127 - accuracy: 0.6653 - val_loss: 0.5531 - val_accuracy: 0.7507\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6675 - val_loss: 0.5523 - val_accuracy: 0.7488\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.6696 - val_loss: 0.5523 - val_accuracy: 0.7444\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6715 - val_loss: 0.5526 - val_accuracy: 0.7421\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6020 - accuracy: 0.6751 - val_loss: 0.5522 - val_accuracy: 0.7446\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5996 - accuracy: 0.6786 - val_loss: 0.5529 - val_accuracy: 0.7516\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 996us/step - loss: 0.5973 - accuracy: 0.6824 - val_loss: 0.5523 - val_accuracy: 0.7515\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5951 - accuracy: 0.6868 - val_loss: 0.5525 - val_accuracy: 0.7513\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5930 - accuracy: 0.6895 - val_loss: 0.5539 - val_accuracy: 0.7509\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 983us/step - loss: 0.5909 - accuracy: 0.6919 - val_loss: 0.5532 - val_accuracy: 0.7478\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 987us/step - loss: 0.5890 - accuracy: 0.6948 - val_loss: 0.5549 - val_accuracy: 0.7300\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.5871 - accuracy: 0.6955 - val_loss: 0.5558 - val_accuracy: 0.7300\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.5853 - accuracy: 0.6954 - val_loss: 0.5556 - val_accuracy: 0.7280\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 970us/step - loss: 0.5836 - accuracy: 0.6963 - val_loss: 0.5559 - val_accuracy: 0.7261\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 958us/step - loss: 0.5820 - accuracy: 0.6967 - val_loss: 0.5583 - val_accuracy: 0.7240\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.5804 - accuracy: 0.6987 - val_loss: 0.5586 - val_accuracy: 0.7223\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.5788 - accuracy: 0.7004 - val_loss: 0.5593 - val_accuracy: 0.7234\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.5773 - accuracy: 0.7043 - val_loss: 0.5606 - val_accuracy: 0.7183\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5759 - accuracy: 0.7056 - val_loss: 0.5605 - val_accuracy: 0.7170\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 991us/step - loss: 0.5744 - accuracy: 0.7069 - val_loss: 0.5597 - val_accuracy: 0.7194\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 924us/step - loss: 0.5729 - accuracy: 0.7077 - val_loss: 0.5620 - val_accuracy: 0.7169\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 989us/step - loss: 0.5715 - accuracy: 0.7088 - val_loss: 0.5625 - val_accuracy: 0.7182\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 993us/step - loss: 0.5701 - accuracy: 0.7089 - val_loss: 0.5635 - val_accuracy: 0.7153\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7104 - val_loss: 0.5642 - val_accuracy: 0.7090\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 977us/step - loss: 0.5675 - accuracy: 0.7130 - val_loss: 0.5633 - val_accuracy: 0.7101\n",
      "Early stopping is used to stop the training of the model when a monitored\n",
      "metric has stopped improving. In this case the monitored\n",
      "metric is the validation loss. The patience parameter\n",
      "is used to specify how many epochs the model should\n",
      "wait before stopping the training. In this case\n",
      "the model stopped training after 42 epochs.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define architecture here\n",
    "model1b = keras.Sequential()\n",
    "model1b.add(layers.Dense(30, input_dim=X_train.shape[1], activation='ReLU'))\n",
    "model1b.add(layers.Dense(15, activation = 'ReLU'))\n",
    "model1b.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model here \n",
    "model1b.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=['accuracy'])\n",
    "\n",
    "# Fit model here\n",
    "model1b_fit = model1b.fit(X_train, y_train, epochs=250, batch_size=2**8, validation_split=0.25, callbacks=[EarlyStopping(patience=20)])\n",
    "\n",
    "when_earlystop_1d = f\"Early stopping is used to stop the training of the model when a monitored\\nmetric has stopped improving. In this case the monitored\\nmetric is the validation loss. The patience parameter\\nis used to specify how many epochs the model should\\nwait before stopping the training. In this case\\nthe model stopped training after {len(model1b_fit.epoch)} epochs.\"\n",
    "\n",
    "print(when_earlystop_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9a95a-7770-493e-9596-52d9ee9aeefc",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "Even though we haven't finished training our neural net, let us use the `evaluate()` function to measure the predictive performance of `model1b` on the test data. Save the result as `res_model1`. \n",
    "\n",
    "What is the accuracy of the model for validation training data and test data, respectively? What is the difference in accuracy? Save your answer in the string variable `difference_in_accuracy_1e`.\n",
    "*Hint:* the training validation accuracy can be extracted from `model1b_fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb727607-775d-44a3-a437-de1d7db42459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5708317160606384, 0.7085726857185364]\n",
      "The accuracy for the validation training data is\n",
      "0.7101174592971802 and the test data accuracy is 0.7085726857185364 and he difference between\n",
      "the model for validation training data accuracy and test data accuracy is\n",
      "0.0015447735786437988\n"
     ]
    }
   ],
   "source": [
    "res_model1b = model1b.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print(res_model1b)\n",
    "\n",
    "difference_in_accuracy_1e = f\"The accuracy for the validation training data is\\n{model1b_fit.history['val_accuracy'][-1]} and the test data accuracy is {res_model1b[1]} and he difference between\\nthe model for validation training data accuracy and test data accuracy is\\n{abs(model1b_fit.history['val_accuracy'][-1]) - res_model1b[1]}\"\n",
    "\n",
    "print(difference_in_accuracy_1e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef98a6a-2952-4733-a42e-45a44cdcc7f9",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "Now we use the `confusion_matrix()` function from the `metrics` module of scikit-learn to disaggregate model performance to class-specific performance. First, get class predictions on the test data using `predict()`. Save these as `prob_model1`. \n",
    "Second, use `confusion_matrix()` to get a confusion matrix and save it as `CM_model1`. Third, calculate true positive rate and false positive rate and save them as `TPR_1f` and `FPR_1f`. \n",
    "Do your results on TPR and FPR suggest that prediction accuracy is approximately equal in both categories? Write your (specific!) answer into the string variable `categorywise_accuracy_1f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d16957-1fcb-48b0-891c-9b778149090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 368us/step\n",
      "[[10418  2463]\n",
      " [ 2864  2534]]\n",
      "0.46943312337902926\n",
      "0.19121186243304092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prob_model1 = model1b.predict(X_test)\n",
    "CM_model1   = confusion_matrix(y_test, prob_model1 > 0.5)\n",
    "\n",
    "TN = CM_model1[0,0]\n",
    "TP = CM_model1[1,1]\n",
    "FN = CM_model1[1,0]\n",
    "FP = CM_model1[0,1]\n",
    "\n",
    "\n",
    "TPR_1f = TP/(TP + FN)\n",
    "FPR_1f = FP/(FP + TN)\n",
    "\n",
    "print(CM_model1)\n",
    "print(TPR_1f)\n",
    "print(FPR_1f)\n",
    "\n",
    "categorywise_accuracy_1f = \"The model is much better at predicting true negatives, where approximately 80 percent of true negatives are predicted correctly, whereas true positive is only approximately 47 percent correctly predicted. The model is also better at predicting false negatives, where approximately 53 percent of false negatives are predicted correctly, whereas false positives is only approximately 20 percent correctly predicted. One could consider decreasing the decision boundary to increase the true positive rate, but this would in turn increase the false positive rate. Which would be worse than the current situation, because the model is better at predicting true negatives than true positives. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6fa2-89ad-4d87-bc6e-5031b13b1f7b",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "In the lectures we have utilized explicit regularization to avoid overfitting. Here we will use $\\ell_2$ regularization to update the weights. Create the architecture of a new neural net `model2` which is identical to that of `model1b` except for $l_2$ regularization with regularization factor `l2_pen` in the two hidden layers.  \n",
    "\n",
    "Then compile and fit this regularized model with the same parameters as in Task 1d. Save the trained neural net as `model2_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7505b575-e5e1-4d4d-b42b-70960a1222d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0621 - accuracy: 0.5544 - val_loss: 0.8882 - val_accuracy: 0.7022\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0345 - accuracy: 0.5581 - val_loss: 0.8819 - val_accuracy: 0.7027\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 1.0111 - accuracy: 0.5732 - val_loss: 0.8729 - val_accuracy: 0.7101\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9909 - accuracy: 0.5831 - val_loss: 0.8629 - val_accuracy: 0.7189\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 987us/step - loss: 0.9730 - accuracy: 0.5936 - val_loss: 0.8523 - val_accuracy: 0.7371\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 968us/step - loss: 0.9568 - accuracy: 0.6056 - val_loss: 0.8428 - val_accuracy: 0.7432\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 971us/step - loss: 0.9421 - accuracy: 0.6157 - val_loss: 0.8337 - val_accuracy: 0.7662\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.9286 - accuracy: 0.6219 - val_loss: 0.8250 - val_accuracy: 0.7669\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.9160 - accuracy: 0.6287 - val_loss: 0.8166 - val_accuracy: 0.7777\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 988us/step - loss: 0.9043 - accuracy: 0.6349 - val_loss: 0.8087 - val_accuracy: 0.7924\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8931 - accuracy: 0.6416 - val_loss: 0.8020 - val_accuracy: 0.8048\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 975us/step - loss: 0.8826 - accuracy: 0.6440 - val_loss: 0.7940 - val_accuracy: 0.8075\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.8727 - accuracy: 0.6495 - val_loss: 0.7866 - val_accuracy: 0.8111\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 953us/step - loss: 0.8633 - accuracy: 0.6553 - val_loss: 0.7796 - val_accuracy: 0.8219\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 957us/step - loss: 0.8544 - accuracy: 0.6593 - val_loss: 0.7736 - val_accuracy: 0.8222\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.8459 - accuracy: 0.6595 - val_loss: 0.7673 - val_accuracy: 0.8305\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.8377 - accuracy: 0.6604 - val_loss: 0.7610 - val_accuracy: 0.8282\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 929us/step - loss: 0.8298 - accuracy: 0.6615 - val_loss: 0.7550 - val_accuracy: 0.8268\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.8222 - accuracy: 0.6643 - val_loss: 0.7492 - val_accuracy: 0.8276\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.8149 - accuracy: 0.6679 - val_loss: 0.7444 - val_accuracy: 0.8233\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.8079 - accuracy: 0.6696 - val_loss: 0.7399 - val_accuracy: 0.8211\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.8013 - accuracy: 0.6729 - val_loss: 0.7349 - val_accuracy: 0.8174\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 946us/step - loss: 0.7949 - accuracy: 0.6728 - val_loss: 0.7307 - val_accuracy: 0.8163\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.7887 - accuracy: 0.6766 - val_loss: 0.7261 - val_accuracy: 0.8162\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.7828 - accuracy: 0.6776 - val_loss: 0.7215 - val_accuracy: 0.8152\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.7771 - accuracy: 0.6807 - val_loss: 0.7172 - val_accuracy: 0.8156\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 955us/step - loss: 0.7716 - accuracy: 0.6829 - val_loss: 0.7125 - val_accuracy: 0.8241\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.7663 - accuracy: 0.6835 - val_loss: 0.7093 - val_accuracy: 0.8219\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 956us/step - loss: 0.7611 - accuracy: 0.6833 - val_loss: 0.7060 - val_accuracy: 0.8209\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 958us/step - loss: 0.7561 - accuracy: 0.6843 - val_loss: 0.7018 - val_accuracy: 0.8224\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.7513 - accuracy: 0.6835 - val_loss: 0.6979 - val_accuracy: 0.8184\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 930us/step - loss: 0.7466 - accuracy: 0.6851 - val_loss: 0.6956 - val_accuracy: 0.8171\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.7421 - accuracy: 0.6856 - val_loss: 0.6922 - val_accuracy: 0.8160\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.7378 - accuracy: 0.6877 - val_loss: 0.6894 - val_accuracy: 0.8131\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 959us/step - loss: 0.7336 - accuracy: 0.6885 - val_loss: 0.6874 - val_accuracy: 0.8108\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.7295 - accuracy: 0.6904 - val_loss: 0.6846 - val_accuracy: 0.8088\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.7256 - accuracy: 0.6919 - val_loss: 0.6814 - val_accuracy: 0.8064\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.7219 - accuracy: 0.6917 - val_loss: 0.6799 - val_accuracy: 0.8052\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 914us/step - loss: 0.7183 - accuracy: 0.6931 - val_loss: 0.6782 - val_accuracy: 0.8048\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.7149 - accuracy: 0.6945 - val_loss: 0.6767 - val_accuracy: 0.8049\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.7115 - accuracy: 0.6953 - val_loss: 0.6744 - val_accuracy: 0.8047\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.7082 - accuracy: 0.6966 - val_loss: 0.6714 - val_accuracy: 0.8046\n",
      "Epoch 43/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.7050 - accuracy: 0.6981 - val_loss: 0.6700 - val_accuracy: 0.8039\n",
      "Epoch 44/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.7019 - accuracy: 0.6987 - val_loss: 0.6678 - val_accuracy: 0.8034\n",
      "Epoch 45/250\n",
      "161/161 [==============================] - 0s 972us/step - loss: 0.6989 - accuracy: 0.7003 - val_loss: 0.6655 - val_accuracy: 0.8023\n",
      "Epoch 46/250\n",
      "161/161 [==============================] - 0s 953us/step - loss: 0.6960 - accuracy: 0.7019 - val_loss: 0.6656 - val_accuracy: 0.7990\n",
      "Epoch 47/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.6931 - accuracy: 0.7027 - val_loss: 0.6637 - val_accuracy: 0.7990\n",
      "Epoch 48/250\n",
      "161/161 [==============================] - 0s 957us/step - loss: 0.6904 - accuracy: 0.7037 - val_loss: 0.6613 - val_accuracy: 0.7987\n",
      "Epoch 49/250\n",
      "161/161 [==============================] - 0s 955us/step - loss: 0.6877 - accuracy: 0.7046 - val_loss: 0.6597 - val_accuracy: 0.7991\n",
      "Epoch 50/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6851 - accuracy: 0.7051 - val_loss: 0.6602 - val_accuracy: 0.7942\n",
      "Epoch 51/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.6825 - accuracy: 0.7064 - val_loss: 0.6573 - val_accuracy: 0.7919\n",
      "Epoch 52/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.6800 - accuracy: 0.7077 - val_loss: 0.6566 - val_accuracy: 0.7914\n",
      "Epoch 53/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.6776 - accuracy: 0.7087 - val_loss: 0.6560 - val_accuracy: 0.7860\n",
      "Epoch 54/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.6752 - accuracy: 0.7123 - val_loss: 0.6542 - val_accuracy: 0.7859\n",
      "Epoch 55/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.6729 - accuracy: 0.7148 - val_loss: 0.6533 - val_accuracy: 0.7901\n",
      "Epoch 56/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.6706 - accuracy: 0.7166 - val_loss: 0.6502 - val_accuracy: 0.7903\n",
      "Epoch 57/250\n",
      "161/161 [==============================] - 0s 934us/step - loss: 0.6684 - accuracy: 0.7159 - val_loss: 0.6505 - val_accuracy: 0.7901\n",
      "Epoch 58/250\n",
      "161/161 [==============================] - 0s 958us/step - loss: 0.6662 - accuracy: 0.7163 - val_loss: 0.6495 - val_accuracy: 0.7895\n",
      "Epoch 59/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.6641 - accuracy: 0.7160 - val_loss: 0.6486 - val_accuracy: 0.7861\n",
      "Epoch 60/250\n",
      "161/161 [==============================] - 0s 950us/step - loss: 0.6620 - accuracy: 0.7177 - val_loss: 0.6480 - val_accuracy: 0.7796\n",
      "Epoch 61/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.6599 - accuracy: 0.7176 - val_loss: 0.6466 - val_accuracy: 0.7802\n",
      "Epoch 62/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.6579 - accuracy: 0.7188 - val_loss: 0.6461 - val_accuracy: 0.7783\n",
      "Epoch 63/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6559 - accuracy: 0.7195 - val_loss: 0.6441 - val_accuracy: 0.7785\n",
      "Epoch 64/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.6539 - accuracy: 0.7183 - val_loss: 0.6444 - val_accuracy: 0.7699\n",
      "Epoch 65/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.6520 - accuracy: 0.7191 - val_loss: 0.6431 - val_accuracy: 0.7757\n",
      "Epoch 66/250\n",
      "161/161 [==============================] - 0s 940us/step - loss: 0.6500 - accuracy: 0.7207 - val_loss: 0.6438 - val_accuracy: 0.7720\n",
      "Epoch 67/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.6481 - accuracy: 0.7223 - val_loss: 0.6425 - val_accuracy: 0.7718\n",
      "Epoch 68/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.6462 - accuracy: 0.7237 - val_loss: 0.6428 - val_accuracy: 0.7715\n",
      "Epoch 69/250\n",
      "161/161 [==============================] - 0s 945us/step - loss: 0.6444 - accuracy: 0.7237 - val_loss: 0.6425 - val_accuracy: 0.7664\n",
      "Epoch 70/250\n",
      "161/161 [==============================] - 0s 938us/step - loss: 0.6426 - accuracy: 0.7243 - val_loss: 0.6403 - val_accuracy: 0.7717\n",
      "Epoch 71/250\n",
      "161/161 [==============================] - 0s 947us/step - loss: 0.6409 - accuracy: 0.7245 - val_loss: 0.6413 - val_accuracy: 0.7660\n",
      "Epoch 72/250\n",
      "161/161 [==============================] - 0s 936us/step - loss: 0.6391 - accuracy: 0.7241 - val_loss: 0.6397 - val_accuracy: 0.7684\n",
      "Epoch 73/250\n",
      "161/161 [==============================] - 0s 948us/step - loss: 0.6374 - accuracy: 0.7245 - val_loss: 0.6395 - val_accuracy: 0.7680\n",
      "Epoch 74/250\n",
      "161/161 [==============================] - 0s 967us/step - loss: 0.6357 - accuracy: 0.7259 - val_loss: 0.6409 - val_accuracy: 0.7594\n",
      "Epoch 75/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6340 - accuracy: 0.7266 - val_loss: 0.6402 - val_accuracy: 0.7486\n",
      "Epoch 76/250\n",
      "161/161 [==============================] - 0s 944us/step - loss: 0.6323 - accuracy: 0.7270 - val_loss: 0.6395 - val_accuracy: 0.7483\n",
      "Epoch 77/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.6306 - accuracy: 0.7275 - val_loss: 0.6410 - val_accuracy: 0.7335\n",
      "Epoch 78/250\n",
      "161/161 [==============================] - 0s 949us/step - loss: 0.6290 - accuracy: 0.7280 - val_loss: 0.6391 - val_accuracy: 0.7427\n",
      "Epoch 79/250\n",
      "161/161 [==============================] - 0s 921us/step - loss: 0.6273 - accuracy: 0.7291 - val_loss: 0.6400 - val_accuracy: 0.7276\n",
      "Epoch 80/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.6257 - accuracy: 0.7295 - val_loss: 0.6398 - val_accuracy: 0.7275\n",
      "Epoch 81/250\n",
      "161/161 [==============================] - 0s 943us/step - loss: 0.6241 - accuracy: 0.7318 - val_loss: 0.6377 - val_accuracy: 0.7422\n",
      "Epoch 82/250\n",
      "161/161 [==============================] - 0s 939us/step - loss: 0.6225 - accuracy: 0.7310 - val_loss: 0.6391 - val_accuracy: 0.7273\n",
      "Epoch 83/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.6209 - accuracy: 0.7324 - val_loss: 0.6385 - val_accuracy: 0.7213\n",
      "Epoch 84/250\n",
      "161/161 [==============================] - 0s 933us/step - loss: 0.6194 - accuracy: 0.7342 - val_loss: 0.6410 - val_accuracy: 0.7203\n",
      "Epoch 85/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.6178 - accuracy: 0.7346 - val_loss: 0.6404 - val_accuracy: 0.7203\n",
      "Epoch 86/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.6162 - accuracy: 0.7355 - val_loss: 0.6407 - val_accuracy: 0.7185\n",
      "Epoch 87/250\n",
      "161/161 [==============================] - 0s 919us/step - loss: 0.6147 - accuracy: 0.7372 - val_loss: 0.6406 - val_accuracy: 0.7249\n",
      "Epoch 88/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.6132 - accuracy: 0.7375 - val_loss: 0.6416 - val_accuracy: 0.7176\n",
      "Epoch 89/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.6117 - accuracy: 0.7375 - val_loss: 0.6415 - val_accuracy: 0.7239\n",
      "Epoch 90/250\n",
      "161/161 [==============================] - 0s 941us/step - loss: 0.6102 - accuracy: 0.7393 - val_loss: 0.6426 - val_accuracy: 0.7240\n",
      "Epoch 91/250\n",
      "161/161 [==============================] - 0s 928us/step - loss: 0.6087 - accuracy: 0.7397 - val_loss: 0.6381 - val_accuracy: 0.7321\n",
      "Epoch 92/250\n",
      "161/161 [==============================] - 0s 988us/step - loss: 0.6072 - accuracy: 0.7412 - val_loss: 0.6421 - val_accuracy: 0.7209\n",
      "Epoch 93/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.6057 - accuracy: 0.7421 - val_loss: 0.6407 - val_accuracy: 0.7289\n",
      "Epoch 94/250\n",
      "161/161 [==============================] - 0s 926us/step - loss: 0.6042 - accuracy: 0.7430 - val_loss: 0.6431 - val_accuracy: 0.7267\n",
      "Epoch 95/250\n",
      "161/161 [==============================] - 0s 937us/step - loss: 0.6027 - accuracy: 0.7452 - val_loss: 0.6436 - val_accuracy: 0.7264\n",
      "Epoch 96/250\n",
      "161/161 [==============================] - 0s 931us/step - loss: 0.6013 - accuracy: 0.7455 - val_loss: 0.6457 - val_accuracy: 0.7230\n",
      "Epoch 97/250\n",
      "161/161 [==============================] - 0s 935us/step - loss: 0.5998 - accuracy: 0.7466 - val_loss: 0.6438 - val_accuracy: 0.7245\n",
      "Epoch 98/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5983 - accuracy: 0.7473 - val_loss: 0.6462 - val_accuracy: 0.7236\n",
      "Epoch 99/250\n",
      "161/161 [==============================] - 0s 942us/step - loss: 0.5969 - accuracy: 0.7468 - val_loss: 0.6454 - val_accuracy: 0.7243\n",
      "Epoch 100/250\n",
      "161/161 [==============================] - 0s 930us/step - loss: 0.5955 - accuracy: 0.7473 - val_loss: 0.6451 - val_accuracy: 0.7215\n",
      "Epoch 101/250\n",
      "161/161 [==============================] - 0s 927us/step - loss: 0.5941 - accuracy: 0.7494 - val_loss: 0.6468 - val_accuracy: 0.7195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2935b6ed0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "l2_pen = 0.005\n",
    "\n",
    "# 1.\n",
    "model2 = keras.Sequential()\n",
    "model2.add(layers.Dense(30, input_dim = X_train.shape[1], activation = 'ReLU', kernel_regularizer = l2(l2_pen)))\n",
    "model2.add(layers.Dense(15, activation = 'ReLU', kernel_regularizer = l2(l2_pen)))\n",
    "model2.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# 2.\n",
    "model2.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 0.00003), metrics = ['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs = 250, batch_size = 2**8, validation_split = 0.25, callbacks=[EarlyStopping(patience=20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a8d24-9b08-48fe-b120-ca68106406fb",
   "metadata": {},
   "source": [
    "### Task 1h)\n",
    "In Task 1e) we compared the prediction accuracy on test and training sets. However, this is bad measure when the data is not well balanced (in terms of the observed output categories). Instead, one can use the cross entropy for the binomial distribution (minus the average log likelihood of the model). In fact, we chose this function as loss function for model training when we compiled `model1` and `model2`.\n",
    "\n",
    "To compare the test error of `model2` to that of `model1b` we don't want to use `loss` from `evaluate` since this includes the $\\ell_2$ penalty.  In the library `MLmetrics` the function `log_loss()` computes the cross entropy for the binomial distribution without penalty term. \n",
    "\n",
    "First, get predicted output probabilities on test data from `model2` and save them as `prob_model2`.\n",
    "Second, use `log_loss()` from the metrics module in scikit-learn to compute the cross-entropy loss for `model2` on the test data and save it as `logloss_model2`. \n",
    "Third, use the string variable `performance_comparison_1h` to describe how the accuracy on test data differs between `model1b` and `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "023a39ea-f599-4f1d-9ca0-4e54aa598cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 378us/step\n",
      "0.5361406466047297\n",
      "From the log loss we can that the model is performing even better when adding a penalty,\n",
      "quite significantly actually, the log loss is 0.536141 compared to 0.570832\n",
      "for model 1b. The accuracy achieved is 0.733574 with\n",
      "a penalty and 0.708573 without a penalty. The difference between loss of the two models\n",
      "is 0.034691 and the difference between the accuracy of the two models is\n",
      "0.025001. One could also imagine that if even more features were added to the model,\n",
      "the penalty term would perform even better.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. \n",
    "prob_model2    = model2.predict(X_test)\n",
    "\n",
    "# 2.\n",
    "logloss_model2 = log_loss(y_test, prob_model2)\n",
    "print(logloss_model2)\n",
    "\n",
    "# 3.\n",
    "accuracy_model2     = model2.evaluate(X_test, y_test, verbose = 0)[1]\n",
    "performance_comparison_1h = f\"From the log loss we can that the model is performing even better when adding a penalty,\\nquite significantly actually, the log loss is {logloss_model2:.6f} compared to {res_model1b[0]:.6f}\\nfor model 1b. The accuracy achieved is {accuracy_model2:.6f} with\\na penalty and {res_model1b[1]:.6f} without a penalty. The difference between loss of the two models\\nis {abs(logloss_model2 - res_model1b[0]):.6f} and the difference between the accuracy of the two models is\\n{abs(accuracy_model2 - res_model1b[1]):.6f}. One could also imagine that if even more features were added to the model,\\nthe penalty term would perform even better.\"\n",
    "print(performance_comparison_1h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8bf1-f86e-4b0c-9cc6-bdd9f1d964c4",
   "metadata": {},
   "source": [
    "## Part 2: Tuning neural nets with caret\n",
    "\n",
    "Keras provides functions that allow the use of scikit-learn for model tuning. Using this functionality requires relatively little effort and in this part we are going to practice the individual steps of model tuning with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac151a-c3c3-44e8-8adb-a8a3cedd6d95",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "First, we need to define the architecture of the neural net that we tune and to compile the model. This needs to be done inside a function. The arguments of this function are the tuning parameters whose candidate values we want to feed into the function one by one.\n",
    "\n",
    "In this task, we work with the architecture defined for `model2` in Task 1g. The only parameter that we want to tune is the regularization parameter for $\\ell_2$ penalization inside the hidden units.\n",
    "\n",
    "Now create a function `modelbuild_2a` which has one argument `l2_pen`. In this function, specify the architecture of a Keras model `model`, identical to that of `model2` and with $\\ell_2$-penalty set to `ell_2`. Compile the model with the same settings as in Task 1g and return it as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecfec657-d581-4b0c-b751-114dc1496386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelbuild_2a(l2_pen=0.0):\n",
    "    ell_2 = l2_pen\n",
    "    model_2a = keras.Sequential()\n",
    "    model_2a.add(layers.Dense(30, input_dim = X_train.shape[1], activation = \"ReLU\", kernel_regularizer=l2(ell_2)))\n",
    "    model_2a.add(layers.Dense(15, activation = \"ReLU\", kernel_regularizer=l2(ell_2)))\n",
    "    model_2a.add(layers.Dense(1, activation= \"sigmoid\"))\n",
    "    model_2a.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=[\"accuracy\"])\n",
    "    return(model_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078933-0ccc-4f9e-abb6-e26520bd8760",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "In order to make the function `modelbuild_2a` compatible with `sklearn`, we need to call it inside the `KerasClassifier()` function from the `wrappers` module of `scikit-learn`. As additional arguments, provide all arguments that you used when fitting `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "027797af-2ee2-4566-8c67-ef98b0f40f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "model2_sklearn_spec = KerasClassifier(build_fn=modelbuild_2a, l2_pen=0.0,\n",
    "                                      epochs=250, \n",
    "                                      batch_size=2**8, \n",
    "                                      verbose=0, \n",
    "                                      validation_split=0.25, \n",
    "                                      callbacks=[EarlyStopping(patience=20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cce66-a557-4597-80dc-bb1ff84b4f1e",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "Next, define a parameter grid `tune_grid_2c`. This must be a dictionary object. Ensure that the only object within `tune_grid_2c` is called `l2_pen`. Its values should be zero as well as $10^{r}$ for a grid of eleven $r$-values from $-4$ and $-1$ at equal distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63925c0d-6abd-49e6-bfad-5adc320ce894",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid_2c = {\n",
    "\"l2_pen\": np.insert(np.logspace(-4, -1, 11), 0, 0).tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978345-6081-478b-9549-771d300a1992",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "Now, we can tune our model. That's computationally quite costly, so we will use merely a fraction of the available training data. The inputs and outputs for this task are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e12d402-63b4-420b-a4b0-8c6f2830fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_ , y_train_small, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847933-232b-499f-847a-04255215d8cd",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "1. Use `Kfold()` from the model selection module in scikit-learn to define a random partition of the training data into five folds. Use $5$ as your random seed. Save this partition as `cv_splits_2d`\n",
    "2. Call `GridSearchCV()` and use the wrapper function from Task 2b, the parameter grid from Task 2c as well as `cv_splits_2d` as arguments\n",
    "3. Apply the `fit()`-method to `GridSearchCV` and use `X_train_small` and `y_train_small` as inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8746fa6-0cd3-469c-afb2-7278ee02ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Users/asger/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation finished in 702.1754260063171 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1.\n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "# 2.\n",
    "NN_tune_2d = GridSearchCV(estimator=model2_sklearn_spec, \n",
    "                          param_grid=tune_grid_2c,\n",
    "                          cv=cv_splits_2d)\n",
    "\n",
    "# 3.\n",
    "NN_tune_2d.fit(X_train_small, y_train_small)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Cross validation finished in {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16a15-f9d0-4d38-896a-627e4d5dde75",
   "metadata": {},
   "source": [
    "### Task 2e)\n",
    "By default, GridSearchCV saves the trained model with the best tuning parameter values as as object `best_estimator_` inside `NN_tune_2d`. However, in our case this model is a KerasClassifier object. We cannot use such an object for making predictions on test data. \n",
    "\n",
    "Still, the KerasClassifier object contains the trained model in the typical Keras format as object `model`. Extract this object-inside-the-object-inside-the-object and save it as `model3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6ec904-ce98-4907-b3fa-cc54b1a43cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_189 (Dense)           (None, 30)                3720      \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 15)                465       \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4201 (16.41 KB)\n",
      "Trainable params: 4201 (16.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = NN_tune_2d.best_estimator_.model_\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6448b-ac63-4129-9236-6dcb23bcf1dd",
   "metadata": {},
   "source": [
    "## Part 3: Saving, loading and retraining neural nets \n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "Training and tuning neural nets can take a lot of time. Therefore, it is possible to save entire fitted models to disk and to import them at a later point in time. Apply the `save()`-method to your most recent `model3` in order to save it as *DABN13_asst6_saved_model3* in your working directory.\n",
    "\n",
    "Ensure that the model is saved in TensorFlow SavedModel format.\n",
    "\n",
    "Additionally, use the file explorer in your operating system to look how exactly the model was saved on your hard drive. Describe this shortly in the string_variable `saved_model_3a`\n",
    "\n",
    "*Note:* Functions for saving and loading models are very nicely described in the [keras documentation](https://keras.io/api/saving/model_saving_and_loading/#save-method)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720e1779-a59c-4153-9c89-39d3be986fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DABN13_asst6_saved_model3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DABN13_asst6_saved_model3/assets\n"
     ]
    }
   ],
   "source": [
    "model3.save(\"DABN13_asst6_saved_model3\")\n",
    "\n",
    "saved_model_3a = \"The save function creates a folder in the current working directory. The folder contains two internal folders \\ncalled 'assets' and 'variables'. In addition to that the folder contains three files called fingerprint.pb\\n keras_metadata.pb, and saved_model.pb. The saved_model.pb file contains the architecture and weights, and everything else \\n is supporting files.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424edf9-37e1-4b9e-9c91-4836c6367709",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Now, use the `load_model` function in the `models` module of Keras to load your saved model into your python session again. Save this model as `model4`.\n",
    "\n",
    "*Note:* The possibility to load a previously saved model from your hard disk is useful for more than just your own models. It even allows you to load pretrained models for specific purposes from the TensorFlow Hub or from Hugging Face. These models could then directly be used for prediction or fine-tuned on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6f89a6c-9730-4dbd-950f-2d5b5f2f0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model4 = load_model(\"DABN13_asst6_saved_model3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385071-18a5-4ca3-906e-895fdeb9c692",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "When we tuned our most recent neural net, we did this on a relatively small fraction of the training data to reduce the computational cost. This was also the data used to train the best model that we extracted from `NN_tune_2d`.\n",
    "Now that we have chosen an optimal tuning parameter, it makes sense to retrain the `model4` on the entire training data `X_train` and `y_train`. Do this by applying the `fit()` method to `model4`. As previously, training should be done for 250 epochs, unless early stopping with a patience of 20 epochs kicks in. Minibatches of $2^8$ data points should be used. Given the large amount of data, hold only 10% of the data aside for monitoring validation loss.\n",
    "\n",
    "Once you retrained your model, obtain predicted class probabilities and save them as `prob_model4`. Then, get the log loss `logloss_model4` on the test data.\n",
    "\n",
    "Finally, to what extend did model tuning and retraining change test set accuracy relative to that of `model2`? Comment on this in the string variable `performance_comparison_3c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f945e31-63c3-48ef-a5e2-54e7b36cdddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7520 - val_loss: 0.4417 - val_accuracy: 0.8592\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5343 - accuracy: 0.7521 - val_loss: 0.4439 - val_accuracy: 0.8591\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7529 - val_loss: 0.4459 - val_accuracy: 0.8583\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7535 - val_loss: 0.4483 - val_accuracy: 0.8572\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5309 - accuracy: 0.7539 - val_loss: 0.4491 - val_accuracy: 0.8497\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7539 - val_loss: 0.4504 - val_accuracy: 0.8494\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5288 - accuracy: 0.7541 - val_loss: 0.4517 - val_accuracy: 0.8473\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5278 - accuracy: 0.7559 - val_loss: 0.4532 - val_accuracy: 0.8471\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5268 - accuracy: 0.7566 - val_loss: 0.4539 - val_accuracy: 0.8471\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7570 - val_loss: 0.4550 - val_accuracy: 0.8487\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7565 - val_loss: 0.4569 - val_accuracy: 0.8457\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7573 - val_loss: 0.4579 - val_accuracy: 0.8432\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7583 - val_loss: 0.4592 - val_accuracy: 0.8422\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7595 - val_loss: 0.4597 - val_accuracy: 0.8351\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7614 - val_loss: 0.4601 - val_accuracy: 0.8354\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7610 - val_loss: 0.4610 - val_accuracy: 0.8349\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5190 - accuracy: 0.7609 - val_loss: 0.4616 - val_accuracy: 0.8349\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.7623 - val_loss: 0.4630 - val_accuracy: 0.8335\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5172 - accuracy: 0.7626 - val_loss: 0.4638 - val_accuracy: 0.8347\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5163 - accuracy: 0.7636 - val_loss: 0.4652 - val_accuracy: 0.8341\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7634 - val_loss: 0.4665 - val_accuracy: 0.8325\n",
      "572/572 [==============================] - 0s 379us/step\n",
      "0.502785639871088\n",
      "\n",
      "The accuracy of model4 on test data is 0.772526 \n",
      "The accuracy of model2 on test data is 0.733574 \n",
      "The difference is 0.0389518\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "model4_fit = model4.fit(x = X_train,\n",
    "                    y = y_train,\n",
    "                    epochs = 250,\n",
    "                    batch_size = 2**8,\n",
    "                    validation_split = 0.25,\n",
    "                    callbacks = [EarlyStopping(patience = 20)])\n",
    "\n",
    "# 2.\n",
    "prob_model4    = model4.predict(X_test)\n",
    "logloss_model4 = log_loss(y_test, prob_model4)\n",
    "print(logloss_model4)\n",
    "\n",
    "# 3.\n",
    "accuracy_model4 = model4.evaluate(X_test, y_test, verbose = 0)[1]\n",
    "\n",
    "performance_comparison_3c = f\"\\nThe accuracy of model4 on test data is {accuracy_model4:.6} \\nThe accuracy of model2 on test data is {accuracy_model2:.6} \\nThe difference is {(accuracy_model4 - accuracy_model2):.6}\"\n",
    "print(performance_comparison_3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766e3e-925d-434b-a546-35b8a77fa177",
   "metadata": {},
   "source": [
    "## Part 4: Manual predictions from a trained neural net\n",
    "\n",
    "In this part we will build predictions *manually* by extracting weights from the trained  `model2` and by constructing the transformations in the layers of the neural net ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3b6b3-6107-41d3-8bd4-09cc1c393791",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "\n",
    "Start this task by creating your own ReLU activation function. Save it as `ReLU`. Then, write your own sigmoid function for the output layer transformation. Save it as  `sigmoid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "755df496-d94c-48b7-8a0d-d406e94d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+ np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8157-ef24-4994-a4bd-34827db28d8e",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "In the slides for lecture 8 we discussed how units in the different layers of a neural net look like. Now we are going to use the equations both hidden units and output unit to construct output predictions for the $n_{test}$ data points in `X_test`. Please do the following:\n",
    "\n",
    "1. Apply the `get_weights()` method on `model2` to obtain a list object which stores the weights and biases of the learned model. Save it as `weight_and_bias_4b`.\n",
    "2. Extract the objects inside `weight_and_bias_4b` into the objects for $\\mathbf{b}_1,\\mathbf{W}_1, \\mathbf{b}_2, \\mathbf{W}_2,\\mathbf{b}_3, \\mathbf{W}_3$ defined in the code chunk below.\n",
    "3. Construct the linear term of the first layer hidden units and save these linear terms as a $30 \\times n_{train}$ vector `Z_1`.\n",
    "4. Construct the $15 \\times n_{train}$ vector `Z_2` of linear terms for the second layer hidden units. Then, obtain the linear term of the output unit and save it as `Z_3`.\n",
    "5. Put `Z_3` into the output layer activation function in order to get predictions for the probability of a Bud Light purchase. Save this as $n_{train} \\times 1$ vector `pred_own_2b`. \n",
    "\n",
    "*Hint*: You can use `dim()` to check the dimension of matrices and `length()` to check that of vectors. Additionally, to ensure that you got the correct result for `prob_model2_own_4b` you can compare it with the output of `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50523588-05be-4174-b294-392f96cd8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Manual predictions  Predict function\n",
      "0                0.489174          0.489174\n",
      "1                0.484462          0.484462\n",
      "2                0.157598          0.157598\n",
      "3                0.266405          0.266405\n",
      "4                0.803105          0.803105\n",
      "...                   ...               ...\n",
      "18274            0.292150          0.292150\n",
      "18275            0.292150          0.292150\n",
      "18276            0.316001          0.316001\n",
      "18277            0.505344          0.505344\n",
      "18278            0.704030          0.704030\n",
      "\n",
      "[18279 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "weight_and_bias_4b = model2.get_weights()\n",
    "\n",
    "# 2.\n",
    "WW_1 = weight_and_bias_4b[0]\n",
    "bb_1 = weight_and_bias_4b[1]\n",
    "WW_2 = weight_and_bias_4b[2]\n",
    "bb_2 = weight_and_bias_4b[3]\n",
    "WW_3 = weight_and_bias_4b[4]\n",
    "bb_3 = weight_and_bias_4b[5]\n",
    "\n",
    "# 3.\n",
    "Z_1 = ReLU(np.dot(X_test, WW_1) + bb_1)\n",
    "\n",
    "# 4.\n",
    "Z_2 = ReLU(np.dot(Z_1, WW_2) + bb_2)\n",
    "Z_3 = np.dot(Z_2, WW_3) + bb_3\n",
    "\n",
    "# Apply sigmoid activation to Z_3\n",
    "prob_model2_own_4b = sigmoid(Z_3)\n",
    "\n",
    "predict_df = pd.DataFrame({\"Manual predictions\": prob_model2_own_4b.flatten(), \"Predict function\": prob_model2.flatten()})\n",
    "print(predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc42fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
