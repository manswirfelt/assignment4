{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1cb77-5344-418c-94b3-7bacb800c746",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 6\n",
    "\n",
    "## Preamble: Data\n",
    "In this lab we are using a dataset on beer purchases. Our goal is to predict if light beer purchased in the US is BUD light. To achieve this goal, we will use the information provided by the following socioeconomic characteristics:\n",
    "* market           - where the beer is bought\n",
    "* buyertype        - who is the buyer () \n",
    "* income           - ranges of income\n",
    "* childrenUnder6   - does the buyer have children under 6 years old\n",
    "* children6to17    - does the buyer have children between 6 and 17\n",
    "* age              - bracketed age groups\n",
    "* employment       - fully employed, partially employed, no employment.\n",
    "* degree           - level of occupation\n",
    "* occuptation      - which sector you are employed in\n",
    "* ethnic           - white, asian, hispanic, black or other\n",
    "* microwave        - own a microwave\n",
    "* dishwasher       - own a dishwasher\n",
    "* tvcable          - what type cable tv subscription you have\n",
    "* singlefamilyhome - are you in a single family home\n",
    "* npeople          - number of people you live with 1,2,3,4, +5\n",
    "\n",
    "First, we load the dataset and create an output variable that indicates purchases of Bud Light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620cb558-0655-4ff6-80d8-51cc89e3c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "np.random.seed(2023)\n",
    "tf.random.set_seed(2023)\n",
    "random.seed(2023)\n",
    "\n",
    "# os.chdir(\"??\") Change working directory if needed \n",
    "\n",
    "lb    = pd.read_csv(\"LightBeer2.csv\")\n",
    "y     = np.zeros(shape=lb.shape[0])\n",
    "y[lb['beer_brand'] == \"BUD LIGHT\"]     = 1\n",
    "demog = lb.iloc[:,9:]\n",
    "demog = pd.get_dummies(demog, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174fd7-d779-4207-a30e-224c89bdcbc9",
   "metadata": {},
   "source": [
    "We also split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5c287b-7069-426c-b3c1-15857ff123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(demog, y, train_size=0.75, shuffle=False)\n",
    "\n",
    "stdz_X = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = stdz_X.transform(X_train)\n",
    "X_test  = stdz_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983d48-57e5-4342-9935-16676fd3a106",
   "metadata": {},
   "source": [
    "## Part 1: Specifying and training neural networks\n",
    "We will now start building a neural network to predict the purchase of Bud Light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7cab-178e-4fee-b942-8ab68a140e23",
   "metadata": {},
   "source": [
    "### Task 1a) \n",
    "We start with specifying the architecture of our very first and very small neural net `model1`.\n",
    "Add three layers to `model1`, two hidden layers with $30$ and $15$ hidden units, respectively, and an output layer.\n",
    "For the two hidden layers you should use the ReLU activation function. Additionally, choose a suitable activation for the output layer, given that we have a classification problem. See [the Keras documentation](https://keras.io/api/layers/activations/) for activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d9388d-cbd6-4d10-90ff-2c3d974a8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize a first model\n",
    "model1 = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model1.add(layers.Dense(30, input_dim = X_train.shape[1], activation = \"ReLU\"))\n",
    "model1.add(layers.Dense(15, activation = \"ReLU\"))\n",
    "model1.add(layers.Dense(1, activation= \"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd214-22bf-42fa-a973-2c27f0643431",
   "metadata": {},
   "source": [
    "### Task 1b) \n",
    "\n",
    "Next, we compile our model specification. From [https://keras.io/api/losses/probabilistic_losses/](losses) select a suitable loss function for our classification problem. As optimization algorithm use *Adam* with learning rate $0.00003$. Lastly, use `accuracy` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a88ddd5-3e85-4238-96a3-81b914f4fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2904463-9a53-4ea3-b175-adfc9ff0f599",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "Now train the model using $250$ epochs, a batch_size of $2^8$, and use $25\\%$ of the data for validation.\n",
    "Use the string variable `loss_valloss_difference_1c` to describe and explain the observed difference between validation loss and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f00f52b-92c7-4100-bfe0-05cdd6f0fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.7756 - accuracy: 0.4681 - val_loss: 0.8387 - val_accuracy: 0.3946\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7330 - accuracy: 0.5154 - val_loss: 0.7501 - val_accuracy: 0.4872\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7061 - accuracy: 0.5498 - val_loss: 0.6868 - val_accuracy: 0.5748\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5680 - val_loss: 0.6428 - val_accuracy: 0.6204\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5858 - val_loss: 0.6139 - val_accuracy: 0.6582\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5995 - val_loss: 0.5943 - val_accuracy: 0.6874\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.6117 - val_loss: 0.5814 - val_accuracy: 0.7081\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.6195 - val_loss: 0.5721 - val_accuracy: 0.7199\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.6227 - val_loss: 0.5657 - val_accuracy: 0.7270\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.6323 - val_loss: 0.5610 - val_accuracy: 0.7473\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6408 - val_loss: 0.5584 - val_accuracy: 0.7542\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.6452 - val_loss: 0.5544 - val_accuracy: 0.7585\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6484 - val_loss: 0.5525 - val_accuracy: 0.7621\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.6517 - val_loss: 0.5506 - val_accuracy: 0.7676\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.6557 - val_loss: 0.5497 - val_accuracy: 0.7677\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6591 - val_loss: 0.5480 - val_accuracy: 0.7637\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.6623 - val_loss: 0.5475 - val_accuracy: 0.7664\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.6655 - val_loss: 0.5463 - val_accuracy: 0.7680\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.6680 - val_loss: 0.5460 - val_accuracy: 0.7799\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.6693 - val_loss: 0.5463 - val_accuracy: 0.7850\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6062 - accuracy: 0.6713 - val_loss: 0.5469 - val_accuracy: 0.7785\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6038 - accuracy: 0.6743 - val_loss: 0.5465 - val_accuracy: 0.7748\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6014 - accuracy: 0.6784 - val_loss: 0.5469 - val_accuracy: 0.7781\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.6805 - val_loss: 0.5466 - val_accuracy: 0.7797\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6817 - val_loss: 0.5463 - val_accuracy: 0.7790\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.6818 - val_loss: 0.5472 - val_accuracy: 0.7782\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6836 - val_loss: 0.5459 - val_accuracy: 0.7774\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.6841 - val_loss: 0.5470 - val_accuracy: 0.7764\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6894 - val_loss: 0.5480 - val_accuracy: 0.7738\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.6934 - val_loss: 0.5458 - val_accuracy: 0.7745\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.6946 - val_loss: 0.5463 - val_accuracy: 0.7709\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.6955 - val_loss: 0.5483 - val_accuracy: 0.7689\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6972 - val_loss: 0.5479 - val_accuracy: 0.7697\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5807 - accuracy: 0.6985 - val_loss: 0.5494 - val_accuracy: 0.7676\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.7011 - val_loss: 0.5496 - val_accuracy: 0.7661\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7014 - val_loss: 0.5494 - val_accuracy: 0.7656\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7023 - val_loss: 0.5480 - val_accuracy: 0.7621\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.7032 - val_loss: 0.5492 - val_accuracy: 0.7591\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.7044 - val_loss: 0.5501 - val_accuracy: 0.7525\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.7044 - val_loss: 0.5511 - val_accuracy: 0.7508\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7065 - val_loss: 0.5514 - val_accuracy: 0.7510\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5691 - accuracy: 0.7086 - val_loss: 0.5508 - val_accuracy: 0.7507\n",
      "Epoch 43/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5678 - accuracy: 0.7098 - val_loss: 0.5519 - val_accuracy: 0.7516\n",
      "Epoch 44/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7105 - val_loss: 0.5518 - val_accuracy: 0.7520\n",
      "Epoch 45/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7116 - val_loss: 0.5514 - val_accuracy: 0.7529\n",
      "Epoch 46/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7123 - val_loss: 0.5550 - val_accuracy: 0.7470\n",
      "Epoch 47/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7149 - val_loss: 0.5542 - val_accuracy: 0.7490\n",
      "Epoch 48/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7162 - val_loss: 0.5530 - val_accuracy: 0.7623\n",
      "Epoch 49/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.7168 - val_loss: 0.5537 - val_accuracy: 0.7567\n",
      "Epoch 50/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7185 - val_loss: 0.5563 - val_accuracy: 0.7532\n",
      "Epoch 51/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5580 - accuracy: 0.7234 - val_loss: 0.5546 - val_accuracy: 0.7531\n",
      "Epoch 52/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5569 - accuracy: 0.7250 - val_loss: 0.5572 - val_accuracy: 0.7525\n",
      "Epoch 53/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5557 - accuracy: 0.7256 - val_loss: 0.5589 - val_accuracy: 0.7483\n",
      "Epoch 54/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5547 - accuracy: 0.7274 - val_loss: 0.5568 - val_accuracy: 0.7487\n",
      "Epoch 55/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7277 - val_loss: 0.5583 - val_accuracy: 0.7456\n",
      "Epoch 56/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5525 - accuracy: 0.7299 - val_loss: 0.5563 - val_accuracy: 0.7518\n",
      "Epoch 57/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7293 - val_loss: 0.5579 - val_accuracy: 0.7582\n",
      "Epoch 58/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7301 - val_loss: 0.5589 - val_accuracy: 0.7613\n",
      "Epoch 59/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7312 - val_loss: 0.5588 - val_accuracy: 0.7625\n",
      "Epoch 60/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7325 - val_loss: 0.5600 - val_accuracy: 0.7563\n",
      "Epoch 61/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7330 - val_loss: 0.5595 - val_accuracy: 0.7567\n",
      "Epoch 62/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5461 - accuracy: 0.7324 - val_loss: 0.5607 - val_accuracy: 0.7542\n",
      "Epoch 63/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7334 - val_loss: 0.5597 - val_accuracy: 0.7542\n",
      "Epoch 64/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5441 - accuracy: 0.7342 - val_loss: 0.5611 - val_accuracy: 0.7524\n",
      "Epoch 65/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7339 - val_loss: 0.5600 - val_accuracy: 0.7525\n",
      "Epoch 66/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7347 - val_loss: 0.5643 - val_accuracy: 0.7497\n",
      "Epoch 67/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5410 - accuracy: 0.7352 - val_loss: 0.5627 - val_accuracy: 0.7497\n",
      "Epoch 68/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5400 - accuracy: 0.7358 - val_loss: 0.5633 - val_accuracy: 0.7505\n",
      "Epoch 69/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7360 - val_loss: 0.5653 - val_accuracy: 0.7468\n",
      "Epoch 70/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7365 - val_loss: 0.5642 - val_accuracy: 0.7408\n",
      "Epoch 71/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7372 - val_loss: 0.5655 - val_accuracy: 0.7401\n",
      "Epoch 72/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7372 - val_loss: 0.5640 - val_accuracy: 0.7418\n",
      "Epoch 73/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7379 - val_loss: 0.5648 - val_accuracy: 0.7408\n",
      "Epoch 74/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7380 - val_loss: 0.5672 - val_accuracy: 0.7400\n",
      "Epoch 75/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7395 - val_loss: 0.5667 - val_accuracy: 0.7400\n",
      "Epoch 76/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7404 - val_loss: 0.5668 - val_accuracy: 0.7394\n",
      "Epoch 77/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7406 - val_loss: 0.5680 - val_accuracy: 0.7348\n",
      "Epoch 78/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7415 - val_loss: 0.5676 - val_accuracy: 0.7350\n",
      "Epoch 79/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7414 - val_loss: 0.5696 - val_accuracy: 0.7346\n",
      "Epoch 80/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5286 - accuracy: 0.7418 - val_loss: 0.5707 - val_accuracy: 0.7319\n",
      "Epoch 81/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7421 - val_loss: 0.5680 - val_accuracy: 0.7325\n",
      "Epoch 82/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.7422 - val_loss: 0.5704 - val_accuracy: 0.7289\n",
      "Epoch 83/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7434 - val_loss: 0.5692 - val_accuracy: 0.7292\n",
      "Epoch 84/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7447 - val_loss: 0.5739 - val_accuracy: 0.7272\n",
      "Epoch 85/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7449 - val_loss: 0.5715 - val_accuracy: 0.7294\n",
      "Epoch 86/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7450 - val_loss: 0.5736 - val_accuracy: 0.7265\n",
      "Epoch 87/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7454 - val_loss: 0.5736 - val_accuracy: 0.7257\n",
      "Epoch 88/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7460 - val_loss: 0.5743 - val_accuracy: 0.7257\n",
      "Epoch 89/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7463 - val_loss: 0.5753 - val_accuracy: 0.7243\n",
      "Epoch 90/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7465 - val_loss: 0.5767 - val_accuracy: 0.7251\n",
      "Epoch 91/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7480 - val_loss: 0.5709 - val_accuracy: 0.7300\n",
      "Epoch 92/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7487 - val_loss: 0.5756 - val_accuracy: 0.7263\n",
      "Epoch 93/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7483 - val_loss: 0.5756 - val_accuracy: 0.7243\n",
      "Epoch 94/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7489 - val_loss: 0.5754 - val_accuracy: 0.7245\n",
      "Epoch 95/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7495 - val_loss: 0.5760 - val_accuracy: 0.7261\n",
      "Epoch 96/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.7503 - val_loss: 0.5783 - val_accuracy: 0.7227\n",
      "Epoch 97/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7500 - val_loss: 0.5767 - val_accuracy: 0.7246\n",
      "Epoch 98/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7513 - val_loss: 0.5787 - val_accuracy: 0.7224\n",
      "Epoch 99/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7528 - val_loss: 0.5764 - val_accuracy: 0.7232\n",
      "Epoch 100/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7542 - val_loss: 0.5768 - val_accuracy: 0.7246\n",
      "Epoch 101/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7553 - val_loss: 0.5783 - val_accuracy: 0.7232\n",
      "Epoch 102/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7563 - val_loss: 0.5787 - val_accuracy: 0.7217\n",
      "Epoch 103/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7571 - val_loss: 0.5797 - val_accuracy: 0.7212\n",
      "Epoch 104/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7587 - val_loss: 0.5794 - val_accuracy: 0.7212\n",
      "Epoch 105/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7593 - val_loss: 0.5819 - val_accuracy: 0.7190\n",
      "Epoch 106/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7596 - val_loss: 0.5829 - val_accuracy: 0.7178\n",
      "Epoch 107/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7607 - val_loss: 0.5827 - val_accuracy: 0.7234\n",
      "Epoch 108/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7618 - val_loss: 0.5839 - val_accuracy: 0.7211\n",
      "Epoch 109/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7610 - val_loss: 0.5834 - val_accuracy: 0.7238\n",
      "Epoch 110/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.7625 - val_loss: 0.5829 - val_accuracy: 0.7211\n",
      "Epoch 111/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7626 - val_loss: 0.5858 - val_accuracy: 0.7206\n",
      "Epoch 112/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7627 - val_loss: 0.5848 - val_accuracy: 0.7206\n",
      "Epoch 113/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7636 - val_loss: 0.5838 - val_accuracy: 0.7207\n",
      "Epoch 114/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7650 - val_loss: 0.5852 - val_accuracy: 0.7206\n",
      "Epoch 115/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7656 - val_loss: 0.5887 - val_accuracy: 0.7178\n",
      "Epoch 116/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7680 - val_loss: 0.5868 - val_accuracy: 0.7204\n",
      "Epoch 117/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7693 - val_loss: 0.5875 - val_accuracy: 0.7144\n",
      "Epoch 118/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7701 - val_loss: 0.5866 - val_accuracy: 0.7192\n",
      "Epoch 119/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7706 - val_loss: 0.5876 - val_accuracy: 0.7135\n",
      "Epoch 120/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.7713 - val_loss: 0.5891 - val_accuracy: 0.7079\n",
      "Epoch 121/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7719 - val_loss: 0.5890 - val_accuracy: 0.7105\n",
      "Epoch 122/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7725 - val_loss: 0.5910 - val_accuracy: 0.6987\n",
      "Epoch 123/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7727 - val_loss: 0.5872 - val_accuracy: 0.7018\n",
      "Epoch 124/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7728 - val_loss: 0.5917 - val_accuracy: 0.7001\n",
      "Epoch 125/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7735 - val_loss: 0.5920 - val_accuracy: 0.6999\n",
      "Epoch 126/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7740 - val_loss: 0.5950 - val_accuracy: 0.6966\n",
      "Epoch 127/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7748 - val_loss: 0.5936 - val_accuracy: 0.6983\n",
      "Epoch 128/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4898 - accuracy: 0.7744 - val_loss: 0.5958 - val_accuracy: 0.6968\n",
      "Epoch 129/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7754 - val_loss: 0.5937 - val_accuracy: 0.6987\n",
      "Epoch 130/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4884 - accuracy: 0.7755 - val_loss: 0.5943 - val_accuracy: 0.6969\n",
      "Epoch 131/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7768 - val_loss: 0.5963 - val_accuracy: 0.6967\n",
      "Epoch 132/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.7764 - val_loss: 0.5968 - val_accuracy: 0.6969\n",
      "Epoch 133/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7768 - val_loss: 0.5963 - val_accuracy: 0.6969\n",
      "Epoch 134/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7782 - val_loss: 0.5978 - val_accuracy: 0.6964\n",
      "Epoch 135/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7781 - val_loss: 0.5975 - val_accuracy: 0.6964\n",
      "Epoch 136/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7783 - val_loss: 0.5995 - val_accuracy: 0.6967\n",
      "Epoch 137/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7781 - val_loss: 0.6016 - val_accuracy: 0.6961\n",
      "Epoch 138/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7793 - val_loss: 0.5997 - val_accuracy: 0.6964\n",
      "Epoch 139/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7799 - val_loss: 0.6007 - val_accuracy: 0.6991\n",
      "Epoch 140/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7804 - val_loss: 0.5992 - val_accuracy: 0.7011\n",
      "Epoch 141/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7805 - val_loss: 0.6000 - val_accuracy: 0.7003\n",
      "Epoch 142/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7808 - val_loss: 0.6010 - val_accuracy: 0.6962\n",
      "Epoch 143/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7809 - val_loss: 0.6017 - val_accuracy: 0.6959\n",
      "Epoch 144/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7816 - val_loss: 0.6013 - val_accuracy: 0.6971\n",
      "Epoch 145/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7825 - val_loss: 0.6044 - val_accuracy: 0.6953\n",
      "Epoch 146/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7831 - val_loss: 0.6038 - val_accuracy: 0.6954\n",
      "Epoch 147/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7825 - val_loss: 0.6052 - val_accuracy: 0.6951\n",
      "Epoch 148/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7838 - val_loss: 0.6073 - val_accuracy: 0.6950\n",
      "Epoch 149/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4756 - accuracy: 0.7839 - val_loss: 0.6066 - val_accuracy: 0.6930\n",
      "Epoch 150/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7841 - val_loss: 0.6106 - val_accuracy: 0.6898\n",
      "Epoch 151/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7844 - val_loss: 0.6081 - val_accuracy: 0.6930\n",
      "Epoch 152/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7847 - val_loss: 0.6065 - val_accuracy: 0.6941\n",
      "Epoch 153/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7851 - val_loss: 0.6079 - val_accuracy: 0.6905\n",
      "Epoch 154/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7853 - val_loss: 0.6086 - val_accuracy: 0.6914\n",
      "Epoch 155/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7858 - val_loss: 0.6111 - val_accuracy: 0.6893\n",
      "Epoch 156/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7856 - val_loss: 0.6143 - val_accuracy: 0.6891\n",
      "Epoch 157/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7850 - val_loss: 0.6116 - val_accuracy: 0.6899\n",
      "Epoch 158/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.7855 - val_loss: 0.6120 - val_accuracy: 0.6903\n",
      "Epoch 159/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7858 - val_loss: 0.6139 - val_accuracy: 0.6899\n",
      "Epoch 160/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.7861 - val_loss: 0.6124 - val_accuracy: 0.6901\n",
      "Epoch 161/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4683 - accuracy: 0.7860 - val_loss: 0.6106 - val_accuracy: 0.6893\n",
      "Epoch 162/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4677 - accuracy: 0.7867 - val_loss: 0.6120 - val_accuracy: 0.6891\n",
      "Epoch 163/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4671 - accuracy: 0.7868 - val_loss: 0.6122 - val_accuracy: 0.6891\n",
      "Epoch 164/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7869 - val_loss: 0.6115 - val_accuracy: 0.6903\n",
      "Epoch 165/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7869 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 166/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.7874 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 167/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7872 - val_loss: 0.6150 - val_accuracy: 0.6903\n",
      "Epoch 168/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7875 - val_loss: 0.6164 - val_accuracy: 0.6910\n",
      "Epoch 169/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7882 - val_loss: 0.6179 - val_accuracy: 0.6906\n",
      "Epoch 170/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7902 - val_loss: 0.6202 - val_accuracy: 0.6882\n",
      "Epoch 171/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7898 - val_loss: 0.6174 - val_accuracy: 0.6890\n",
      "Epoch 172/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.7903 - val_loss: 0.6191 - val_accuracy: 0.6882\n",
      "Epoch 173/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7913 - val_loss: 0.6213 - val_accuracy: 0.6893\n",
      "Epoch 174/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7913 - val_loss: 0.6214 - val_accuracy: 0.6891\n",
      "Epoch 175/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7922 - val_loss: 0.6201 - val_accuracy: 0.6898\n",
      "Epoch 176/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7923 - val_loss: 0.6239 - val_accuracy: 0.6895\n",
      "Epoch 177/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7935 - val_loss: 0.6214 - val_accuracy: 0.6895\n",
      "Epoch 178/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.7932 - val_loss: 0.6228 - val_accuracy: 0.6896\n",
      "Epoch 179/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7933 - val_loss: 0.6212 - val_accuracy: 0.6906\n",
      "Epoch 180/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.7933 - val_loss: 0.6271 - val_accuracy: 0.6895\n",
      "Epoch 181/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.7938 - val_loss: 0.6245 - val_accuracy: 0.6901\n",
      "Epoch 182/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7940 - val_loss: 0.6219 - val_accuracy: 0.6905\n",
      "Epoch 183/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.7942 - val_loss: 0.6256 - val_accuracy: 0.6901\n",
      "Epoch 184/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.7939 - val_loss: 0.6269 - val_accuracy: 0.6901\n",
      "Epoch 185/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7946 - val_loss: 0.6271 - val_accuracy: 0.6858\n",
      "Epoch 186/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7945 - val_loss: 0.6297 - val_accuracy: 0.6856\n",
      "Epoch 187/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7948 - val_loss: 0.6295 - val_accuracy: 0.6855\n",
      "Epoch 188/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7950 - val_loss: 0.6252 - val_accuracy: 0.6875\n",
      "Epoch 189/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7950 - val_loss: 0.6286 - val_accuracy: 0.6856\n",
      "Epoch 190/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4524 - accuracy: 0.7962 - val_loss: 0.6276 - val_accuracy: 0.6871\n",
      "Epoch 191/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4519 - accuracy: 0.7970 - val_loss: 0.6244 - val_accuracy: 0.6886\n",
      "Epoch 192/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.7965 - val_loss: 0.6283 - val_accuracy: 0.6880\n",
      "Epoch 193/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.7978 - val_loss: 0.6303 - val_accuracy: 0.6860\n",
      "Epoch 194/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.7975 - val_loss: 0.6311 - val_accuracy: 0.6859\n",
      "Epoch 195/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7980 - val_loss: 0.6320 - val_accuracy: 0.6842\n",
      "Epoch 196/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.7986 - val_loss: 0.6301 - val_accuracy: 0.6858\n",
      "Epoch 197/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.7985 - val_loss: 0.6314 - val_accuracy: 0.6851\n",
      "Epoch 198/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.7983 - val_loss: 0.6339 - val_accuracy: 0.6822\n",
      "Epoch 199/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7989 - val_loss: 0.6315 - val_accuracy: 0.6825\n",
      "Epoch 200/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.7993 - val_loss: 0.6360 - val_accuracy: 0.6806\n",
      "Epoch 201/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7996 - val_loss: 0.6345 - val_accuracy: 0.6807\n",
      "Epoch 202/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.7999 - val_loss: 0.6339 - val_accuracy: 0.6820\n",
      "Epoch 203/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8004 - val_loss: 0.6335 - val_accuracy: 0.6820\n",
      "Epoch 204/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8010 - val_loss: 0.6365 - val_accuracy: 0.6819\n",
      "Epoch 205/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4450 - accuracy: 0.8016 - val_loss: 0.6351 - val_accuracy: 0.6819\n",
      "Epoch 206/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.8014 - val_loss: 0.6342 - val_accuracy: 0.6820\n",
      "Epoch 207/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.8021 - val_loss: 0.6405 - val_accuracy: 0.6791\n",
      "Epoch 208/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8019 - val_loss: 0.6407 - val_accuracy: 0.6803\n",
      "Epoch 209/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8015 - val_loss: 0.6377 - val_accuracy: 0.6805\n",
      "Epoch 210/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.8020 - val_loss: 0.6388 - val_accuracy: 0.6804\n",
      "Epoch 211/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4423 - accuracy: 0.8025 - val_loss: 0.6409 - val_accuracy: 0.6791\n",
      "Epoch 212/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8021 - val_loss: 0.6391 - val_accuracy: 0.6798\n",
      "Epoch 213/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8028 - val_loss: 0.6395 - val_accuracy: 0.6798\n",
      "Epoch 214/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8024 - val_loss: 0.6405 - val_accuracy: 0.6797\n",
      "Epoch 215/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8031 - val_loss: 0.6416 - val_accuracy: 0.6789\n",
      "Epoch 216/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8028 - val_loss: 0.6439 - val_accuracy: 0.6782\n",
      "Epoch 217/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8029 - val_loss: 0.6439 - val_accuracy: 0.6781\n",
      "Epoch 218/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8029 - val_loss: 0.6433 - val_accuracy: 0.6789\n",
      "Epoch 219/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8042 - val_loss: 0.6425 - val_accuracy: 0.6785\n",
      "Epoch 220/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.8039 - val_loss: 0.6441 - val_accuracy: 0.6783\n",
      "Epoch 221/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8049 - val_loss: 0.6470 - val_accuracy: 0.6706\n",
      "Epoch 222/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.8048 - val_loss: 0.6464 - val_accuracy: 0.6777\n",
      "Epoch 223/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8050 - val_loss: 0.6464 - val_accuracy: 0.6645\n",
      "Epoch 224/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8053 - val_loss: 0.6453 - val_accuracy: 0.6588\n",
      "Epoch 225/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8060 - val_loss: 0.6495 - val_accuracy: 0.6573\n",
      "Epoch 226/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.8059 - val_loss: 0.6505 - val_accuracy: 0.6572\n",
      "Epoch 227/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8057 - val_loss: 0.6480 - val_accuracy: 0.6610\n",
      "Epoch 228/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8058 - val_loss: 0.6475 - val_accuracy: 0.6620\n",
      "Epoch 229/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8063 - val_loss: 0.6517 - val_accuracy: 0.6606\n",
      "Epoch 230/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8061 - val_loss: 0.6495 - val_accuracy: 0.6614\n",
      "Epoch 231/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8063 - val_loss: 0.6503 - val_accuracy: 0.6608\n",
      "Epoch 232/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8073 - val_loss: 0.6509 - val_accuracy: 0.6596\n",
      "Epoch 233/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8069 - val_loss: 0.6537 - val_accuracy: 0.6596\n",
      "Epoch 234/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8073 - val_loss: 0.6562 - val_accuracy: 0.6584\n",
      "Epoch 235/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8076 - val_loss: 0.6542 - val_accuracy: 0.6589\n",
      "Epoch 236/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8079 - val_loss: 0.6517 - val_accuracy: 0.6630\n",
      "Epoch 237/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8090 - val_loss: 0.6545 - val_accuracy: 0.6609\n",
      "Epoch 238/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8089 - val_loss: 0.6568 - val_accuracy: 0.6614\n",
      "Epoch 239/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8094 - val_loss: 0.6562 - val_accuracy: 0.6618\n",
      "Epoch 240/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8103 - val_loss: 0.6611 - val_accuracy: 0.6580\n",
      "Epoch 241/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8099 - val_loss: 0.6606 - val_accuracy: 0.6590\n",
      "Epoch 242/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8106 - val_loss: 0.6580 - val_accuracy: 0.6599\n",
      "Epoch 243/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8107 - val_loss: 0.6607 - val_accuracy: 0.6594\n",
      "Epoch 244/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.8113 - val_loss: 0.6603 - val_accuracy: 0.6593\n",
      "Epoch 245/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8106 - val_loss: 0.6605 - val_accuracy: 0.6593\n",
      "Epoch 246/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8110 - val_loss: 0.6584 - val_accuracy: 0.6596\n",
      "Epoch 247/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8113 - val_loss: 0.6614 - val_accuracy: 0.6594\n",
      "Epoch 248/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8112 - val_loss: 0.6645 - val_accuracy: 0.6583\n",
      "Epoch 249/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8114 - val_loss: 0.6619 - val_accuracy: 0.6594\n",
      "Epoch 250/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8114 - val_loss: 0.6651 - val_accuracy: 0.6596\n"
     ]
    }
   ],
   "source": [
    "model1a_fit = model1.fit(x=X_train, y=y_train, epochs=250, batch_size=2**8, validation_split=0.25)\n",
    "\n",
    "loss_valloss_difference_1c = \"The training loss keeps going down the more epochs we run. The validation loss goes down in\\\n",
    "    the beginning but after a while it starts increasing. This is because the model starts overfitting, making it perform\\\n",
    "        worse on new data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189128-f787-4b8c-8c96-6b782065dad7",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "In Lecture 9 we used early stopping to avoid overfitting. Apply this here, with `patience` argument set to 20, a new model `model1b` which otherwise should have a setup identical to `model1`. In which epoch did the model training procedure stop? Figure this out by counting the number of elements in `model1b_fit.history['loss']` and write your answer into the string variable `when_earlystop_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f13f2cb-b29b-4288-9b8d-e69f331faff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 0.7777 - accuracy: 0.4800 - val_loss: 0.8446 - val_accuracy: 0.3018\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7399 - accuracy: 0.4967 - val_loss: 0.7632 - val_accuracy: 0.4357\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7144 - accuracy: 0.5259 - val_loss: 0.7054 - val_accuracy: 0.5207\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.5480 - val_loss: 0.6647 - val_accuracy: 0.5753\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5724 - val_loss: 0.6366 - val_accuracy: 0.6321\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.5979 - val_loss: 0.6164 - val_accuracy: 0.6610\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6058 - val_loss: 0.6021 - val_accuracy: 0.6817\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6105 - val_loss: 0.5914 - val_accuracy: 0.6880\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6197 - val_loss: 0.5833 - val_accuracy: 0.7119\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.6282 - val_loss: 0.5765 - val_accuracy: 0.7303\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6395 - accuracy: 0.6349 - val_loss: 0.5725 - val_accuracy: 0.7410\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.6398 - val_loss: 0.5676 - val_accuracy: 0.7534\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6303 - accuracy: 0.6445 - val_loss: 0.5636 - val_accuracy: 0.7557\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6439 - val_loss: 0.5604 - val_accuracy: 0.7553\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.6482 - val_loss: 0.5581 - val_accuracy: 0.7510\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.6583 - val_loss: 0.5560 - val_accuracy: 0.7496\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.6618 - val_loss: 0.5542 - val_accuracy: 0.7523\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.6653 - val_loss: 0.5531 - val_accuracy: 0.7507\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.6675 - val_loss: 0.5523 - val_accuracy: 0.7488\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.6696 - val_loss: 0.5523 - val_accuracy: 0.7444\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6715 - val_loss: 0.5526 - val_accuracy: 0.7421\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.6752 - val_loss: 0.5522 - val_accuracy: 0.7446\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5996 - accuracy: 0.6785 - val_loss: 0.5529 - val_accuracy: 0.7516\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6823 - val_loss: 0.5522 - val_accuracy: 0.7515\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.6869 - val_loss: 0.5526 - val_accuracy: 0.7513\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6895 - val_loss: 0.5539 - val_accuracy: 0.7509\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6919 - val_loss: 0.5532 - val_accuracy: 0.7478\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6948 - val_loss: 0.5549 - val_accuracy: 0.7300\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5871 - accuracy: 0.6956 - val_loss: 0.5558 - val_accuracy: 0.7291\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.6954 - val_loss: 0.5556 - val_accuracy: 0.7280\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.6964 - val_loss: 0.5559 - val_accuracy: 0.7261\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.6969 - val_loss: 0.5583 - val_accuracy: 0.7240\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.6987 - val_loss: 0.5586 - val_accuracy: 0.7223\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.7004 - val_loss: 0.5593 - val_accuracy: 0.7234\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.7044 - val_loss: 0.5605 - val_accuracy: 0.7183\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7057 - val_loss: 0.5605 - val_accuracy: 0.7170\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7070 - val_loss: 0.5597 - val_accuracy: 0.7194\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5729 - accuracy: 0.7077 - val_loss: 0.5620 - val_accuracy: 0.7170\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.7088 - val_loss: 0.5625 - val_accuracy: 0.7182\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7089 - val_loss: 0.5635 - val_accuracy: 0.7157\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7103 - val_loss: 0.5642 - val_accuracy: 0.7090\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7128 - val_loss: 0.5633 - val_accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define architecture here\n",
    "model1b = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model1b.add(layers.Dense(30, input_dim = X_train.shape[1], activation = \"ReLU\"))\n",
    "model1b.add(layers.Dense(15, activation = \"ReLU\"))\n",
    "model1b.add(layers.Dense(1, activation= \"sigmoid\"))\n",
    "\n",
    "# Compile model here \n",
    "model1b.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit model here\n",
    "model1b_fit = model1b.fit(x=X_train, \n",
    "                         y=y_train, \n",
    "                         epochs=250, \n",
    "                         batch_size=2**8, \n",
    "                         validation_split=0.25, \n",
    "                         callbacks=[EarlyStopping(patience=20)])\n",
    "\n",
    "\n",
    "when_earlystop_1d = f\"The number of elements is {len(model1b_fit.epoch)}, this is the epoch when training stops \\nmeaning that validation loss has not decreased for the last 20 epochs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd9a95a-7770-493e-9596-52d9ee9aeefc",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "Even though we haven't finished training our neural net, let us use the `evaluate()` function to measure the predictive performance of `model1b` on the test data. Save the result as `res_model1`. \n",
    "\n",
    "What is the accuracy of the model for validation training data and test data, respectively? What is the difference in accuracy? Save your answer in the string variable `difference_in_accuracy_1e`.\n",
    "*Hint:* the training validation accuracy can be extracted from `model1b_fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb727607-775d-44a3-a437-de1d7db42459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5708505511283875, 0.7084632515907288]\n"
     ]
    }
   ],
   "source": [
    "res_model1 = model1b.evaluate(X_test, y_test, verbose=0)\n",
    "print(res_model1)\n",
    "difference_in_accuracy_1e = f\"The accuracy on validation data is {model1b_fit.history['val_accuracy'][-1]} \\nThe accuracy on test data is {res_model1[1]} \\nThe difference is {model1b_fit.history['val_accuracy'][-1] - res_model1[1]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18fcba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on validation data is 0.7100445032119751 \n",
      "The accuracy on test data is 0.7084632515907288 \n",
      "The difference is 0.0015812516212463379\n"
     ]
    }
   ],
   "source": [
    "print(difference_in_accuracy_1e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef98a6a-2952-4733-a42e-45a44cdcc7f9",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "Now we use the `confusion_matrix()` function from the `metrics` module of scikit-learn to disaggregate model performance to class-specific performance. First, get class predictions on the test data using `predict()`. Save these as `prob_model1`. \n",
    "Second, use `confusion_matrix()` to get a confusion matrix and save it as `CM_model1`. Third, calculate true positive rate and false positive rate and save them as `TPR_1f` and `FPR_1f`. \n",
    "Do your results on TPR and FPR suggest that prediction accuracy is approximately equal in both categories? Write your (specific!) answer into the string variable `categorywise_accuracy_1f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76d16957-1fcb-48b0-891c-9b778149090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 742us/step\n",
      "[[10416  2465]\n",
      " [ 2864  2534]]\n",
      "0.46943312337902926\n",
      "0.1913671298812204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prob_model1 = model1b.predict(X_test)\n",
    "CM_model1   = confusion_matrix(y_test, prob_model1 > 0.5)\n",
    "\n",
    "TN = CM_model1[0,0]\n",
    "TP = CM_model1[1,1]\n",
    "FN = CM_model1[1,0]\n",
    "FP = CM_model1[0,1]\n",
    "\n",
    "\n",
    "TPR_1f = TP/(TP+FN)\n",
    "FPR_1f = FP/(FP+TN)\n",
    "\n",
    "print(CM_model1)\n",
    "print(TPR_1f)\n",
    "print(FPR_1f)\n",
    "\n",
    "categorywise_accuracy_1f = \"The model is quite a lot better at predicting negatives (~80%) than positives (~47%) which is \\n likely due to the fact that there are way more negatives in the dataset than there are positives. So, depending on our \\ngoal we would for example suggest decreasing the decision boundary if we want to correctly predict the positives more often. However this also increases the FPR.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6fa2-89ad-4d87-bc6e-5031b13b1f7b",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "In the lectures we have utilized explicit regularization to avoid overfitting. Here we will use $\\ell_2$ regularization to update the weights. Create the architecture of a new neural net `model2` which is identical to that of `model1b` except for $l_2$ regularization with regularization factor `l2_pen` in the two hidden layers.  \n",
    "\n",
    "Then compile and fit this regularized model with the same parameters as in Task 1d. Save the trained neural net as `model2_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7505b575-e5e1-4d4d-b42b-70960a1222d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "161/161 [==============================] - 1s 2ms/step - loss: 1.0621 - accuracy: 0.5544 - val_loss: 0.8882 - val_accuracy: 0.7022\n",
      "Epoch 2/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0345 - accuracy: 0.5581 - val_loss: 0.8819 - val_accuracy: 0.7027\n",
      "Epoch 3/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0111 - accuracy: 0.5732 - val_loss: 0.8729 - val_accuracy: 0.7101\n",
      "Epoch 4/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9909 - accuracy: 0.5831 - val_loss: 0.8629 - val_accuracy: 0.7189\n",
      "Epoch 5/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9730 - accuracy: 0.5936 - val_loss: 0.8523 - val_accuracy: 0.7371\n",
      "Epoch 6/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.6056 - val_loss: 0.8428 - val_accuracy: 0.7432\n",
      "Epoch 7/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9421 - accuracy: 0.6157 - val_loss: 0.8337 - val_accuracy: 0.7662\n",
      "Epoch 8/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9286 - accuracy: 0.6219 - val_loss: 0.8250 - val_accuracy: 0.7669\n",
      "Epoch 9/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9160 - accuracy: 0.6287 - val_loss: 0.8166 - val_accuracy: 0.7777\n",
      "Epoch 10/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9043 - accuracy: 0.6349 - val_loss: 0.8087 - val_accuracy: 0.7924\n",
      "Epoch 11/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8931 - accuracy: 0.6416 - val_loss: 0.8020 - val_accuracy: 0.8048\n",
      "Epoch 12/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8826 - accuracy: 0.6440 - val_loss: 0.7940 - val_accuracy: 0.8075\n",
      "Epoch 13/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8727 - accuracy: 0.6495 - val_loss: 0.7866 - val_accuracy: 0.8111\n",
      "Epoch 14/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8633 - accuracy: 0.6553 - val_loss: 0.7796 - val_accuracy: 0.8219\n",
      "Epoch 15/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8544 - accuracy: 0.6593 - val_loss: 0.7736 - val_accuracy: 0.8222\n",
      "Epoch 16/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8459 - accuracy: 0.6595 - val_loss: 0.7673 - val_accuracy: 0.8305\n",
      "Epoch 17/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8377 - accuracy: 0.6604 - val_loss: 0.7610 - val_accuracy: 0.8282\n",
      "Epoch 18/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.6615 - val_loss: 0.7550 - val_accuracy: 0.8268\n",
      "Epoch 19/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8222 - accuracy: 0.6643 - val_loss: 0.7492 - val_accuracy: 0.8276\n",
      "Epoch 20/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8149 - accuracy: 0.6679 - val_loss: 0.7444 - val_accuracy: 0.8233\n",
      "Epoch 21/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8079 - accuracy: 0.6696 - val_loss: 0.7399 - val_accuracy: 0.8211\n",
      "Epoch 22/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8013 - accuracy: 0.6729 - val_loss: 0.7349 - val_accuracy: 0.8174\n",
      "Epoch 23/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7949 - accuracy: 0.6728 - val_loss: 0.7307 - val_accuracy: 0.8163\n",
      "Epoch 24/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7887 - accuracy: 0.6767 - val_loss: 0.7261 - val_accuracy: 0.8162\n",
      "Epoch 25/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7828 - accuracy: 0.6776 - val_loss: 0.7214 - val_accuracy: 0.8152\n",
      "Epoch 26/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.7771 - accuracy: 0.6807 - val_loss: 0.7172 - val_accuracy: 0.8156\n",
      "Epoch 27/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7716 - accuracy: 0.6828 - val_loss: 0.7124 - val_accuracy: 0.8241\n",
      "Epoch 28/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7663 - accuracy: 0.6835 - val_loss: 0.7093 - val_accuracy: 0.8219\n",
      "Epoch 29/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7611 - accuracy: 0.6833 - val_loss: 0.7060 - val_accuracy: 0.8207\n",
      "Epoch 30/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7561 - accuracy: 0.6843 - val_loss: 0.7018 - val_accuracy: 0.8224\n",
      "Epoch 31/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7513 - accuracy: 0.6835 - val_loss: 0.6979 - val_accuracy: 0.8184\n",
      "Epoch 32/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7466 - accuracy: 0.6851 - val_loss: 0.6956 - val_accuracy: 0.8171\n",
      "Epoch 33/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7421 - accuracy: 0.6856 - val_loss: 0.6922 - val_accuracy: 0.8160\n",
      "Epoch 34/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7378 - accuracy: 0.6876 - val_loss: 0.6894 - val_accuracy: 0.8131\n",
      "Epoch 35/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7336 - accuracy: 0.6885 - val_loss: 0.6874 - val_accuracy: 0.8104\n",
      "Epoch 36/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7296 - accuracy: 0.6904 - val_loss: 0.6846 - val_accuracy: 0.8088\n",
      "Epoch 37/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7256 - accuracy: 0.6919 - val_loss: 0.6814 - val_accuracy: 0.8065\n",
      "Epoch 38/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7219 - accuracy: 0.6917 - val_loss: 0.6799 - val_accuracy: 0.8052\n",
      "Epoch 39/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7183 - accuracy: 0.6931 - val_loss: 0.6782 - val_accuracy: 0.8047\n",
      "Epoch 40/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7149 - accuracy: 0.6945 - val_loss: 0.6767 - val_accuracy: 0.8049\n",
      "Epoch 41/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.6954 - val_loss: 0.6745 - val_accuracy: 0.8047\n",
      "Epoch 42/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7082 - accuracy: 0.6966 - val_loss: 0.6714 - val_accuracy: 0.8046\n",
      "Epoch 43/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7050 - accuracy: 0.6982 - val_loss: 0.6700 - val_accuracy: 0.8039\n",
      "Epoch 44/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7019 - accuracy: 0.6987 - val_loss: 0.6677 - val_accuracy: 0.8034\n",
      "Epoch 45/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6989 - accuracy: 0.7004 - val_loss: 0.6655 - val_accuracy: 0.8023\n",
      "Epoch 46/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.7017 - val_loss: 0.6656 - val_accuracy: 0.7990\n",
      "Epoch 47/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.7027 - val_loss: 0.6637 - val_accuracy: 0.7987\n",
      "Epoch 48/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.7038 - val_loss: 0.6613 - val_accuracy: 0.7987\n",
      "Epoch 49/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.7047 - val_loss: 0.6597 - val_accuracy: 0.7992\n",
      "Epoch 50/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.7051 - val_loss: 0.6601 - val_accuracy: 0.7942\n",
      "Epoch 51/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.7063 - val_loss: 0.6572 - val_accuracy: 0.7920\n",
      "Epoch 52/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6800 - accuracy: 0.7077 - val_loss: 0.6566 - val_accuracy: 0.7914\n",
      "Epoch 53/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.7086 - val_loss: 0.6560 - val_accuracy: 0.7860\n",
      "Epoch 54/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6752 - accuracy: 0.7121 - val_loss: 0.6542 - val_accuracy: 0.7856\n",
      "Epoch 55/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.7148 - val_loss: 0.6533 - val_accuracy: 0.7899\n",
      "Epoch 56/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.7163 - val_loss: 0.6503 - val_accuracy: 0.7901\n",
      "Epoch 57/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.7160 - val_loss: 0.6505 - val_accuracy: 0.7901\n",
      "Epoch 58/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.7162 - val_loss: 0.6495 - val_accuracy: 0.7896\n",
      "Epoch 59/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.7159 - val_loss: 0.6486 - val_accuracy: 0.7861\n",
      "Epoch 60/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.7177 - val_loss: 0.6481 - val_accuracy: 0.7791\n",
      "Epoch 61/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.7177 - val_loss: 0.6466 - val_accuracy: 0.7801\n",
      "Epoch 62/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.7190 - val_loss: 0.6462 - val_accuracy: 0.7783\n",
      "Epoch 63/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.7195 - val_loss: 0.6442 - val_accuracy: 0.7784\n",
      "Epoch 64/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.7183 - val_loss: 0.6444 - val_accuracy: 0.7698\n",
      "Epoch 65/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6520 - accuracy: 0.7198 - val_loss: 0.6431 - val_accuracy: 0.7698\n",
      "Epoch 66/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.7211 - val_loss: 0.6437 - val_accuracy: 0.7711\n",
      "Epoch 67/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.7226 - val_loss: 0.6425 - val_accuracy: 0.7706\n",
      "Epoch 68/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.7241 - val_loss: 0.6428 - val_accuracy: 0.7653\n",
      "Epoch 69/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.7235 - val_loss: 0.6426 - val_accuracy: 0.7656\n",
      "Epoch 70/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.7245 - val_loss: 0.6403 - val_accuracy: 0.7715\n",
      "Epoch 71/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.7242 - val_loss: 0.6414 - val_accuracy: 0.7658\n",
      "Epoch 72/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.7241 - val_loss: 0.6397 - val_accuracy: 0.7683\n",
      "Epoch 73/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.7248 - val_loss: 0.6395 - val_accuracy: 0.7672\n",
      "Epoch 74/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.7256 - val_loss: 0.6410 - val_accuracy: 0.7583\n",
      "Epoch 75/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.7271 - val_loss: 0.6402 - val_accuracy: 0.7468\n",
      "Epoch 76/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.7273 - val_loss: 0.6396 - val_accuracy: 0.7437\n",
      "Epoch 77/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.7274 - val_loss: 0.6411 - val_accuracy: 0.7430\n",
      "Epoch 78/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.7281 - val_loss: 0.6393 - val_accuracy: 0.7427\n",
      "Epoch 79/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.7290 - val_loss: 0.6400 - val_accuracy: 0.7418\n",
      "Epoch 80/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.7298 - val_loss: 0.6400 - val_accuracy: 0.7419\n",
      "Epoch 81/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.7319 - val_loss: 0.6378 - val_accuracy: 0.7420\n",
      "Epoch 82/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.7312 - val_loss: 0.6393 - val_accuracy: 0.7415\n",
      "Epoch 83/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.7328 - val_loss: 0.6386 - val_accuracy: 0.7358\n",
      "Epoch 84/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.7337 - val_loss: 0.6410 - val_accuracy: 0.7337\n",
      "Epoch 85/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6179 - accuracy: 0.7342 - val_loss: 0.6403 - val_accuracy: 0.7321\n",
      "Epoch 86/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.7348 - val_loss: 0.6404 - val_accuracy: 0.7321\n",
      "Epoch 87/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.7360 - val_loss: 0.6403 - val_accuracy: 0.7321\n",
      "Epoch 88/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.7366 - val_loss: 0.6414 - val_accuracy: 0.7313\n",
      "Epoch 89/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6117 - accuracy: 0.7365 - val_loss: 0.6414 - val_accuracy: 0.7313\n",
      "Epoch 90/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6102 - accuracy: 0.7384 - val_loss: 0.6424 - val_accuracy: 0.7311\n",
      "Epoch 91/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6087 - accuracy: 0.7392 - val_loss: 0.6381 - val_accuracy: 0.7469\n",
      "Epoch 92/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6073 - accuracy: 0.7411 - val_loss: 0.6423 - val_accuracy: 0.7351\n",
      "Epoch 93/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.7416 - val_loss: 0.6407 - val_accuracy: 0.7424\n",
      "Epoch 94/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.7426 - val_loss: 0.6431 - val_accuracy: 0.7408\n",
      "Epoch 95/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.7447 - val_loss: 0.6433 - val_accuracy: 0.7404\n",
      "Epoch 96/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.7446 - val_loss: 0.6454 - val_accuracy: 0.7373\n",
      "Epoch 97/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5999 - accuracy: 0.7453 - val_loss: 0.6433 - val_accuracy: 0.7398\n",
      "Epoch 98/250\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.7464 - val_loss: 0.6458 - val_accuracy: 0.7363\n",
      "Epoch 99/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.7462 - val_loss: 0.6450 - val_accuracy: 0.7292\n",
      "Epoch 100/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5956 - accuracy: 0.7473 - val_loss: 0.6445 - val_accuracy: 0.7348\n",
      "Epoch 101/250\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.7487 - val_loss: 0.6462 - val_accuracy: 0.7363\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "l2_pen = 0.005\n",
    "\n",
    "# 1.\n",
    "model2 = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model2.add(layers.Dense(30, input_dim = X_train.shape[1], activation = \"ReLU\", kernel_regularizer=l2(l2_pen)))\n",
    "model2.add(layers.Dense(15, activation = \"ReLU\", kernel_regularizer=l2(l2_pen)))\n",
    "model2.add(layers.Dense(1, activation= \"sigmoid\"))\n",
    "\n",
    "# 2.\n",
    "model2.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=[\"accuracy\"])\n",
    "\n",
    "model2_fit = model2.fit(x=X_train, \n",
    "                         y=y_train, \n",
    "                         epochs=250, \n",
    "                         batch_size=2**8, \n",
    "                         validation_split=0.25, \n",
    "                         callbacks=[EarlyStopping(patience=20)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a8d24-9b08-48fe-b120-ca68106406fb",
   "metadata": {},
   "source": [
    "### Task 1h)\n",
    "In Task 1e) we compared the prediction accuracy on test and training sets. However, this is bad measure when the data is not well balanced (in terms of the observed output categories). Instead, one can use the cross entropy for the binomial distribution (minus the average log likelihood of the model). In fact, we chose this function as loss function for model training when we compiled `model1` and `model2`.\n",
    "\n",
    "To compare the test error of `model2` to that of `model1b` we don't want to use `loss` from `evaluate` since this includes the $\\ell_2$ penalty.  In the library `MLmetrics` the function `log_loss()` computes the cross entropy for the binomial distribution without penalty term. \n",
    "\n",
    "First, get predicted output probabilities on test data from `model2` and save them as `prob_model2`.\n",
    "Second, use `log_loss()` from the metrics module in scikit-learn to compute the cross-entropy loss for `model2` on the test data and save it as `logloss_model2`. \n",
    "Third, use the string variable `performance_comparison_1h` to describe how the accuracy on test data differs between `model1b` and `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "023a39ea-f599-4f1d-9ca0-4e54aa598cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - 0s 824us/step\n",
      "0.5360120591448864\n",
      "572/572 [==============================] - 1s 967us/step - loss: 0.6129 - accuracy: 0.7376\n",
      "\n",
      "The accuracy of model2 on test data is 0.737568 \n",
      "The accuracy of model1b on test data is 0.708463 \n",
      "The difference is 0.0291045, so ~3 percentage units\n",
      "\n",
      "So we can see that our model with the l2 penalty is actually quite a bit more accurate and the loss is lower which is also nice.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. \n",
    "prob_model2    = model2.predict(X_test)\n",
    "\n",
    "# 2.\n",
    "logloss_model2 = log_loss(y_test, prob_model2)\n",
    "print(logloss_model2)\n",
    "\n",
    "# 3.\n",
    "accuracy_model2 = model2.evaluate(X_test, y_test)[1]\n",
    "performance_comparison_1h = f\"\\nThe accuracy of model2 on test data is {accuracy_model2:.6} \\nThe accuracy of model1b on test data is {res_model1[1]:.6} \\nThe difference is {(accuracy_model2 - res_model1[1]):.6}, so ~3 percentage units\\n\\nSo we can see that our model with the l2 penalty is actually quite a bit more accurate and the loss is lower which is also nice.\"\n",
    "print(performance_comparison_1h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8bf1-f86e-4b0c-9cc6-bdd9f1d964c4",
   "metadata": {},
   "source": [
    "## Part 2: Tuning neural nets with caret\n",
    "\n",
    "Keras provides functions that allow the use of scikit-learn for model tuning. Using this functionality requires relatively little effort and in this part we are going to practice the individual steps of model tuning with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac151a-c3c3-44e8-8adb-a8a3cedd6d95",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "First, we need to define the architecture of the neural net that we tune and to compile the model. This needs to be done inside a function. The arguments of this function are the tuning parameters whose candidate values we want to feed into the function one by one.\n",
    "\n",
    "In this task, we work with the architecture defined for `model2` in Task 1g. The only parameter that we want to tune is the regularization parameter for $\\ell_2$ penalization inside the hidden units.\n",
    "\n",
    "Now create a function `modelbuild_2a` which has one argument `l2_pen`. In this function, specify the architecture of a Keras model `model`, identical to that of `model2` and with $\\ell_2$-penalty set to `ell_2`. Compile the model with the same settings as in Task 1g and return it as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ecfec657-d581-4b0c-b751-114dc1496386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelbuild_2a(l2_pen=0.0):\n",
    "    ell_2 = l2_pen\n",
    "    model_2a = keras.Sequential()\n",
    "    model_2a.add(layers.Dense(30, input_dim = X_train.shape[1], activation = \"ReLU\", kernel_regularizer=l2(ell_2)))\n",
    "    model_2a.add(layers.Dense(15, activation = \"ReLU\", kernel_regularizer=l2(ell_2)))\n",
    "    model_2a.add(layers.Dense(1, activation= \"sigmoid\"))\n",
    "    model_2a.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.00003), metrics=[\"accuracy\"])\n",
    "    return(model_2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078933-0ccc-4f9e-abb6-e26520bd8760",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "In order to make the function `modelbuild_2a` compatible with `sklearn`, we need to call it inside the `KerasClassifier()` function from the `wrappers` module of `scikit-learn`. As additional arguments, provide all arguments that you used when fitting `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "027797af-2ee2-4566-8c67-ef98b0f40f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "model2_sklearn_spec = KerasClassifier(build_fn=modelbuild_2a, l2_pen=0.0,\n",
    "                                      epochs=250, \n",
    "                                      batch_size=2**8, \n",
    "                                      verbose=1, \n",
    "                                      validation_split=0.25, \n",
    "                                      callbacks=[EarlyStopping(patience=20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cce66-a557-4597-80dc-bb1ff84b4f1e",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "Next, define a parameter grid `tune_grid_2c`. This must be a dictionary object. Ensure that the only object within `tune_grid_2c` is called `l2_pen`. Its values should be zero as well as $10^{r}$ for a grid of eleven $r$-values from $-4$ and $-1$ at equal distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "63925c0d-6abd-49e6-bfad-5adc320ce894",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid_2c = {\n",
    "\"l2_pen\": np.insert(np.logspace(-4, -1, 11), 0, 0).tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "062bf809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(np.logspace(-4, -1, 11), 0, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978345-6081-478b-9549-771d300a1992",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "Now, we can tune our model. That's computationally quite costly, so we will use merely a fraction of the available training data. The inputs and outputs for this task are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e12d402-63b4-420b-a4b0-8c6f2830fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_ , y_train_small, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847933-232b-499f-847a-04255215d8cd",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "1. Use `Kfold()` from the model selection module in scikit-learn to define a random partition of the training data into five folds. Use $5$ as your random seed. Save this partition as `cv_splits_2d`\n",
    "2. Call `GridSearchCV()` and use the wrapper function from Task 2b, the parameter grid from Task 2c as well as `cv_splits_2d` as arguments\n",
    "3. Apply the `fit()`-method to `GridSearchCV` and use `X_train_small` and `y_train_small` as inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8746fa6-0cd3-469c-afb2-7278ee02ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1.\n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "# 2.\n",
    "NN_tune_2d = GridSearchCV(estimator=model2_sklearn_spec, \n",
    "                          param_grid=tune_grid_2c,\n",
    "                          cv=cv_splits_2d)\n",
    "\n",
    "# 3.\n",
    "NN_tune_2d.fit(X_train_small, y_train_small)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Cross validation finished in {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16a15-f9d0-4d38-896a-627e4d5dde75",
   "metadata": {},
   "source": [
    "### Task 2e)\n",
    "By default, GridSearchCV saves the trained model with the best tuning parameter values as as object `best_estimator_` inside `NN_tune_2d`. However, in our case this model is a KerasClassifier object. We cannot use such an object for making predictions on test data. \n",
    "\n",
    "Still, the KerasClassifier object contains the trained model in the typical Keras format as object `model`. Extract this object-inside-the-object-inside-the-object and save it as `model3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ed6ec904-ce98-4907-b3fa-cc54b1a43cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NN_tune_2d.best_estimator_.model_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3998bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x25082419c90>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_tune_2d.best_estimator_.model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6448b-ac63-4129-9236-6dcb23bcf1dd",
   "metadata": {},
   "source": [
    "## Part 3: Saving, loading and retraining neural nets \n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "Training and tuning neural nets can take a lot of time. Therefore, it is possible to save entire fitted models to disk and to import them at a later point in time. Apply the `save()`-method to your most recent `model3` in order to save it as *DABN13_asst6_saved_model3* in your working directory.\n",
    "\n",
    "Ensure that the model is saved in TensorFlow SavedModel format.\n",
    "\n",
    "Additionally, use the file explorer in your operating system to look how exactly the model was saved on your hard drive. Describe this shortly in the string_variable `saved_model_3a`\n",
    "\n",
    "*Note:* Functions for saving and loading models are very nicely described in the [keras documentation](https://keras.io/api/saving/model_saving_and_loading/#save-method)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720e1779-a59c-4153-9c89-39d3be986fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save(\"DABN13_asst6_saved_model3\")\n",
    "\n",
    "saved_model_3a = \"The save function creates a folder in the current working directory. The folder contains two internal folders \\ncalled 'assets' and 'variables'. In addition to that the folder contains three files called fingerprint.pb\\n keras_metadata.pb, and saved_model.pb. The saved_model.pb file contains the architecture and weights, and everything else \\n is supporting files.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424edf9-37e1-4b9e-9c91-4836c6367709",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Now, use the `load_model` function in the `models` module of Keras to load your saved model into your python session again. Save this model as `model4`.\n",
    "\n",
    "*Note:* The possibility to load a previously saved model from your hard disk is useful for more than just your own models. It even allows you to load pretrained models for specific purposes from the TensorFlow Hub or from Hugging Face. These models could then directly be used for prediction or fine-tuned on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c6f89a6c-9730-4dbd-950f-2d5b5f2f0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model4 = load_model(\"DABN13_asst6_saved_model3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385071-18a5-4ca3-906e-895fdeb9c692",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "When we tuned our most recent neural net, we did this on a relatively small fraction of the training data to reduce the computational cost. This was also the data used to train the best model that we extracted from `NN_tune_2d`.\n",
    "Now that we have chosen an optimal tuning parameter, it makes sense to retrain the `model4` on the entire training data `X_train` and `y_train`. Do this by applying the `fit()` method to `model4`. As previously, training should be done for 250 epochs, unless early stopping with a patience of 20 epochs kicks in. Minibatches of $2^8$ data points should be used. Given the large amount of data, hold only 10% of the data aside for monitoring validation loss.\n",
    "\n",
    "Once you retrained your model, obtain predicted class probabilities and save them as `prob_model4`. Then, get the log loss `logloss_model4` on the test data.\n",
    "\n",
    "Finally, to what extend did model tuning and retraining change test set accuracy relative to that of `model2`? Comment on this in the string variable `performance_comparison_3c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8f945e31-63c3-48ef-a5e2-54e7b36cdddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "193/193 [==============================] - 1s 2ms/step - loss: 0.5906 - accuracy: 0.7432 - val_loss: 0.3356 - val_accuracy: 0.9717\n",
      "Epoch 2/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.7439 - val_loss: 0.3451 - val_accuracy: 0.9716\n",
      "Epoch 3/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.7469 - val_loss: 0.3506 - val_accuracy: 0.9664\n",
      "Epoch 4/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7486 - val_loss: 0.3532 - val_accuracy: 0.9621\n",
      "Epoch 5/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.7496 - val_loss: 0.3583 - val_accuracy: 0.9617\n",
      "Epoch 6/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.7522 - val_loss: 0.3618 - val_accuracy: 0.9581\n",
      "Epoch 7/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7543 - val_loss: 0.3652 - val_accuracy: 0.9533\n",
      "Epoch 8/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7566 - val_loss: 0.3678 - val_accuracy: 0.9520\n",
      "Epoch 9/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7588 - val_loss: 0.3705 - val_accuracy: 0.9522\n",
      "Epoch 10/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.7598 - val_loss: 0.3725 - val_accuracy: 0.9511\n",
      "Epoch 11/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7604 - val_loss: 0.3738 - val_accuracy: 0.9369\n",
      "Epoch 12/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.7613 - val_loss: 0.3759 - val_accuracy: 0.9369\n",
      "Epoch 13/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7624 - val_loss: 0.3778 - val_accuracy: 0.9333\n",
      "Epoch 14/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7642 - val_loss: 0.3779 - val_accuracy: 0.9362\n",
      "Epoch 15/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7657 - val_loss: 0.3807 - val_accuracy: 0.9307\n",
      "Epoch 16/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7676 - val_loss: 0.3822 - val_accuracy: 0.9307\n",
      "Epoch 17/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7697 - val_loss: 0.3830 - val_accuracy: 0.9292\n",
      "Epoch 18/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7700 - val_loss: 0.3826 - val_accuracy: 0.9292\n",
      "Epoch 19/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.7710 - val_loss: 0.3846 - val_accuracy: 0.9254\n",
      "Epoch 20/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7715 - val_loss: 0.3851 - val_accuracy: 0.9256\n",
      "Epoch 21/250\n",
      "193/193 [==============================] - 0s 2ms/step - loss: 0.5581 - accuracy: 0.7718 - val_loss: 0.3853 - val_accuracy: 0.9251\n",
      "572/572 [==============================] - 1s 850us/step\n",
      "0.4877437385093696\n",
      "572/572 [==============================] - 1s 1ms/step - loss: 0.5505 - accuracy: 0.7774\n",
      "\n",
      "The accuracy of model2 on test data is 0.77745 \n",
      "The accuracy of model1b on test data is 0.737568 \n",
      "The difference is 0.0398818\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "model4_fit = model4.fit(x=X_train, \n",
    "                        y=y_train, \n",
    "                        epochs=250, \n",
    "                        batch_size=2**8, \n",
    "                        validation_split=0.10, \n",
    "                        callbacks=[EarlyStopping(patience=20)])\n",
    "\n",
    "# 2.\n",
    "prob_model4    = model4.predict(X_test)\n",
    "logloss_model4 = log_loss(y_test, prob_model4)\n",
    "print(logloss_model4)\n",
    "\n",
    "# 3.\n",
    "accuracy_model4 = model4.evaluate(X_test, y_test)[1]\n",
    "performance_comparison_3c = f\"\\nThe accuracy of model4 on test data is {accuracy_model4:.6} \\nThe accuracy of model2 on test data is {accuracy_model2:.6} \\nThe difference is {(accuracy_model4 - accuracy_model2):.6}\"\n",
    "print(performance_comparison_3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766e3e-925d-434b-a546-35b8a77fa177",
   "metadata": {},
   "source": [
    "## Part 4: Manual predictions from a trained neural net\n",
    "\n",
    "In this part we will build predictions *manually* by extracting weights from the trained  `model2` and by constructing the transformations in the layers of the neural net ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3b6b3-6107-41d3-8bd4-09cc1c393791",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "\n",
    "Start this task by creating your own ReLU activation function. Save it as `ReLU`. Then, write your own sigmoid function for the output layer transformation. Save it as  `sigmoid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "755df496-d94c-48b7-8a0d-d406e94d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8157-ef24-4994-a4bd-34827db28d8e",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "In the slides for lecture 8 we discussed how units in the different layers of a neural net look like. Now we are going to use the equations both hidden units and output unit to construct output predictions for the $n_{test}$ data points in `X_test`. Please do the following:\n",
    "\n",
    "1. Apply the `get_weights()` method on `model2` to obtain a list object which stores the weights and biases of the learned model. Save it as `weight_and_bias_4b`.\n",
    "2. Extract the objects inside `weight_and_bias_4b` into the objects for $\\mathbf{b}_1,\\mathbf{W}_1, \\mathbf{b}_2, \\mathbf{W}_2,\\mathbf{b}_3, \\mathbf{W}_3$ defined in the code chunk below.\n",
    "3. Construct the linear term of the first layer hidden units and save these linear terms as a $30 \\times n_{train}$ vector `Z_1`.\n",
    "4. Construct the $15 \\times n_{train}$ vector `Z_2` of linear terms for the second layer hidden units. Then, obtain the linear term of the output unit and save it as `Z_3`.\n",
    "5. Put `Z_3` into the output layer activation function in order to get predictions for the probability of a Bud Light purchase. Save this as $n_{train} \\times 1$ vector `pred_own_2b`. \n",
    "\n",
    "*Hint*: You can use `dim()` to check the dimension of matrices and `length()` to check that of vectors. Additionally, to ensure that you got the correct result for `prob_model2_own_4b` you can compare it with the output of `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "50523588-05be-4174-b294-392f96cd8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "weight_and_bias_4b = model2.get_weights()\n",
    "\n",
    "# 2.\n",
    "WW_1 = weight_and_bias_4b[0]\n",
    "bb_1 = weight_and_bias_4b[1]\n",
    "WW_2 = weight_and_bias_4b[2]\n",
    "bb_2 = weight_and_bias_4b[3]\n",
    "WW_3 = weight_and_bias_4b[4]\n",
    "bb_3 = weight_and_bias_4b[5]\n",
    "\n",
    "# 3.\n",
    "Z_1 = ReLU(bb_1 + np.dot(X_test, WW_1))\n",
    "\n",
    "# 4.\n",
    "Z_2 = ReLU(bb_2 + np.dot(Z_1, WW_2))\n",
    "Z_3 = bb_3 + np.dot(Z_2, WW_3)\n",
    "\n",
    "# Apply sigmoid activation to Z_3\n",
    "prob_model2_own_4b = sigmoid(Z_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a8a34b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Manual predictions  Predict function\n",
      "0                0.489508          0.489508\n",
      "1                0.488845          0.488845\n",
      "2                0.159609          0.159609\n",
      "3                0.267166          0.267166\n",
      "4                0.803362          0.803362\n",
      "...                   ...               ...\n",
      "18274            0.285980          0.285980\n",
      "18275            0.285980          0.285980\n",
      "18276            0.321304          0.321304\n",
      "18277            0.495023          0.495023\n",
      "18278            0.706342          0.706342\n",
      "\n",
      "[18279 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "predict_df = pd.DataFrame({\"Manual predictions\": prob_model2_own_4b.flatten(), \"Predict function\": prob_model2.flatten()})\n",
    "print(predict_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
