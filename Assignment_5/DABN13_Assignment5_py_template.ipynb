{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbf4e29-cdbe-4ba1-8682-4da0feba73fe",
   "metadata": {},
   "source": [
    "## DABN13 - Assignment 5\n",
    "In line with Lecture 7, Assignment 5 is somewhat more technical. This means there are no basic warm-up tasks since even the use of a canned routine for optimization requires us to write a function for our cost function.\n",
    "\n",
    "Still, this assignment covers frequently occuring coding tasks in the context of function optimization. That is, you will write many functions, transfer a lot of mathematical expressions from the lecture slides into this script, create some simple plots and, most importantly, spend far too much time on trying to figure out what is wrong with your code. On the upside, this assignment gives you a little more freedom to write code in your own way. Enjoy!\n",
    "\n",
    "### Preamble: Preparing the data\n",
    "\n",
    "We are going to recycle a dataset on purchases in online shops that we already used in Assignment 2. \n",
    "Load the data into R and save it in an object called `shoppers`. The dataset is contained in a comma-separated spreadsheet. Accordingly, you will need to use the ´ read.csv()´ command in R.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0248fef5-8e3a-48e8-bc87-973c1c8d382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shoppers = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbed032-0810-436b-b5e0-d2c675c09526",
   "metadata": {},
   "source": [
    "In the steps below, we will fit a quite small logistic regression model including the following variables in addition to the intercept:\n",
    "\n",
    "1. ` ExitRate ` without further transformation\n",
    "2. The (natural) logarithm of ` ProductRelated_Duration + 1 `\n",
    "\n",
    "The code below does all data transformations for you. However, please note the shortened variable names that we are going to use from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7b40aa-3248-4927-b2d4-d4e882e6642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers = shoppers.rename(columns={\"ExitRates\": \"ER\"})\n",
    "shoppers[\"lPR_Dur\"] = np.log(shoppers[\"ProductRelated_Duration\"] +1)\n",
    "shoppers[\"constant\"] = 1\n",
    "X        = np.asarray(shoppers[[\"constant\", \"ER\", \"lPR_Dur\"]])\n",
    "\n",
    "y        = np.ones((X.shape[0],1))\n",
    "y[shoppers[\"Revenue\"]==False] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aae253-7fda-4d4c-abe5-90b3df439e6c",
   "metadata": {},
   "source": [
    "## Part 1: Learning logistic regression without `sklearn`\n",
    "\n",
    "### Task 1a)\n",
    "\n",
    "If we want to learn a logistic regression manually, we need to specify the cost function that we are going to minimize So as a first step, create a function `cost_logistic_1a()` whose inputs are \n",
    "\n",
    "1. `theta`: a one-dimensional NumPy array of coefficients,\n",
    "2. `y`: a $n \\times 1$ NumPy array of outputs,\n",
    "3. `X`: a $n \\times p$ NumPy array of inputs.\n",
    "\n",
    "The function is then supposed to return the cost on the data consisting of `y` and `X` with logistic loss and coefficient vector values `theta`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dace4a58-e902-41f6-bac9-a05ac67bd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logistic_1a(theta, y, X):\n",
    "    n = len(y)\n",
    "    total_loss = 0\n",
    "    for i in range(0, n):\n",
    "        loss = np.log(1 + np.exp(np.dot(-y[i]*theta.T, X[i])))\n",
    "        total_loss += loss\n",
    "    cost = (1/n) * total_loss\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38820b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805600665"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_logistic_1a(theta=np.zeros(3), y=y, X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa5781-5d0f-4ad6-8e40-c19d4510d351",
   "metadata": {},
   "source": [
    "### Task 1b) \n",
    "\n",
    "As a next step, we need a function that returns the gradient of the cost function. Write such a function `grad_logistic_1b()` that takes the same arguments as `cost_logistic()` and which returns a one-dimensional NumPy array (with $p$ elements) containing the gradient of our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8b3f112-ed90-4848-b312-8e5ad2379f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logistic_1b(theta, y, X):\n",
    "    n = len(y)\n",
    "    total = np.zeros(len(theta))\n",
    "    for i in range(0, n):\n",
    "        part = (-y[i] * X[i]) / (1 + np.exp(np.dot(y[i]*X[i].T, theta)))\n",
    "        total += part\n",
    "    gradient = (1/n) * total\n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e6cc934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34525547, 0.01851034, 1.9107748 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_logistic_1b(theta = [0,0,0], y=y, X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d5d85-8d03-4173-8933-8e51f39ce0a9",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "\n",
    "A canned routine for optimization is provided in the SciPy package: The `minimize()` function. `minimize()` minimizes a function that you feed into it. It also allows you to provide a gradient function to speed up optimization and to achieve better numerical stability in complicated cases. \n",
    "\n",
    "The function minimized by `minimize()` should have only one argument: The parameters of our minimization problem. This does not really fit the structure of `cost_logistic_1a` and `grad_logistic_1b` which have three arguments each. However, the last two arguments specify the data that we use. Therefore we make a short detour to arrive at functions of one argument only.\n",
    "\n",
    "1. Create a [lambda function](https://docs.python.org/3/reference/expressions.html#lambdas) `cost_logistic_1c` which takes `theta` as its only input and which calls `cost_logistic_1a` from Task 1a with `theta` as well as  the previosly created output `y` and inputs `X`.\n",
    "2. In the same fashion, create a lambda function `grad_logistic_1c` that turns `grad_logistic_1b` into a function with one single argument `theta` as well as `y` and `X` as (fixed) output and inputs, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385578b6-761e-44ac-aae5-f7a6785133ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_logistic_1c = lambda theta: cost_logistic_1a(theta, y, X)\n",
    "grad_logistic_1c = lambda theta: grad_logistic_1b(theta, y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891c938-700f-4fce-b0a1-b06c4d3454c0",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "The`minimize()`-function allows you to choose from several algorithms from the very broad literature on optimization methods. We are going to use the `BFGS` method which is a very popular modification of Newton's method. \n",
    "\n",
    "Now use `minimize()` to minimize `cost_logistic_1c` on your training data. Follow the instructions below:\n",
    "\n",
    "1. Let all initial coefficient values be zero.\n",
    "2. Choose the BFGS minimization method.\n",
    "3. Supply your gradient function `grad_logistic_1c` as well. If you could not solve Task 1b, leave out the gradient. \n",
    "4. Use the desired tolerance level `tol_1d` as the tolerance argument.\n",
    "\n",
    "Save the resulting list object as `optim_result_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83bdc74d-98bf-4af4-97b5-197d9839a2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 0.38688976841366296\n",
      "        x: [-2.489e+00 -3.028e+01  2.474e-01]\n",
      "      nit: 35\n",
      "      jac: [ 3.490e-09  5.541e-11  2.622e-08]\n",
      " hess_inv: [[ 3.440e+02 -1.397e+03 -4.490e+01]\n",
      "            [-1.397e+03  3.550e+04  9.731e+01]\n",
      "            [-4.490e+01  9.731e+01  6.273e+00]]\n",
      "     nfev: 92\n",
      "     njev: 82\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "tol_1d = 1e-9\n",
    "\n",
    "optim_result_1d  = minimize(cost_logistic_1c,x0=np.zeros(X.shape[1]), method='BFGS', \n",
    "                            jac=grad_logistic_1c, tol=tol_1d)\n",
    "\n",
    "print(optim_result_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529c1c0-d512-42ae-a50d-3a3dfa247101",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "\n",
    "Have a closer look at the OptimizeResult object `optim_result_1d`. What do its different components tell us about our solved minimization problem? Explain this in your *own* words by writing explanations into the four string variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe0591-cbd9-4cc5-9965-e179228bd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "whatis_x_1d       = \"??\" \n",
    "whatis_fun_1d     = \"??\"\n",
    "whatis_nit_1d     = \"??\"\n",
    "whatis_success_1d = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58718161-e00c-4859-bbf3-9ca85edd3317",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent manually\n",
    "\n",
    "In this part, you are going to write your own routine for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73cf72-7256-4a04-97ed-a27b4f70382f",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "\n",
    "Below, I have prepared a fragment of a function that is supposed to conduct gradient descent. This function, `grad_desc`, takes the following inputs:\n",
    "\n",
    "- `par`: initial coefficient values as one-dimensional NumPy array\n",
    "- `fn`: The objective function\n",
    "- `gr`: The gradient of the objective function\n",
    "- `stepsize`: The step size \n",
    "- `maxitr`: The maximum number of parameter updates. Default is set to 5000.\n",
    "- `tol`: The tolerance for coefficient updates to be considered \"effectively 0\". Default is set to 0.000001\n",
    "-  `y`: an $n \\times 1$ NumPy array of outputs\n",
    "-  `X`: an $n \\times p$ NumPy array of inputs.\n",
    "\n",
    "Additionally, the function already contains the following objects:\n",
    "\n",
    "- `coef_path`: A two-dimensional NumPy array whose columns eventually contain the entire sequence of coefficient vectors,\n",
    "- `coef_upd`: A two-dimensional NumPy array whose columns eventually contain the entire sequence of coefficient updates,\n",
    "- `fn_path`: A one-dimensional NumPy array that eventually contains all values of the objective function.\n",
    "\n",
    "Your task is to write code that conducts the coefficient updates of gradient descent until the Euclidean norm of a coefficient update is below the tolerance level `tol`. \n",
    "\n",
    "The full sequence of coefficient vectors, coefficient updates and function values should be saved in the objects `coef_path`, `coef_upd` and `fn_path`, respectively. Maximally `maxitr` coefficient updates should be made (fewer if coefficient updates are below tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbee4e-d13f-4cc4-85b3-279d4db30d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(par, fn, gr, stepsize, maxitr=5000, tol=1e-6, y=None, X=None):\n",
    "\n",
    "    # Don't change anything here\n",
    "    coef_path = par[:,None]\n",
    "    coef_upd  = np.empty((len(par), 0))\n",
    "    fn_path   = [fn(theta=par, X=X, y=y)]\n",
    "    \n",
    "    # Start writing stuff here\n",
    "    ??\n",
    "    \n",
    "    # Don't change anything here\n",
    "    return {\n",
    "        'coef_final': coef_path[:,-1],\n",
    "        'itr': itr,\n",
    "        'coef_path': coef_path,\n",
    "        'updates': coef_upd,\n",
    "        'fun_path': fn_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa7aee-e45d-4d72-bddc-abf4e523fee3",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "\n",
    "Now we tune the step size of gradient descent. In order to do this, write a function `plot_cost_2b` which runs gradient descent with a chosen step size and only 100 coefficient updates. The function should then create an object containing a line plot of the number of coefficient updates against the cost at the specific update. \n",
    "\n",
    "I have already prepared a dictionary object `gd_ctrl_2b` for you that contains the gradient descent function as well as most inputs to this function. Use `gd_ctrl_2b` as an input and define further arguments to your step size as well as the data.\n",
    "\n",
    "Once you have created `plot_cost_2b`, play around with different step sizes. Save a plot with a step size that you deem as too low (high) as object `plot_tooLO_2b` (`plot_tooHI_2b`). Save another plot `plot_decent_2b` which contains the development of cost with a good step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e297349-1d21-40f9-ae7b-bf3d383a2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_ctrl_2b = {\n",
    "    'par': np.array([0, 0, 0]),\n",
    "    'fn': cost_logistic_1a,\n",
    "    'gr': grad_logistic_1b,\n",
    "    'gd_fun': grad_desc,\n",
    "    'maxitr': 100\n",
    "}\n",
    "\n",
    "??\n",
    "\n",
    "# Step size too low:\n",
    "??\n",
    "# Step size too high:\n",
    "??\n",
    "# Steo size quite ok:\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75ab64-ed04-4e0c-9dcd-dd93db54f75d",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "\n",
    "Use the string variable `stepsize_motivate_2c` to motivate why you chose the three step size examples in Task 2b. More specifically, explain what lead you to the conclusion that a step size is too low or too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1ac71-9cac-459c-a72a-0f36fac387fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize_motivate_2c <- \"??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8f1b7-0b79-43b3-943a-2161de6f787a",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "\n",
    "Use `grad_desc` with your chosen step size to learn the coefficients of the logistic regression model that we have been working with so far. Keep the maximum number of updates at its default value. Save the resulting obect as `gd_result_2d`. \n",
    "\n",
    "Does gradient descent stop before the maximum number of updates? If not, how far does gradient descent get the cost function towards the value achieved by `optim()` in Task 1c? Write your answer into the string variable `gd_conclusion_2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5eb84-15df-4ae8-8719-5565628b1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_result_2d = ??\n",
    "\n",
    "gd_conclusion_2d = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82cae4-9992-44c4-a3b8-adcc64e48715",
   "metadata": {},
   "source": [
    "## Part 3: Newton's method manually\n",
    "\n",
    "Given that you have implemented gradient descent in Part 2, you have already done most of the hard work that is required for Newton's method. After all, the only thing that differs is the expression for your coefficient update. For this reason, we will work on our own routine for Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eecdf6-af30-451d-9d65-c8eda33d8282",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Newton's method is more demanding than gradient descent in that it requires us to obtain the Hessian of our objective function for any desired vector of coefficients. Write such as function `hessian_logistic_3a` which has the same arguments as `grad_logistic_1b` and which returns the hessian of your cost function with logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370af0b2-4e91-4226-857e-70c7628f7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04251bc8-a4b2-4d5b-9fd2-1bf83233e671",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Modify `grad_desc` to turn it into a function for Newton's method. This only requires you to remove step size from the function arguments, to add a new argument for the Hessian and to change the line for your coefficient update. Save your function as object `newton_3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d98e5f-3789-40a1-95e7-e867ef1a2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be55708-80ab-46d5-b272-f7eeecb34d8b",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "Use your function `newton_3b` to learn the logistic regression model that you already learned in Tasks 1c and 2d. Keep the maximum number of updates at its default value. Save the resulting obect as `nm_result_3c`. \n",
    "\n",
    "Does Newton's method stop before the maximum number of updates? If not, how far does gradient descent get the cost function towards the value achieved by `optim()` in Task 1c? What is your verdict about the performance of Newton's method relative to gradient descent and the `optim()` command in the case of logistic regression? Write your answer into the string variable `nm_conclusion_3c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37107c16-4624-4f81-bff8-b41713e7cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_result_3c = ??\n",
    "print(nm_result_3c)\n",
    "nm_conclusion_3c = \"??\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
