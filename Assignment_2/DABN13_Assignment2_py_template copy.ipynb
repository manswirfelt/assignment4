{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff03aa7d-85fe-44e9-ad19-19c845b9457c",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 2\n",
    "## Preamble: Predicting purchases in online shops. \n",
    "This assignment will be based on a dataset on online shopper purchase data which is available on the UC Irvine Machine Learning Library. A description of all variables is available [here  ](https://www.kaggle.com/henrysue/online-shoppers-intention). Among the 11 variables in the dataset, we will only use the three following:\n",
    "\n",
    " - **Revenue**: (TRUE/FALSE) Whether a purchase was made by a visitor to the online shop\n",
    " - **ProductRelated_Duration**: (numerical) Time spend on pages relevant/related to the product in question.\n",
    " - **ExitRate**: (numerical) The percentage of visits to the online shop that end with visiting the site of the product at issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6312ad4d-9bb5-4c6b-9aca-73068d5e3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyreadr as prdr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff2b82-e37e-470d-a55f-16116de150c0",
   "metadata": {},
   "source": [
    "## Part 1: Logistic regression with `sklearn`\n",
    "\n",
    "In this basic part, we are getting some experience with using scikit-learn to learn logistic regressions. In the steps below, we will train a quite small logistic regression model with `Revenue` as output variable and the following inputs (in addition to the intercept):\n",
    "\n",
    "1. ` ExitRate ` without further transformation\n",
    "2. The (natural) logarithm of ` ProductRelated_Duration + 1 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e9b0-43f6-490f-b292-2def4224a950",
   "metadata": {},
   "source": [
    "### Task 1a)\n",
    "First, we need to prepare our data. Conduct the following steps:\n",
    "\n",
    "1. Load the data into R and save it in an object called `shoppers`. The dataset is contained in a comma-separated spreadsheet. Accordingly, you will need to use the `read_csv()` command in Pandas.\n",
    "2. Use the `column` method on `shoppers` to change the variable name of `ExitRate` to `ER`. More specifically, you will need to rename a particular column of `shoppers`. \n",
    "3. Create a new variable `lPR_Dur` inside `shoppers` that contains the (natural) logarithm of  `ProductRelated_Duration + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "47c854c8-f1a6-4ff5-adb0-845ee7cf5cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ER</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>lPR_Dur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.174387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.299283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6.443336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>3</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>1783.791667</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>12.241717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7.487057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6.145794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5.221706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.849325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>New_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3.102342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Administrative  Administrative_Duration  Informational  \\\n",
       "0                   0                      0.0              0   \n",
       "1                   0                      0.0              0   \n",
       "2                   0                      0.0              0   \n",
       "3                   0                      0.0              0   \n",
       "4                   0                      0.0              0   \n",
       "...               ...                      ...            ...   \n",
       "12325               3                    145.0              0   \n",
       "12326               0                      0.0              0   \n",
       "12327               0                      0.0              0   \n",
       "12328               4                     75.0              0   \n",
       "12329               0                      0.0              0   \n",
       "\n",
       "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                         0.0               1                 0.000000   \n",
       "1                         0.0               2                64.000000   \n",
       "2                         0.0               1                 0.000000   \n",
       "3                         0.0               2                 2.666667   \n",
       "4                         0.0              10               627.500000   \n",
       "...                       ...             ...                      ...   \n",
       "12325                     0.0              53              1783.791667   \n",
       "12326                     0.0               5               465.750000   \n",
       "12327                     0.0               6               184.250000   \n",
       "12328                     0.0              15               346.000000   \n",
       "12329                     0.0               3                21.250000   \n",
       "\n",
       "       BounceRates        ER  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.200000  0.200000    0.000000         0.0   Feb                 1   \n",
       "1         0.000000  0.100000    0.000000         0.0   Feb                 2   \n",
       "2         0.200000  0.200000    0.000000         0.0   Feb                 4   \n",
       "3         0.050000  0.140000    0.000000         0.0   Feb                 3   \n",
       "4         0.020000  0.050000    0.000000         0.0   Feb                 3   \n",
       "...            ...       ...         ...         ...   ...               ...   \n",
       "12325     0.007143  0.029031   12.241717         0.0   Dec                 4   \n",
       "12326     0.000000  0.021333    0.000000         0.0   Nov                 3   \n",
       "12327     0.083333  0.086667    0.000000         0.0   Nov                 3   \n",
       "12328     0.000000  0.021053    0.000000         0.0   Nov                 2   \n",
       "12329     0.000000  0.066667    0.000000         0.0   Nov                 3   \n",
       "\n",
       "       Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \\\n",
       "0            1       1            1  Returning_Visitor    False    False   \n",
       "1            2       1            2  Returning_Visitor    False    False   \n",
       "2            1       9            3  Returning_Visitor    False    False   \n",
       "3            2       2            4  Returning_Visitor    False    False   \n",
       "4            3       1            4  Returning_Visitor     True    False   \n",
       "...        ...     ...          ...                ...      ...      ...   \n",
       "12325        6       1            1  Returning_Visitor     True    False   \n",
       "12326        2       1            8  Returning_Visitor     True    False   \n",
       "12327        2       1           13  Returning_Visitor     True    False   \n",
       "12328        2       3           11  Returning_Visitor    False    False   \n",
       "12329        2       1            2        New_Visitor     True    False   \n",
       "\n",
       "        lPR_Dur  \n",
       "0      0.000000  \n",
       "1      4.174387  \n",
       "2      0.000000  \n",
       "3      1.299283  \n",
       "4      6.443336  \n",
       "...         ...  \n",
       "12325  7.487057  \n",
       "12326  6.145794  \n",
       "12327  5.221706  \n",
       "12328  5.849325  \n",
       "12329  3.102342  \n",
       "\n",
       "[12330 rows x 19 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir('C:\\\\Users\\\\??') # Chane working directory if needed\n",
    "\n",
    "# 1.\n",
    "shoppers = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "\n",
    "# 2.\n",
    "shoppers.rename(columns={'ExitRates': 'ER'}, inplace=True)\n",
    "\n",
    "# 3.\n",
    "shoppers['lPR_Dur'] = np.log(shoppers['ProductRelated_Duration'] + 1)\n",
    "\n",
    "shoppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9544a5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10422\n",
       "True      1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoppers['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7f533-7912-45b8-a8e0-3536a6bd0dcc",
   "metadata": {},
   "source": [
    "### Task 1b)\n",
    "\n",
    "Technically, we could use the `statsmodels` package to learn a logistic regressions. However, the functionality of `statsmodels` addresses applications in classical statistics rather than machine learning. Since we want to use logistic regression as a supervised learning algorithm in order to predict the output of new data points, it is much wiser to switch to scikit-learn (`sklearn`). `sklearn` provides you with simple functions that implement model training, tuning and validation and therefore covers the entire spectrum of standard methods in supervised learning.\n",
    "\n",
    "Learning a logistic regression with `sklearn` works almost identically as with `statsmodels`. Do it as follows: \n",
    "\n",
    "1. Create a  Pandas series `y_1b` that contains the `Revenue` variable from `shoppers`\n",
    "2. Create a similar data frame `X_1b` whose columns are  ` ER `, ` lPR_Dur ` and the square of `lPR_Dur`.\n",
    "3. Instantiate a logistic regression model using the `LogisticRegression()` function from the `linear_model` module and save this model specification as `glm_spec_1b`. The function has an argument `penalty`. Set this argument to `None`. Furthermore ensure that an intercept is automatically added to the input variables.\n",
    "4. Apply the `fit()`-method to `glm_spec_1b` to learn the model and save the learned model as `glm_fit_1b`. Here, specify that the model is learned using with inputs `X_1b` and output `y_1b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4b4a8d78-597a-459f-bc78-0c38fd3f3d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-32.63987528  -0.58940172   0.06451171]] [0.16879732]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1.\n",
    "y_1b = pd.Series(shoppers['Revenue'])\n",
    "\n",
    "\n",
    "# 2.\n",
    "X_1b = pd.DataFrame()\n",
    "X_1b['ER'] = shoppers['ER']\n",
    "X_1b['lPR_Dur'] = shoppers['lPR_Dur']\n",
    "X_1b['square_lPR_Dur'] = np.square(shoppers['lPR_Dur'])\n",
    "\n",
    "# 3.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "glm_spec_1b = LogisticRegression(penalty = None, fit_intercept = True)\n",
    "\n",
    "# 4.\n",
    "glm_fit_1b  = glm_spec_1b.fit(X_1b, y_1b)\n",
    "\n",
    "print(glm_fit_1b.coef_, glm_fit_1b.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb034-a558-4904-b5ae-613e5a03e175",
   "metadata": {},
   "source": [
    "### Task 1c) \n",
    "A fundamental principle of machine learning is that we divide the data available to us into different sets which we use for learning, model tuning and algorithm choice. \n",
    "\n",
    "`sklearn` makes this very simple for us since its `model_selection`-module contains the `train_test_split()` function. It returns splitted versions of all objects that you provide as inputs. I prepared a preliminary code chunk below that shows how you need to specify the desired splitted objects. Complete the code chunk by entering the inputs to `train_test_split()`. More specifically, provide\n",
    "\n",
    "1. the data objects before splitting in correct order,\n",
    "2. an argument that allocates 50% of all data points to the training data\n",
    "3. an argument that sets the initial state of the random number generator to 3 (we need this to ensure replicability). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eb3ddaf8-10e3-4bb2-860a-3986e89c4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_1b_train, X_1b_test, y_1b_train, y_1b_test = train_test_split(X_1b, y_1b, test_size=0.5, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd7a7e-7be6-459a-918d-6fae1dcde2f2",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "Now, please refit the model from Task 1a using only your training data from Task 1c. Save the resulting learned model as `glm_fit_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "befc1502-1faf-436b-8624-575f88426474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-32.23990095  -0.68454114   0.07092482]] [0.50609387]\n"
     ]
    }
   ],
   "source": [
    "glm_fit_1d =  glm_spec_1b.fit(X_1b_train, y_1b_train)\n",
    "\n",
    "print(glm_fit_1d.coef_, glm_fit_1d.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa53b29-6bd5-40c0-a880-7580c943dba6",
   "metadata": {},
   "source": [
    "### Task 1e) \n",
    "Now that we have fitted our model, we want to evaluate its predictive performance on hold-out validation data. We have already created this test data in Task 1c as `y_1b_test` and `X_1b_test.`\n",
    "\n",
    "Before we can evaluate model performance, we first need to obtain predicted probabilities for purchases on the test data. Use the ` predict_proba() `-method on `glm_fit_1d` to obtain such predicted conditional probabilities from the model fit in Task 1d on the observations in our test set. Save them as `glm_prob_1e`\n",
    "\n",
    "According to the sklearn documentation, the columns of `glm_prob_1e` contain class probabilities where classes are ordered as they are in `glm_fit_1d.classes_`. Which column index contains the probabilities for class `True`? Write your answer into the string variable `which_column_truepreds_1e`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b34d0b8b-edc7-469d-96ab-f085dae9eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_prob_1e = glm_fit_1d.predict_proba(X_1b_test)[:, 1]\n",
    "\n",
    "# Find the index of the class labeled as \"True\" (assuming classes are integers)\n",
    "which_column_truepreds_1e = glm_fit_1d.classes_.tolist().index(True)\n",
    "\n",
    "which_column_truepreds_1e = \"Column index 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324b30-692c-426c-bd3c-45f27dac2795",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "In a next step, we apply a classification rule to map our predicted probabilities into class predictions. Our classification rule is to predict the most likely class.\n",
    "\n",
    "First, create a new vector ` glm_pred_1f ` which has as many elements as ` glm_prob_1e ` and which consists entirely of the logical statement `False` (without citation marks!) \n",
    "\n",
    "Second, replace `False` in ` glm_pred_1f ` with `True` for all elements where the corresponding predicted probability for category `True` exceeds the threshold used for the classifier mentioned above. You can do this by indexing `glm_pred_1e` using square brackets. Simply write a true-or-false (or logical) statement in the square brackets. For rows where it is true, the value of ` glm_pred_1f ` will be changed.\n",
    "\n",
    "Please additionally write the true-or-false statement that you use into the string variable ` logical_1f ` for the sake of making assignment evaluation simpler for us.\n",
    "\n",
    "*Note:* We could even get class predictions by applying the `predict()`-method to `glm_fit_1d`. However, that does not allow us to fully control the classification rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2427d51e-44ba-4161-9aa2-06271d84e182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False,  True]), array([6150,   15]))\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "glm_pred_1f = [False] * len(glm_prob_1e)\n",
    "\n",
    "# 2.\n",
    "threshold = 0.5\n",
    "\n",
    "glm_pred_1f = np.array(glm_pred_1f)\n",
    "\n",
    "# Apply the threshold to the predicted probabilities (1D condition)\n",
    "condition = glm_prob_1e > threshold\n",
    "\n",
    "glm_pred_1f[condition] = True\n",
    "\n",
    "# 3.\n",
    "print(np.unique(glm_pred_1f, return_counts=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa8cef-d6a3-4822-97cb-5c46cd8097d1",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "Choose an appropriate error function and write its name in the string variable `chosenerrfun_1g`. Then, use the objects created in the previous tasks of this part to obtain (overall) test error for the logistic regression model fitted in Task 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d24da15-606a-4cef-82ad-16ae14da6c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification Error: 0.1551\n"
     ]
    }
   ],
   "source": [
    "misclassified_count = np.sum(glm_pred_1f != y_1b_test)\n",
    "total_instances = len(y_1b_test) \n",
    "\n",
    "misclassification_error = misclassified_count / total_instances\n",
    "\n",
    "chosenerrfun_1g = \"Misclassification Error\"\n",
    "\n",
    "print(f\"{chosenerrfun_1g}: {misclassification_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e0142-c9f6-4e87-b998-bd092842e92a",
   "metadata": {},
   "source": [
    "## Part 2: Class-specific prediction errors\n",
    "\n",
    "This part is more advanced than parts 1 and 3. In classification problems, overall test error may not always be our primary concern. To get a more differentiated picture, confusion matrices and the ROC curve are useful tools. We will get both using the metric module of Scikit-learn which provides a large number of performance criteria for binary classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1debb5b-e443-400a-845d-b5b67c4c34d0",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "A basic confusion matrix can easily be obtained using the `confusion_matrix` function of `sklearn.metrics`. This function only requires two inputs:\n",
    "\n",
    "1. The test outcomes\n",
    "2. The predicted class on the test set.\n",
    "\n",
    "Obtain the confusion matrix and save it as `confumat_2a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5204   10]\n",
      " [ 946    5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confumat_2a = confusion_matrix(y_1b_test, glm_pred_1f)\n",
    "\n",
    "print(confumat_2a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d7315-4754-4637-bad2-90715cc2b8cc",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "The confusion matrix obtained in the last task is rather rudimentary. For this reason, we are now going to write a function which produces a more luxurious confusion matrix with additional performance measures. \n",
    "\n",
    "Below, I prepared a function that takes a true outcomes, predicted probabilities for the category of interest and a desired threshold probability for the classification rule as inputs and returns a dictionary object containing the corresponding confusion matrix, TPR, FPR and overall classification error. All that is left for you is the following tasks:\n",
    "\n",
    "1. Specify the row and column names of the confusion matrix correctly\n",
    "2. Use the four elements of `cmat` to calculate `FPR`, `TPR` and the classification error `error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a72c23f2-24e8-45e6-b871-f189b96b97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  performancemetrics(y_true, y_prob, cutoff):\n",
    "    # Don't change the 2 rows below\n",
    "    y_pred       = np.greater(y_prob,cutoff)\n",
    "    cmat         = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "    # 1.\n",
    "    column_names = [\"TN\", \"FN\"]\n",
    "    row_names    = [\"FP\", \"TP\"]\n",
    "\n",
    "    TN = cmat[0, 0]  # True negative\n",
    "    FN = cmat[1, 0]  # False negative\n",
    "    FP = cmat[0, 1]  # False positive\n",
    "    TP = cmat[1, 1]  # True positive\n",
    "\n",
    "    # 2.\n",
    "    FPR = FP / (FP + TN)\n",
    "    TPR = TP / (TP + FN)\n",
    "    error = (FP + FN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    # Don't change the two lines below\n",
    "    cmat         = pd.DataFrame(cmat, columns=column_names, index=row_names)\n",
    "    allresults = {'confusion_matrix': cmat, 'FPR':FPR, \"TPR\":TPR, 'classification_error':error}\n",
    "    return(allresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d2641-3b7a-4f39-baed-254eaa8d2a19",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "\n",
    "Now save as `metrics_2c` the output of `performancemetrics()` with the prediction object of task 2a and a threshold probability of 50% as input. Additionally, answer two questions:\n",
    "\n",
    "1. Are you satisfied with the overall accuracy with which our model predicts purchases?\n",
    "2. Is the accuracy with which observed purchases are correctly predicted satisfactory? Assume here that we have considerable interest in predicting actual purchases correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7c2316ca-c3d5-4627-b76e-885e5ee8fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix':       TN  FN\n",
      "FP  5204  10\n",
      "TP   946   5, 'FPR': 0.0019179133103183737, 'TPR': 0.005257623554153523, 'classification_error': 0.15506893755068937}\n"
     ]
    }
   ],
   "source": [
    "metrics_2c = performancemetrics(y_1b_test, glm_pred_1f, threshold)\n",
    "print(metrics_2c)\n",
    "overall_acc_verdict2c      = \"??\" #Need to answer this one\n",
    "obs_purchase_acc_verdict2c = \"??\" #Need to answer this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8e85a-d080-4767-ad6b-a9ea49984797",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "\n",
    "Assume we would like to get a classifier that has relatively balanced class-specific performance. In other words, we want to choose a threshold such that TPR is approximately 1-FPR. In order to see the trade-offs that are available to us, we will look at a ROC curve.\n",
    "\n",
    "ROC curves can be plotted using the `RocCurveDisplay.from_predictions()` function in the sklearn metrics module. The inputs to this function are \n",
    "\n",
    "1. The test outcomes\n",
    "2. The predicted probabilities for class 1 on the test set.\n",
    "\n",
    "Use the code chunk below to plot a ROC curve. Then proceed with the rest of this task in the following text block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "46ebc0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABp2ElEQVR4nO3deVwU9f8H8NdyH3KoHAoiIIr3CR7g1zzyxG+aZWqa9xFpmVKaZolWSlkpmrfimVcman29K+8jFcELT0RBBRFUQJBr9/P7w5+jK6DsssvA8no+Hjya+czszGtnjX0z85n5KIQQAkREREQGwkjuAERERES6xOKGiIiIDAqLGyIiIjIoLG6IiIjIoLC4ISIiIoPC4oaIiIgMCosbIiIiMigmcgcoaSqVCnfv3oWNjQ0UCoXccYiIiKgIhBBIT0+Hi4sLjIxefW6m3BU3d+/ehZubm9wxiIiISAvx8fGoVq3aK9cpd8WNjY0NgKcHx9bWVuY0REREVBRpaWlwc3OTvsdfpdwVN88uRdna2rK4ISIiKmOK0qWEHYqJiIjIoLC4ISIiIoPC4oaIiIgMCosbIiIiMigsboiIiMigsLghIiIig8LihoiIiAwKixsiIiIyKCxuiIiIyKCwuCEiIiKDImtxc+jQIbz11ltwcXGBQqHAtm3bXvuagwcPwsfHBxYWFqhRowYWL16s/6BERERUZsha3GRkZKBx48aYP39+kdaPjY1FQEAA2rRpg8jISHz55ZcYO3YstmzZouekREREVFbIOnBmt27d0K1btyKvv3jxYlSvXh2hoaEAgLp16+L06dP46aef8O677+opJRERERVEqRLIVaqQq1QhTymQq1IhVykAAK72lrLlKlOjgh8/fhydO3dWa+vSpQvCwsKQm5sLU1PTfK/Jzs5Gdna2NJ+Wlqb3nERERMWRkZ2HjJw87L+chJw8VZFeI/BisSGQpxTIU6mQ8/+FR55ShZz//2+eSvx/+9Nl0jqqF9aRipX/n1aK/59WIff/9yNEwVmq2FrgxJdv6u6AaKhMFTeJiYlwdnZWa3N2dkZeXh6Sk5NRtWrVfK8JCQnB9OnTSyoiERGVc7dSMhAV/6hI6/4ecRvGRgooXmjbf+W+XnKVBIUCMDU2gqmJ4vUr61GZKm4AQKFQP2Di/8vGl9ufmTx5MoKCgqT5tLQ0uLm56S8gEREZnPvp2fjln2tIfZJb4PLtUXf1tm8zYyO8WdepSOsaGSlgZmwEEyMFTIyNYGb89L8mxs/aX5g2VjwtRIwVMDEygqmJEUz//3Wm/7/MxEjx/+0vrf9sHbV2IxgbyVvUPFOmipsqVaogMTFRrS0pKQkmJiaoXLlyga8xNzeHubl5ScQjIqIy7lFmDn7YfRkbTsbD2swYRv//h3N6dp7G2/KrUblIX/YZOXl4v0V1tTaHCmZoXdMBxoqnhQRppkwVN35+fvjzzz/V2vbu3QtfX98C+9sQEREV5PbDTGRkK6X5hQeu48+zd6F6oQ9JRo4y3+vcK1thkJ9Hgdu0MDVCx7rOMDFSwN7KrNScxSiPZC1uHj9+jOvXr0vzsbGxiIqKQqVKlVC9enVMnjwZd+7cwZo1awAAgYGBmD9/PoKCgjBy5EgcP34cYWFh2LBhg1xvgYiISrFHmTl4mJmLXRcSsOCf63CrZIXLiemvfZ2JkQITu9ZGp3pVpDYLUyNUtZPvDiAqOlmLm9OnT6N9+/bS/LO+MYMHD8aqVauQkJCAuLg4abmnpyd27tyJ8ePHY8GCBXBxccG8efN4GzgREUGpEsjMeXr5KPlxDtr/dCDfOi8XNg4VzAAAKgE8yMjBgv7N0NjNDtUqWuk9L+mPQojCbuQyTGlpabCzs0NqaipsbW3ljkNERMWQmpmLmTsvITYlAydjH7xyXSszYwzx90Drmg6wMjNG42r2MOKlozJDk+/vMtXnhoiIyqfY5AzM3HkJ+6LvAXh6yzGAQp+z8oyve0WsH9kKZibslFuesLghIiJZPcrMwR9n7+JSQhpMC7gzKPlxNnaeV79T9uWixtRYgalv1YdfjUrSJaVnt0NT+cPihoiISoQQAjdTMpGrVOHMrYe49SATf0Xfw7Wkx0XeRn0XW4x6owb8vRykNhsLE1iYGusjMpVRLG6IiEhvtkbexprjt5CelYfrrylijBTAwFbusLMyK3BZl/pVULcq+0rS67G4ISIinRBC4FrSYyw+GIOr99Jx4U7hY/lVsjbDg4wcDPJzR55KYFzHWnCysSjBtGTIWNwQEVGxJKVn4fudlxEeeafQdbrWr4L3fKuhqp0l6rnw7AvpF4sbIiIqsutJj3H65gP8fTkJ+6LvQaEo+I6lelVt8X7L6qhmbwn/mpVhbsI+MVRyWNwQEZEkMycPlxPTsS/6HhYdiIGdpal02/WjzPyDRr5Y2DjbmmNev6Zo7lGJz48hWbG4ISIiCCGw4uhNfPu/aLX2wkbBdrW3RHOPiuhQ1xmtPCvBzsqUZ2eo1GBxQ0RUzu25mIgP10aotTlUMEd6Vi4md6uD/9RylNqNFIBHZWuemaFSjcUNEVE59SRHibpTd+drXz7IFx3rOcuQiEg3WNwQEZVD26Pu4NONUWptQ1t7YOp/60Gh4FkZKttY3BARlQMqlcDX2y/gSa4SQgBbX7ht28RIgfPTusDSjH1myDCwuCEiMmB7LyZi1Ev9aV60aVQrtKxRuQQTEekfixsiIgOSq1Qh7kEmUp/koveiY1AV8Ayayd3qQODpiNm+HpVKPCORvrG4ISIyALlKFeIfZKLDzwcLXP5lQB38t5ELXOwtSzgZUcljcUNEVEadiXuIib+fg72lKU7feljgOk3c7PHriJaoYM5f91R+8F87EVEZk52nxJaIO/hy6/kCl3es64zlg31LOBVR6cHihoioDIm5/xhvvnTpqXujquhSvwp83CvClZediFjcEBGVBdF30zDh97O4eDdNrf3r/9bD8P94ypSKqHRicUNEVIpl5uThvcXH8xU1o96ogS8D6sqUiqh0Y3FDRFQKPc7Ow8nYFAxbdVqtvaVnJfzwbiN4OFjLlIyo9GNxQ0RUSuTkqXAzJQOd5xwqcPm/X74JZ1uLEk5FVPawuCEiktHZ+EeYufMS/o19UOg63/asj4F+HiUXiqiMY3FDRFTC4h9kIu5BJgYs/7fQddrUcsCyQb6wMOV4T0SaYnFDRFQC0rJycTM5Ax/9egZ3Hj3Jt/zNOk7o09wN/l6VYWVmAmMjjsxNpC0WN0REerb88A18t+NSvvbazjYwMVbgf5/8BwoFixkiXWFxQ0SkRydjH6gVNlXtLODlWAGrhjaHibGRjMmIDBeLGyIiPTl18wH6LDkuzR+c0A7ulXkLN5G+aVXc3Lx5E4cPH8bNmzeRmZkJR0dHNG3aFH5+frCw4G2KRFS+bY28jfGbzqq1ze3XhIUNUQnRqLhZv3495s2bh5MnT8LJyQmurq6wtLTEgwcPEBMTAwsLCwwYMABffPEF3N3d9ZWZiKjU+PvSPSw/HIvjN1JgYWqErFxVvnWm96iPnk1cZUhHVD4Vubhp1qwZjIyMMGTIEPz222+oXr262vLs7GwcP34cGzduhK+vLxYuXIj33ntP54GJiEqL4zEpGL76+ROEXy5svupeF++3qA5rc/YAICpJCiGEKMqKO3bsQPfu3Yu00eTkZMTGxqJ58+bFCqcPaWlpsLOzQ2pqKmxtbeWOQ0RlSJ5ShTuPniA2OQOhf11DVPwjadknHWqirbcjqtg9vTRfxdaCHYaJdEiT7+8i/zlR1MIGABwcHODg4FDk9YmISjulSqDmlF0FLpvbrwkvOxGVIlr/WRETE4OvvvoK77//PpKSkgAAu3fvxsWLF3UWjoiotPD6cme+tje8HXF4YnsWNkSljFbFzcGDB9GwYUP8+++/CA8Px+PHjwEA586dQ3BwsE4DEhHJ7X56tjRd38UWN7/vjpvfd8eaYS3gVslKxmREVBCterlNmjQJ3333HYKCgmBjYyO1t2/fHnPnztVZOCKikvYgIwdXEtMBAOFnbmNzxG215f/75D9yxCIiDWhV3Jw/fx7r16/P1+7o6IiUlJRihyIiKmlCCAxZeQoHr94vdJ2OdZ04TAJRGaBVcWNvb4+EhAR4enqqtUdGRsLVldeeiajsWXQwRq2wqeFgDSiA7FwVPuvsjf/UcoCTDR9SSlQWaFXc9O/fH1988QU2b94MhUIBlUqFo0eP4vPPP8egQYN0nZGISK8OXEnCrN1XpPmL07vw2TREZZhWHYpnzJiB6tWrw9XVFY8fP0a9evXwxhtvwN/fH1999ZWuMxIR6Y1S9fRy1DMbR7ViYUNUxhX5IX4FiYmJQWRkJFQqFZo2bYpatWrpMpte8CF+RPRM/INMtJm1X5of/h9PfP3fejImIqLC6OUhfgXx8vKCl5dXcTZBRFRiriSmY/HBGNxKycCZuEf5lk8JqFvyoYhI57QqboYNG/bK5StWrNAqDBGRvpyMfYA+S44XuKxzPWcsHeRbwomISF+0Km4ePnyoNp+bm4sLFy7g0aNH6NChg06CERHpyuTwc9hwMl6af7OOExq42qFNLQfUrmIDGwtTGdMRka5pVdxs3bo1X5tKpcLo0aNRo0aNYociItKVBxk5aoXNF13r4KN2vJxOZMiK1aH4ZVeuXEG7du2QkJCgq03qHDsUE5Uf0XfTEDDvsDS/fUxrNHazly8QEWmtxDoUvywmJgZ5eXm63CQRkVaWHorBzJ2Xpfn/NqrKwoaonNCquAkKClKbF0IgISEBO3bswODBg3USjIhIW7cfZqoVNh+3r4nPu9SWMRERlSStipvIyEi1eSMjIzg6OuLnn39+7Z1URET6kJWrROhf13A96TH+unRPav/ns7ao4VhBxmREVNI0Lm6EEFi1ahUcHR1hZWWlj0xERBo5fO0+BoadzNc+tkNNFjZE5ZBWxU2tWrVw8eLFMvFEYiIybKdvPshX2Az2c0ef5m6o72InUyoikpPGxY2RkRFq1aqFlJQUFjdEJBulSuDTjZH437nnd2f+2LsR3vN1kzEVEZUGWg2cOWvWLEyYMAEXLlzQdR4iotc6dfMBvL7cqVbYfNOzPgsbIgKg4ZmbNWvWoE+fPvjggw+QmZmJxo0bw8zMDJaWlmrrPXjwQKchiYieSUzNwnuLnw+jYGKkwNFJHeBsayFjKiIqTTQqboYOHYquXbtizpw5UCgU+spERFSgnDwVWoX8Lc3zFm8iKohGxc2zhxkPGTJEH1mIiAq1NfI2xm86K82/51ONhQ0RFUjjPje6PmOzcOFCeHp6wsLCAj4+Pjh8+PAr11+3bh0aN24MKysrVK1aFUOHDkVKSopOMxFR6TJ6XYRaYWNjboIf32ssYyIiKs00vltqyJAhMDc3f+U64eHhRdrWpk2bMG7cOCxcuBCtW7fGkiVL0K1bN0RHR6N69er51j9y5AgGDRqEOXPm4K233sKdO3cQGBiIESNGFDiYJxGVfTN2RGPn+URpfvkgX3Ss5yxjIiIq7TQubmxsbPJ1INbW7NmzMXz4cIwYMQIAEBoaij179mDRokUICQnJt/6JEyfg4eGBsWPHAgA8PT3x4YcfYtasWYXuIzs7G9nZ2dJ8WlqaTrITkX5l5ykRuDYC+6/cl9r2jX8DtZxtZExFRGWBxsXNvHnz4OTkVOwd5+TkICIiApMmTVJr79y5M44dO1bga/z9/TFlyhTs3LkT3bp1Q1JSEn7//Xd079690P2EhIRg+vTpxc5LRCVn5/kEjF53Rq3tzNedUMnaTKZERFSWaNTnRpf9bZKTk6FUKuHsrH562dnZGYmJiQW+xt/fH+vWrUPfvn1hZmaGKlWqwN7eHr/88kuh+5k8eTJSU1Oln/j4eJ29ByLSvbXHb+YrbPaMe4OFDREVmUbFzbO7pXTp5YJJCFFoERUdHY2xY8di6tSpiIiIwO7duxEbG4vAwMBCt29ubg5bW1u1HyIqnT7ffBZfb78ozU8JqIsbMwNQuwovRRFR0Wl0WWr//v2oVKmSTnbs4OAAY2PjfGdpkpKS8p3NeSYkJAStW7fGhAkTAACNGjWCtbU12rRpg++++w5Vq1bVSTYiKlmTw8/h39gHuHE/Q2pbP6Il/Gs6yJiKiMoqjc7ctG3bFiYmGnfTKZCZmRl8fHywb98+tfZ9+/bB39+/wNdkZmbCyEg9srGxMQD9nFUiIv3KVaowe99VbDgZr1bY/PNZWxY2RKQ13VQqWgoKCsLAgQPh6+sLPz8/LF26FHFxcdJlpsmTJ+POnTtYs2YNAOCtt97CyJEjsWjRInTp0gUJCQkYN24cWrRoARcXFznfChEV0eXENFy79xifbIjMt2zhgGbwq1EZFdm/hoiKQdbipm/fvkhJScE333yDhIQENGjQADt37oS7uzsAICEhAXFxcdL6Q4YMQXp6OubPn4/PPvsM9vb26NChA3744Qe53gIRaaDPkuM4GVvw2HPz3m+KgIa8tExExacQ5ex6TlpaGuzs7JCamsrOxUQl5Js/o7HiaKxaW02nCvhPTQeM7+QNO0tTmZIRUVmhyfe3rGduiMhwCSFw/3E2Vhy5ma+wOTyxPdwqWcmUjIgMndbFTfv27eHu7o5Vq1ZJbYMHD0Z8fDz++ecfXWQjojJq94UEBP56Jl/7uhEt0cKzEkyNNR7WjoioyLQubjw8PPLdeu3q6prvbiYiKl8C10Zg98X8D+Lc8pEffNx18ygJIqJXYZ8bItKZPKUKNafskuZn92mMd5pVkzERERkK9rkhIll0nXtYmj47tTPsrNhRmIhKXpGLm3nz5hV5o89G7Sai8uFM3EMMDjuJ9Ow8qY2FDRHJpciXpTw9PYu2QYUCN27cKFYofeJlKSLdEkLAc/JOtbaIrzqicgVzmRIRkSHSy2Wp2NjY169EROVKRnYe3vrliDTfs4kLJnSpzcKGiGRVrD43OTk5iI2NhZeXl87GnCKisiExNQutQv6W5utWtcXcfk1lTERE9JRW921nZmZi+PDhsLKyQv369aUhEsaOHYvvv/9epwGJqPRJSH2iVtiYGiuwelhzGRMRET2nVXEzefJknD17FgcOHICFhYXU3rFjR2zatEln4Yio9Bm04iT8Qp4/qLNOFRtcmxEAJxuLV7yKiKjkaHUtadu2bdi0aRNatWoFhUIhtderVw8xMTE6C0dEpcvsvVdw6Op9ad7RxhzrRrSUMRERUX5aFTf379+Hk5NTvvaMjAy1YoeIDMfDjBzM++e6NM87ooiotNLqslTz5s2xY8cOaf5ZQbNs2TL4+fnpJhkRlRp5ShWafrtPml83oiULGyIqtbQ6cxMSEoKuXbsiOjoaeXl5mDt3Li5evIjjx4/j4MGDus5IRDI6ePU+Bq84Kc37e1VG65oOMiYiIno1rc7c+Pv74+jRo8jMzISXlxf27t0LZ2dnHD9+HD4+PrrOSEQySH2SixGrT6sVNgDYx4aISj0OnElEavZfScKnGyKRlpWn1j6nb2P0aspBMIlIHiUycKZSqcTWrVtx6dIlKBQK1K1bFz179uTD/IjKqJdH9H7G1sIEG0a1Qn0XOxlSERFpTqtK5MKFC+jZsycSExNRu3ZtAMDVq1fh6OiIP/74Aw0bNtRpSCLSH5VKYO7f1zD372tq7Z939ka/FtXhwI7DRFTGaFXcjBgxAvXr18fp06dRsWJFAMDDhw8xZMgQjBo1CsePH9dpSCLSD6VKwOtL9UEvXewscOSLDjAy4mMdiKhs0qq4OXv2rFphAwAVK1bEjBkz0Lw5H8FOVFb0W6r+h8iqoc3Rrnb+Z1gREZUlWt0tVbt2bdy7dy9fe1JSEmrWrFnsUESkf1fvpePUzYfSfGxIAAsbIjIIRS5u0tLSpJ+ZM2di7Nix+P3333H79m3cvn0bv//+O8aNG4cffvhBn3mJSAfiH2Si85xD0vyhCe35dHEiMhhFvixlb2+v9stPCIE+ffpIbc/uKH/rrbegVCp1HJOIdCErV4kP10bg4AvjQzVwtUX1ylYypiIi0q0iFzf79+/XZw4i0rPbDzPxnx/U/z+2NjNG+EetZUpERKQfRS5u2rZtq88cRKQnQgj854f9uPPoiVr74Ynt4VaJZ2yIyPAU64l7mZmZiIuLQ05Ojlp7o0aNihWKiHRn9r6raoVNs+r22DCqFcxNjGVMRUSkP1oVN/fv38fQoUOxa1f+p5kCYJ8bolIgO0+JkJ2XserYTantynddWdQQkcHT6lbwcePG4eHDhzhx4gQsLS2xe/durF69GrVq1cIff/yh64xEpCEhBGp/tVutsNnykT8LGyIqF7Q6c/PPP/9g+/btaN68OYyMjODu7o5OnTrB1tYWISEh6N69u65zEpEGBr00kvfSgT7wca9YyNpERIZFq+ImIyMDTk5PH/ZVqVIl3L9/H97e3mjYsCHOnDmj04BEpJmUx9k4fC1Zmo+ZGQBjDqVAROWI1k8ovnLlCgCgSZMmWLJkCe7cuYPFixejatWqOg1IREWnUgn4fPeXNH/ki/YsbIio3NHqzM24ceOQkJAAAAgODkaXLl2wbt06mJmZYdWqVbrMR0RFkJ2nRO2vdqu1mRgpUK0ib/UmovJHIZ49WrgYMjMzcfnyZVSvXh0ODg66yKU3aWlpsLOzQ2pqKmxtbeWOQ6QTDaftQXpWnlob74wiIkOiyfd3sZ5z84yVlRWaNWumi00RkYY+++2sWmFz8ss3UbmCOS9HEVG5VeTiJigoqMgbnT17tlZhiKjoTt98gHGbonD74fMH9J2d2hl2VqYypiIikl+Ri5vIyMgirceRhYn0b/eFBAT+qn5n4qkpHVnYEBGBA2cSlTl/X7qnVtj0aOyCb3rWh72VmYypiIhKD530uSGikrHjXALGrH9e2Pz0XmP09qkmYyIiotJHq+fcEJE8XixsQt5pyMKGiKgAPHNDVIoJIRBzPwM3kzMwYs1pqX3ZIF90qucsYzIiotKLxQ1RKXUpIQ3d5h4ucFnHuk4lnIaIqOzgZSmiUuheWlaBhc0gP3fcmBnAuxKJiF5B6zM3a9euxeLFixEbG4vjx4/D3d0doaGh8PT0RM+ePXWZkahc6b/sBI7FpEjz/l6VsX5kKxkTERGVLVqduVm0aBGCgoIQEBCAR48eQalUAgDs7e0RGhqqy3xE5covf19TK2yqV7LC2uEtZUxERFT2aFXc/PLLL1i2bBmmTJkCY+PnY9f4+vri/PnzOgtHVF5k5Sqx7NAN/LzvqtR2blpnHJrIUb2JiDSl1WWp2NhYNG3aNF+7ubk5MjIyih2KqDyJS8nEGz+qPyRz16dtYGvBpw0TEWlDqzM3np6eiIqKyte+a9cu1KtXr7iZiMqN1Ce5+QqbFUN8UbcqR6wnItKWVmduJkyYgDFjxiArKwtCCJw8eRIbNmxASEgIli9fruuMRAZp5dFYTP8zWppvVaMSNo7ykzEREZFh0Kq4GTp0KPLy8jBx4kRkZmaif//+cHV1xdy5c9GvXz9dZyQyOHlKlVphAwBLPvCVKQ0RkWHR+lbwkSNHYuTIkUhOToZKpYKTEx8qRlRUB67cl6b/CnoDNZ1sZExDRGRYtOpzM336dMTExAAAHBwcWNgQFVF2nhKBayPUhlJgYUNEpFtaFTdbtmyBt7c3WrVqhfnz5+P+/fuvfxFRObfzfAJqf7Ubuy8mSm3Te9SXMRERkWHSqrg5d+4czp07hw4dOmD27NlwdXVFQEAA1q9fj8zMTF1nJCrTElOz4DFpB0avO6PWvntcGwz295AnFBGRAVMIIURxN3L06FGsX78emzdvRlZWFtLS0nSRTS/S0tJgZ2eH1NRU2NrydlvSr6xcJep8vVut7du3G2BgK3eZEhERlU2afH/rZFRwa2trWFpawszMDOnp6brYJFGZF/8gE21mPX+GTR/fapjRqyFMjTleLRGRPmn9WzY2NhYzZsxAvXr14OvrizNnzmDatGlITEx8/YtfsHDhQnh6esLCwgI+Pj44fDj/SMgvys7OxpQpU+Du7g5zc3N4eXlhxYoV2r4NIr3IzMlTK2zqVrXFrN6NWdgQEZUArc7c+Pn54eTJk2jYsCGGDh0qPedGU5s2bcK4ceOwcOFCtG7dGkuWLEG3bt0QHR2N6tWrF/iaPn364N69ewgLC0PNmjWRlJSEvLw8bd4GkV5cvZeOznMOSfM9Grtg3vv5hyshIiL90KrPzZdffokBAwagfv3i3enRsmVLNGvWDIsWLZLa6tati7fffhshISH51t+9ezf69euHGzduoFKlSkXaR3Z2NrKzs6X5tLQ0uLm5sc8N6UV2nhK1v1LvY3Pz++4ypSEiMhya9LnR6hz5zJkzi13Y5OTkICIiAp07d1Zr79y5M44dO1bga/744w/4+vpi1qxZcHV1hbe3Nz7//HM8efKk0P2EhITAzs5O+nFzcytWbqJXebGwecPbEVe/6yZjGiKi8qnIl6WCgoLw7bffwtraGkFBQa9cd/bs2a/dXnJyMpRKJZydndXanZ2dC+23c+PGDRw5cgQWFhbYunUrkpOTMXr0aDx48KDQfjeTJ09Wy/vszA2Rrv196Z7a/JphLWRKQkRUvhW5uImMjERubq40rSsKhUJtXgiRr+0ZlUoFhUKBdevWwc7ODsDTQqp3795YsGABLC0t873G3Nwc5ubmOstL9DKVSuDw9WQMX/38qcM3ZgbImIiIqHwrcnGzf//+Aqe15eDgAGNj43xnaZKSkvKdzXmmatWqcHV1lQob4GkfHSEEbt++jVq1ahU7F5EmzsQ9xDsL1S+jznu/KYyMCi7QiYhI/7TqczNs2LACn2eTkZGBYcOGFWkbZmZm8PHxwb59+9Ta9+3bB39//wJf07p1a9y9exePHz+W2q5evQojIyNUq1ZNg3dAVHwDlp/IV9h83tkbPRq7yJSIiIgALYub1atXF9iJ98mTJ1izZk2RtxMUFITly5djxYoVuHTpEsaPH4+4uDgEBgYCeNpfZtCgQdL6/fv3R+XKlTF06FBER0fj0KFDmDBhAoYNG1bgJSkifYm5/xhHr6dI8+82q4ab33fHxx149pCISG4aPecmLS0NQggIIZCeng4LCwtpmVKpxM6dOzUaIbxv375ISUnBN998g4SEBDRo0AA7d+6Eu/vTR9MnJCQgLi5OWr9ChQrYt28fPvnkE/j6+qJy5cro06cPvvvuO03eBlGxXLybiu7zjjyfn94F1uY6edg3ERHpgEbPuTEyMiq0sy/wtHPw9OnTMWXKFJ2E0weOLUXaEkJg3t/XMeevq1LbmPZemNCljoypiIjKB72NLbV//34IIdChQwds2bJF7UF6ZmZmcHd3h4sL+xuQ4VGqBIavPoUDV+5LbQNbubOwISIqhTQqbtq2bQvg6bhS1atXf+VZHCJDMnZDpFphM6dvY/RorPmQI0REpH9FLm7OnTuHBg0awMjICKmpqTh//nyh6zZq1Egn4YhKg/SsXOw4nyDNH/i8HTwcrGVMREREr1Lk4qZJkyZITEyEk5MTmjRpAoVCgYK66ygUCiiVSp2GJJJTux8PSNM7x7ZhYUNEVMoVubiJjY2Fo6OjNE1UHmyPuoOUjBxpvp4LO6ETEZV2RS5unt2e/fI0kaFKeZyNTzdGSfNnvu4kXxgiIioyrR/it2PHDml+4sSJsLe3h7+/P27duqWzcERyyczJg893f0nzM3s1RCVrMxkTERFRUWlV3MycOVN6IvDx48cxf/58zJo1Cw4ODhg/frxOAxKVlDylCjN3XoLHpB2oN3WP1N6omh36t6wuYzIiItKEVo9VjY+PR82aNQEA27ZtQ+/evTFq1Ci0bt0a7dq102U+ohJx7V46Os05lK+9Qx0nrBjSXIZERESkLa2KmwoVKiAlJQXVq1fH3r17pbM1FhYWBY45RVSaLToQgx92X1ZrW/xBM/jXdICthalMqYiISFtaFTedOnXCiBEj0LRpU1y9ehXdu3cHAFy8eBEeHh66zEekV5cS0tQKmy71nRHatykszYxlTEVERMWhVZ+bBQsWwM/PD/fv38eWLVtQuXJlAEBERATef/99nQYk0qf3Fh+XpreNaY0lA31Z2BARlXEaDZxpCDhwJj3z/a7LWHwwBgAwrLUnpr5VT+ZERERUGL0NnPmiR48eISwsDJcuXYJCoUDdunUxfPhw2NnZabtJohJzPz1bKmwAYEKX2jKmISIiXdLqstTp06fh5eWFOXPm4MGDB0hOTsacOXPg5eWFM2fO6Dojkc41n/H8GTaHJrTnpSgiIgOi1Zmb8ePHo0ePHli2bBlMTJ5uIi8vDyNGjMC4ceNw6FD+W2qJSqNG1exQvbKV3DGIiEiHtCpuTp8+rVbYAICJiQkmTpwIX19fnYUj0ofrSenS9KIPfGRMQkRE+qDVZSlbW1vExcXla4+Pj4eNjU2xQxHpS1pWLjrOfn5m0dXeUsY0RESkD1oVN3379sXw4cOxadMmxMfH4/bt29i4cSNGjBjBW8Gp1FKpBBpN2yvND+CQCkREBkmry1I//fQTFAoFBg0ahLy8PACAqakpPvroI3z//fc6DUikK1O2nZemvZ0rYEavhjKmISIifSnWc24yMzMRExMDIQRq1qwJK6vS3zGTz7kpn3acS8CY9c/v5LsxMwBGRgoZExERkSY0+f7W6LJUZmYmxowZA1dXVzg5OWHEiBGoWrUqGjVqVCYKGyo/8pQq3H6YidsPMzF6XYRaYbPlIz8WNkREBkyjy1LBwcFYtWoVBgwYAAsLC2zYsAEfffQRNm/erK98RBpLzcxF42/2FrhsTt/G8HGvVMKJiIioJGlU3ISHhyMsLAz9+vUDAHzwwQdo3bo1lEoljI35EDSSX8rjbPh895dam4WpEYwUChz9ogMqWpvJlIyIiEqKRsVNfHw82rRpI823aNECJiYmuHv3Ltzc3HQejkhTLxY2Heo4YcWQ5jKmISIiOWjU50apVMLMTP0vXxMTE+mOKSI5LT30fKyoNrUcWNgQEZVTGp25EUJgyJAhMDc3l9qysrIQGBgIa2trqS08PFx3CYmKYOXRWMzceVmaXzu8pYxpiIhIThoVN4MHD87X9sEHH+gsDJE2Dl69j+l/Rkvz4aP9ZUxDRERy06i4Wblypb5yEGklNjkDg1eclOZnvdsIzapXlDERERHJTasnFBOVBhfupOK/vxyR5n/s3Qjv+bJjOxFReVfkDsWBgYGIj48v0rqbNm3CunXrtA5F9CoX76bC+6tdaoVNp3rOLGyIiAiABmduHB0d0aBBA/j7+6NHjx7w9fWFi4sLLCws8PDhQ0RHR+PIkSPYuHEjXF1dsXTpUn3mpnIqcG0Edl9MVGt7v4UbpvdoIFMiIiIqbTQaWyopKQlhYWHYuHEjLly4oLbMxsYGHTt2xKhRo9C5c2edB9UVji1VtnlM2iFNN6pmh7XDW8LO0lTGREREVBI0+f7WeuDMR48e4datW3jy5AkcHBzg5eUFhaL0j9fD4qbs+nDtaey5eA8AsH5kS/h7OciciIiISoom399adyi2t7eHvb29ti8n0sipmw+kwgYACxsiIioU75aiUm3SlnPYeEq9I/u2Ma1lSkNERGWBRsMvEJWkOfuu5itsPnyjBpq42csTiIiIygSeuaFSa+7f16TpxR80Q/s6TjA34ejzRET0aixuqFR6nP18MNZhrT3RtUFVGdMQEVFZovVlqby8PPz1119YsmQJ0tPTAQB3797F48ePdRaOyq9hq05J0+M71ZIxCRERlTVanbm5desWunbtiri4OGRnZ6NTp06wsbHBrFmzkJWVhcWLF+s6J5UjDzJycDL2gTRvY8Hn2BARUdFpdebm008/ha+vLx4+fAhLS0upvVevXvj77791Fo7Kn3cXHUOzb/dJ80cndZAxDRERlUVanbk5cuQIjh49CjMzM7V2d3d33LlzRyfBqPxJfZKLiFsPpfkWHpXgam/5ilcQERHlp1Vxo1KpoFQq87Xfvn0bNjY2xQ5F5dN3/4uWpqOmdoK9ldkr1iYiIiqYVpelOnXqhNDQUGleoVDg8ePHCA4ORkBAgK6yUTmzOeK2NM3ChoiItKXVmZs5c+agffv2qFevHrKystC/f39cu3YNDg4O2LBhg64zUjnQMHiPNB3UyVvGJEREVNZpVdy4uLggKioKGzduREREBFQqFYYPH44BAwaodTAmKorBK04i/YXn2gS29ZIxDRERlXVajQp+6NAh+Pv7w8REvTbKy8vDsWPH8MYbb+gsoK5xVPDS5ee9V/DLP9el+diQgDIxujwREZUsTb6/tepz0759ezx48CBfe2pqKtq3b6/NJqkcyspVqhU2Z77uxMKGiIiKTaviRghR4JdQSkoKrK2tix2KDJ8QAnW+3i3NLx/ki0rW7ERMRETFp1Gfm3feeQfA07ujhgwZAnNzc2mZUqnEuXPn4O/vr9uEZHDup2ejzax/pHkLUyN0rOcsYyIiIjIkGhU3dnZ2AJ7+1W1jY6PWedjMzAytWrXCyJEjdZuQDEp2nhLNZ/yl1nbpm64ypSEiIkOkUXGzcuVKAICHhwc+//xzXoIijU3ddlGablbdHr+OaMl+NkREpFNa3S1VlvFuKXmoVALjNkXhj7N3pbab33eXMREREZUlmnx/a/WcGwD4/fff8dtvvyEuLg45OTlqy86cOaPtZskA/bD7MhYdiFFr+/Pj/8iUhoiIDJ1Wd0vNmzcPQ4cOhZOTEyIjI9GiRQtUrlwZN27cQLdu3XSdkcq4lwubA5+3Q8NqdjKlISIiQ6dVcbNw4UIsXboU8+fPh5mZGSZOnIh9+/Zh7NixSE1N1Xhbnp6esLCwgI+PDw4fPlyk1x09ehQmJiZo0qSJFu+ASsrGk3HSdPhof9z8vjs8HNhXi4iI9Eer4iYuLk665dvS0hLp6ekAgIEDB2o0ttSmTZswbtw4TJkyBZGRkWjTpg26deuGuLi4V74uNTUVgwYNwptvvqlNfCohOXkqTAo/L803q15RxjRERFReaFXcVKlSBSkpKQAAd3d3nDhxAgAQGxsLTfonz549G8OHD8eIESNQt25dhIaGws3NDYsWLXrl6z788EP0798ffn5+2sSnEpCrVMH7q13S/Ffd68qYhoiIyhOtipsOHTrgzz//BAAMHz4c48ePR6dOndC3b1/06tWrSNvIyclBREQEOnfurNbeuXNnHDt2rNDXrVy5EjExMQgODi7SfrKzs5GWlqb2Q/o3flOUNG2kAEa0qSFfGCIiKle0ultq6dKlUKlUAIDAwEBUqlQJR44cwVtvvYXAwMAibSM5ORlKpRLOzupPpnV2dkZiYmKBr7l27RomTZqEw4cP5xu0szAhISGYPn16kdal4svKVaoNqwAAMTMDZEpDRETlkVbFjZGREYyMnp/06dOnD/r06QMAuHPnDlxdXYu8rZcf4FbYuFVKpRL9+/fH9OnT4e3tXeTtT548GUFBQdJ8Wloa3Nzcivx60szLhc2Osf/hQ/qIiKhEaf2cm5clJiZixowZWL58OZ48efLa9R0cHGBsbJzvLE1SUlK+szkAkJ6ejtOnTyMyMhIff/wxAEClUkEIARMTE+zduxcdOnTI9zpzc3O1MbBIf+JSMqVpG3MTHJ3cAbYWpjImIiKi8kijPjePHj3CgAED4OjoCBcXF8ybNw8qlQpTp05FjRo1cOLECaxYsaJI2zIzM4OPjw/27dun1r5v374CB9+0tbXF+fPnERUVJf0EBgaidu3aiIqKQsuWLTV5K6RjuUoV3vhxvzR/fnoXFjZERCQLjc7cfPnllzh06BAGDx6M3bt3Y/z48di9ezeysrKwa9cutG3bVqOdBwUFYeDAgfD19YWfnx+WLl2KuLg4qd/O5MmTcefOHaxZswZGRkZo0KCB2uudnJxgYWGRr51KVvyDTLSZ9bywaelZScY0RERU3mlU3OzYsQMrV65Ex44dMXr0aNSsWRPe3t4IDQ3Vaud9+/ZFSkoKvvnmGyQkJKBBgwbYuXMn3N3dAQAJCQmvfeYNye+9xcfV5lcPayFTEiIiIg0HzjQ1NcWtW7fg4uICALCyssLJkyfL1JkTDpypW6uP3UTwH09H+jYzMcKpKR1hZ8nLUUREpFuafH9r1OdGpVLB1PT5F5exsTGsrfko/fLsWWEDAAcntGNhQ0REstPospQQAkOGDJHuPsrKykJgYGC+Aic8PFx3CanUOnj1vjQ99b/1UNXOUsY0RERET2lU3AwePFht/oMPPtBpGCo7bqVkYPCKk9J8/5bVZUxDRET0nEbFzcqVK/WVg8qYsRujpOlP36wFC1Nj+cIQERG9QKuxpah8u/voCc7GPwIA+LhXxPhORX9iNBERkb6xuCGN+X//jzQ9q3cjGZMQERHlx+KGNJKWlStNN3Gzh5djBRnTEBER5cfihjTy26l4aXrDyFYyJiEiIiqYzgbOJMM3Ofw8Npx8/sRoSzN2IiYiotJH6zM3a9euRevWreHi4oJbt24BAEJDQ7F9+3adhaPS415allphs2hAMxnTEBERFU6r4mbRokUICgpCQEAAHj16BKVSCQCwt7fXepwpKt1azvxbmo74qiO6NawqYxoiIqLCaVXc/PLLL1i2bBmmTJkCY+PnlyZ8fX1x/vx5nYUj+alUAh6TdkjzzT0qonIFcxkTERERvZpWxU1sbCyaNm2ar93c3BwZGRnFDkWlh9eUnWrzv33oJ1MSIiKiotGquPH09ERUVFS+9l27dqFevXrFzUSlROqTXLw4Zvy1Gd2gUCjkC0RERFQEWt0tNWHCBIwZMwZZWVkQQuDkyZPYsGEDQkJCsHz5cl1nJJk0nr5Xmr42oxtMjfnkACIiKv20Km6GDh2KvLw8TJw4EZmZmejfvz9cXV0xd+5c9OvXT9cZSWYWpkYsbIiIqMzQ+jk3I0eOxMiRI5GcnAyVSgUnJydd5iKZrTl+U5re9ekb8gUhIiLSkFZ/jk+fPh0xMTEAAAcHBxY2Bmjq9ovStEdlKxmTEBERaUar4mbLli3w9vZGq1atMH/+fNy/f1/XuUhGh64+/zxHt/NiJ2IiIipTtCpuzp07h3PnzqFDhw6YPXs2XF1dERAQgPXr1yMzM1PXGakECSEwaMVJaX5Cl9oypiEiItKc1r1E69evj5kzZ+LGjRvYv38/PD09MW7cOFSpUkWX+aiEnbjxQJpeOaQ5z9oQEVGZo5NbYKytrWFpaQkzMzPk5ubqYpMkk3GbIqXp9nXYl4qIiMoerYub2NhYzJgxA/Xq1YOvry/OnDmDadOmITExUZf5qAR9v+sy7qVlAwD+U9NB5jRERETa0epWcD8/P5w8eRINGzbE0KFDpefcUNn1y9/XsPhgjDQ/u09jGdMQERFpT6vipn379li+fDnq16+v6zwkg+MxKfh531VpfnOgH5xsLWRMREREpD2FEC+OHmT40tLSYGdnh9TUVNja2sodR3ZpWbloNO35MAvho/3RrHpFGRMRERHlp8n3d5HP3AQFBeHbb7+FtbU1goKCXrnu7Nmzi7pZktm8v65J0wsHNGNhQ0REZV6Ri5vIyEjpTqjIyMjXrE1lgRACy4/EAgBc7S0R0LCqzImIiIiKr8jFzf79+wucprLrkw3Pi9RverL/FBERGQatbgUfNmwY0tPT87VnZGRg2LBhxQ5F+vfTniv437kEaf7Nus4ypiEiItIdrYqb1atX48mTJ/nanzx5gjVr1hQ7FOlXRnYe5u+/Ls3/75P/yJiGiIhItzS6FTwtLQ1CCAghkJ6eDguL57cLK5VK7Ny5kyOElwEbTsZJ0wc+bwcPB2sZ0xAREemWRsWNvb09FAoFFAoFvL298y1XKBSYPn26zsKR7mVk5+G7HZcAALYWJixsiIjI4GhU3Ozfvx9CCHTo0AFbtmxBpUqVpGVmZmZwd3eHi4uLzkOS7pyMfT4w5vz+zWRMQkREpB8aFTdt27YF8HRcqerVq3PE6DIoPPIOAMDESIE3vB1lTkNERKR7RS5uzp07hwYNGsDIyAipqak4f/58oes2atRIJ+FIt1QqgT/P3gUANHazlzcMERGRnhS5uGnSpAkSExPh5OSEJk2aQKFQoKCRGxQKBZRKpU5Dkm5si7ojTY9s4yljEiIiIv0pcnETGxsLR0dHaZrKnqDfzkrTXRvwacRERGSYilzcuLu7FzhNZcP1pOcPXZzYtbaMSYiIiPRL64f47dixQ5qfOHEi7O3t4e/vj1u3buksHOlGXEomOs4+JM1/+IaXjGmIiIj0S6viZubMmbC0tAQAHD9+HPPnz8esWbPg4OCA8ePH6zQgFc+1e+l448fnY4ENbOUOYyPe5UZERIZLo1vBn4mPj0fNmjUBANu2bUPv3r0xatQotG7dGu3atdNlPioGlUqg05znZ2x6NHbBt283kDERERGR/ml15qZChQpISUkBAOzduxcdO3YEAFhYWBQ45hTJ4635R6Tp3j7VMO/9pjKmISIiKhlanbnp1KkTRowYgaZNm+Lq1avo3r07AODixYvw8PDQZT4qhot306Tpn95rLGMSIiKikqPVmZsFCxbAz88P9+/fx5YtW1C5cmUAQEREBN5//32dBiTtnIl7KE2vHNJcxiREREQlSyEKehKfAUtLS4OdnR1SU1Nha2srdxy98Zj0/G62y992hYWpsYxpiIiIikeT72+tLksBwKNHjxAWFoZLly5BoVCgbt26GD58OOzs7LTdJOnI+dup0vSI/3iysCEionJFq8tSp0+fhpeXF+bMmYMHDx4gOTkZc+bMgZeXF86cOaPrjKSB8DO31ToSf/XfejKmISIiKnlanbkZP348evTogWXLlsHE5Okm8vLyMGLECIwbNw6HDh16zRZIH+IfZKoNsfBF1zoypiEiIpKHVn1uLC0tERkZiTp11L88o6Oj4evri8zMTJ0F1DVD7XOjVAl4fblTmt8c6IfmHpVkTERERKQ7mnx/a3VZytbWFnFxcfna4+PjYWNjo80mqZj+uZwkTfdvWZ2FDRERlVtaFTd9+/bF8OHDsWnTJsTHx+P27dvYuHEjRowYwVvBZbLnYqI0PYNPISYionJMqz43P/30ExQKBQYNGoS8vDwAgKmpKT766CN8//33Og1Ir5erVOH3iNsAgLbejlAoOHYUERGVX1oVN2ZmZpg7dy5CQkIQExMDIQRq1qwJKysrXeejIjhyPVma7t6oqoxJiIiI5KfRZanMzEyMGTMGrq6ucHJywogRI1C1alU0atSIhY2Mdp1PkKZ7NXWVMQkREZH8NCpugoODsWrVKnTv3h39+vXDvn378NFHH+krGxXRgSv3AQDNqtvD1FirblREREQGQ6NvwvDwcISFhWHp0qWYN28eduzYgW3btkGpVGodYOHChfD09ISFhQV8fHxw+PDhV+6/U6dOcHR0hK2tLfz8/LBnzx6t920IfjsVj6T0bADAYH8PecMQERGVAhoVN/Hx8WjTpo0036JFC5iYmODu3bta7XzTpk0YN24cpkyZgsjISLRp0wbdunUr8DZzADh06BA6deqEnTt3IiIiAu3bt8dbb72FyMhIrfZvCCZuOSdNd65XRcYkREREpYNGD/EzNjZGYmIiHB0dpTYbGxucO3cOnp6eGu+8ZcuWaNasGRYtWiS11a1bF2+//TZCQkKKtI369eujb9++mDp1apHWN6SH+GXm5KHe1KdnrjaNaoWWNSrLnIiIiEg/9DZwphACQ4YMgbm5udSWlZWFwMBAWFtbS23h4eGv3VZOTg4iIiIwadIktfbOnTvj2LFjRcqjUqmQnp6OSpUKf2BddnY2srOzpfm0tLQibbss+GnPVWm6hScf2kdERARoWNwMHjw4X9sHH3yg1Y6Tk5OhVCrh7Oys1u7s7IzExMRCXqXu559/RkZGBvr06VPoOiEhIZg+fbpWGUu7FUdjpWk+24aIiOgpjYqblStX6jzAy1/KQogifVFv2LAB06ZNw/bt2+Hk5FToepMnT0ZQUJA0n5aWBjc3N+0DlxJn4x9J02uHt5AvCBERUSmj1UP8dMHBwUHqw/OipKSkfGdzXrZp0yYMHz4cmzdvRseOHV+5rrm5udplNEPx7f+ipen/1HSQMQkREVHpIttDUczMzODj44N9+/apte/btw/+/v6Fvm7Dhg0YMmQI1q9fj+7du+s7Zql0OTENp289BAB0qe/MS1JEREQvkO3MDQAEBQVh4MCB8PX1hZ+fH5YuXYq4uDgEBgYCeHpJ6c6dO1izZg2Ap4XNoEGDMHfuXLRq1Uo662NpaQk7OzvZ3kdJG77qtDQ9uVtdGZMQERGVPrIWN3379kVKSgq++eYbJCQkoEGDBti5cyfc3d0BAAkJCWrPvFmyZAny8vIwZswYjBkzRmofPHgwVq1aVdLxZaFUCdx59AQA4OVoDQ8H69e8goiIqHzR6Dk3hqCsP+fm2PVk9F/+LwDg8MT2cKvEMb2IiMjwafL9rXWfm7Vr16J169ZwcXHBrVu3AAChoaHYvn27tpukIvjz3POnQbOwISIiyk+r4mbRokUICgpCQEAAHj16JI0tZW9vj9DQUF3moxcIIbDhZDwAwMZc1iuKREREpZZWxc0vv/yCZcuWYcqUKTA2NpbafX19cf78eZ2FI3Xf77osTc98p6GMSYiIiEovrYqb2NhYNG3aNF+7ubk5MjIyih2K8jt/OxVLDt2Q5t9q7CJjGiIiotJLq+LG09MTUVFR+dp37dqFevXqFTcTvSQnT4W35h+R5rd85CdjGiIiotJNq44bEyZMwJgxY5CVlQUhBE6ePIkNGzYgJCQEy5cv13XGcm/SlnPS9Jj2XvBx5yCZREREhdGquBk6dCjy8vIwceJEZGZmon///nB1dcXcuXPRr18/XWcs9/ZfSZKmJ3SpI2MSIiKi0k/rW25GjhyJkSNHIjk5GSqV6pWDV5L27qVl4WFmLgBgeo/6MqchIiIq/Yp9P7GDAwdt1KeWM/+WpnuwEzEREdFraVXceHp6vnKwxhs3bhS6jIpue9QdabqWUwVUtDaTMQ0REVHZoFVxM27cOLX53NxcREZGYvfu3ZgwYYIuchGAvy4972vz5yf/kTEJERFR2aFVcfPpp58W2L5gwQKcPn26wGWkuT/PPh1qoWcTF1iYGr9mbSIiIgKKMbZUQbp164YtW7bocpPl0oU7qfCYtEOa96tRWcY0REREZYtOi5vff/8dlSrxGSzFcfFuKv77yxG1tj6+bjKlISIiKnu0uizVtGlTtQ7FQggkJibi/v37WLhwoc7ClTe3H2ai+7znhU3PJi6Y06cJjIwK77xNRERE6rQqbt5++221eSMjIzg6OqJdu3aoU4cPmdNGZk4e/vPDfmn+l/ebcvwoIiIiLWhc3OTl5cHDwwNdunRBlSpV9JGpXDoZ+0CantClNgsbIiIiLWnc58bExAQfffQRsrOz9ZGn3Bq3KQoAYGFqhDHta8obhoiIqAzTqkNxy5YtERkZqess5dqj/x9igYiIiIpHqz43o0ePxmeffYbbt2/Dx8cH1tbWassbNWqkk3Dlxbnbj6Tp5YOayxeEiIjIAGhU3AwbNgyhoaHo27cvAGDs2LHSMoVCASEEFAoFlEqlblMauB/3XJGm/b34TBsiIqLi0Ki4Wb16Nb7//nvExsbqK0+5VMH86cdQvZIVb/smIiIqJo2KGyEEAMDd3V0vYcqjJzlK7LqQCAAY+2YtmdMQERGVfRp3KH7VaOCkmYzsPNSduluar8xRv4mIiIpN4w7F3t7ery1wHjx48Mrl9FSbWfvV5tvXcZIpCRERkeHQuLiZPn067Ozs9JGlXLnz6AkeZORI87EhATKmISIiMhwaFzf9+vWDkxPPMBTX5tPx0vTucW14uY+IiEhHNOpzwy9g3XiUmYPQv64BAHzdK6JOFVuZExERERkOjYqbZ3dLUfH0XXJCmm5d00HGJERERIZHo8tSKpVKXznKjZw8Fa7cS5fmP+nAcaSIiIh0SauxpUh73ecdlqb//qwtTIz5ERAREekSv1lL2LWkx9K0l2MFGZMQEREZJhY3JczS1BgAsG5ES5mTEBERGSYWNyVo+eEbeJL7dFDRmk48a0NERKQPLG5K0Hc7LknTlTjUAhERkV6wuCkh99Ozpenw0f4wZUdiIiIiveA3bAn57YUnEjdy5fAVRERE+sLipgQoVQI/7rkC4OnI37z9m4iISH/4LVsCYpMzpOkvA+rKmISIiMjwsbgpAbdSnhc37/pUkzEJERGR4WNxUwKeXZJyqGAucxIiIiLDx+JGz5QqgcuJT8eSql7JUuY0REREho/FjZ7dffREmp7Vu5GMSYiIiMoHFjd6tuN8gjRd08lGxiRERETlg4ncAQyd2f/f9m1spJA5CZH+CSGQl5cHpVIpdxQiKoNMTU1hbGxc7O2wuNGzb/4XDQB4q1FVmZMQ6VdOTg4SEhKQmZkpdxQiKqMUCgWqVauGChWKN/4iixs9CjsSK00rFDxzQ4ZLpVIhNjYWxsbGcHFxgZmZGf/NE5FGhBC4f/8+bt++jVq1ahXrDA6LGz06fO2+NP3t2w1kTEKkXzk5OVCpVHBzc4OVlZXccYiojHJ0dMTNmzeRm5tbrOKGHYr16Mi1ZADAsNaeqGDOOpIMn5ERf6UQkfZ0dcaXv4n0RAiBPJUAANSuUrxrh0RERFR0LG70QAiBpt/uk+a9nXkLOBERUUlhcaMHaU/y8CgzV5pv4GonYxoikpuHhwdCQ0O1fv2qVatgb2+vszxl1c2bN6FQKBAVFaX3feXk5KBmzZo4evSo3vdVXiQlJcHR0RF37tzR+75Y3OjBzgvPH9x3fUY3mBrzMBOVVkOGDMHbb7+t132cOnUKo0aNKtK6BRVCffv2xdWrV4u8v3bt2kGhUEChUMDMzAxeXl6YPHkysrOzNYld6ri5uSEhIQENGuj/Bo2lS5fC3d0drVu3zrds1KhRMDY2xsaNG/MtK+zfU1RUFBQKBW7evCm1CSGwdOlStGzZEhUqVIC9vT18fX0RGhqq10cqPHz4EAMHDoSdnR3s7OwwcOBAPHr06JWvefbv6eWfH3/8UVrnww8/hJeXFywtLeHo6IiePXvi8uXL0nInJycMHDgQwcHB+nprEn7r6sG0Py5K0yYsbIjKPUdHx2LdRWZpaQknJyeNXjNy5EgkJCTg+vXrmDVrFhYsWIBp06ZpnaEolEolVCqV3rZvbGyMKlWqwMRE/zdo/PLLLxgxYkS+9szMTGzatAkTJkxAWFhYsfYxcOBAjBs3Dj179sT+/fsRFRWFr7/+Gtu3b8fevXuLte1X6d+/P6KiorB7927s3r0bUVFRGDhw4Ctfk5CQoPazYsUKKBQKvPvuu9I6Pj4+WLlyJS5duoQ9e/ZACIHOnTurPdRz6NChWLduHR4+fKi39wcAEOVMamqqACBSU1P1sv2Ux9nC/Yv/Cfcv/icW7L+ml30QlTZPnjwR0dHR4smTJ1KbSqUSGdm5svyoVKoiZx88eLDo2bNnocsPHDggmjdvLszMzESVKlXEF198IXJzc6XlaWlpon///sLKykpUqVJFzJ49W7Rt21Z8+umn0jru7u5izpw50nxwcLBwc3MTZmZmomrVquKTTz4RQgjRtm1bAUDtRwghVq5cKezs7NRybd++Xfj4+Ahzc3NRuXJl0atXL2nZy/sXQoh33nlHNGvWTJpXqVTihx9+EJ6ensLCwkI0atRIbN68Od8+atasKSwsLES7du3EqlWrBADx8OFDtVx//vmnqFu3rjA2NhY3btwQ2dnZYsKECcLFxUVYWVmJFi1aiP3790vbvXnzpvjvf/8r7O3thZWVlahXr57YsWOHEEKIBw8eiP79+wsHBwdhYWEhatasKVasWCGEECI2NlYAEJGRkUX+fNq2bSs++eQTMWHCBFGxYkXh7OwsgoOD83/QL4iIiBBGRkYFfk+sWrVKtGrVSjx69EhYWlqK2NhYteWF/XuKjIwUAKT1N23aJACIbdu25VtXpVKJR48evTKjtqKjowUAceLECant+PHjAoC4fPlykbfTs2dP0aFDh1euc/bsWQFAXL9+Xa3dw8NDhIWFFfiagn6XPKPJ9zfvT9axjrMPStND/T1lTEIkrye5StSbukeWfUd/0wVWZsX/9Xbnzh0EBARgyJAhWLNmDS5fvoyRI0fCwsJCOgsSFBSEo0eP4o8//oCzszOmTp2KM2fOoEmTJgVu8/fff8ecOXOwceNG1K9fH4mJiTh79iwAIDw8HI0bN8aoUaMwcuTIQnPt2LED77zzDqZMmYK1a9ciJycHO3bsKHT9s2fP4ujRo/Dw8JDavvrqK4SHh2PRokWoVasWDh06hA8++ACOjo5o27Ytbt68id69e+PTTz/FiBEjEBkZic8//zzftjMzMxESEoLly5ejcuXKcHJywtChQ3Hz5k1s3LgRLi4u2Lp1K7p27Yrz58+jVq1aGDNmDHJycnDo0CFYW1sjOjpaeiLt119/jejoaOzatQsODg64fv06njx5km+/Rf18AGD16tUICgrCv//+i+PHj2PIkCFo3bo1OnXqVOB2Dx06BG9vb9ja2uZbFhYWhg8++AB2dnYICAjAypUrMX369EKPfWHWrVuH2rVro2fPnvmWKRQK2NkV3lfzdU/vbdOmDXbt2lXgsuPHj8POzg4tW7aU2lq1agU7OzscO3YMtWvXfm32e/fuYceOHVi9enWh62RkZGDlypXw9PSEm5ub2rIWLVrg8OHDGDZs2Gv3pS3Zi5uFCxfixx9/REJCAurXr4/Q0FC0adOm0PUPHjyIoKAgXLx4ES4uLpg4cSICAwNLMHHhhBB4kJEDALA2M4alWfHHxyAi+SxcuBBubm6YP38+FAoF6tSpg7t37+KLL77A1KlTkZGRgdWrV2P9+vV48803AQArV66Ei4tLoduMi4tDlSpV0LFjR5iamqJ69epo0aIFAKBSpUowNjaGjY0NqlSpUug2ZsyYgX79+ql9qTZu3Dhf9uXLlyM3Nxc5OTkwMjLCggULADz94pk9ezb++ecf+Pn5AQBq1KiBI0eOYMmSJWjbti0WL16M2rVrS30qateujQsXLmDGjBlq+8nNzcXChQul/cfExGDDhg24ffu2dBw+//xz7N69GytXrsTMmTMRFxeHd999Fw0bNpT2/eLxadq0KXx9fQFArSB72es+n2fPXWrUqJHUz6NWrVqYP38+/v7770KLm5s3bxb4GV67dg0nTpxAeHg4AOCDDz7A2LFjERwcrPEznq5du1akQqIgr+tQbWlpWeiyxMTEAi9xOjk5ITExsUj7X716NWxsbPDOO+/kW7Zw4UJMnDgRGRkZqFOnDvbt2wczMzO1dVxdXREZGVmkfWlL1uJm06ZNGDduHBYuXIjWrVtjyZIl6NatG6Kjo1G9evV868fGxiIgIAAjR47Er7/+iqNHj2L06NFwdHRUu+4nl10Xnv/D+PuzdvIFISoFLE2NEf1NF9n2rQuXLl2Cn5+f2oPFWrdujcePH+P27dt4+PAhcnNzpeIEAOzs7F75pfXee+8hNDQUNWrUQNeuXREQEIC33npLo34kUVFRrzyzAwADBgzAlClTkJaWhh9++AG2trbS78no6GhkZWXl+3LPyclB06ZNAQBXrlxB8+bN1Za/+D6fMTMzQ6NGjaT5M2fOQAgBb29vtfWys7NRuXJlAMDYsWPx0UcfYe/evejYsSPeffddaRsfffQR3n33XZw5cwadO3fG22+/DX9//wLf4+s+n2ffIy/mA4CqVasiKSmpkCMHPHnyBBYWFvnaw8LC0KVLFzg4OAAAAgICMHz4cPz111/o3LlzodsriBBC6wfW1axZU6vXPVPQfjXJs2LFCgwYMKDAYzRgwAB06tQJCQkJ+Omnn9CnTx8cPXpUbV1LS0u9j0Ena3Eze/ZsDB8+XOq0FRoaij179mDRokUICQnJt/7ixYtRvXp16U6CunXr4vTp0/jpp59kL26UKoElh25I81Xs8n/oROWJQqHQyaUhORX0C1+Ipw/nVCgUatMFrVMQNzc3XLlyBfv27cNff/2F0aNH48cff8TBgwdhampapFyv+sv8GTs7O+lL8Ndff0X9+vURFhaG4cOHS51+d+zYAVdXV7XXmZubS++hKO/L0tJSbT2VSgVjY2NERETke3z+s8spI0aMQJcuXbBjxw7s3bsXISEh+Pnnn/HJJ5+gW7duuHXrFnbs2IG//voLb775JsaMGYOffvop375f9/k88/JxVSgUr+z47ODggPPnz6u1KZVKrFmzBomJiWqFqFKpRFhYmFTc2Nra4tatW/m2+exupGeXm7y9vXHp0qVCM7xKcS5LValSBffu3cvXfv/+fTg7O79234cPH8aVK1ewadOmApc/uwOrVq1aaNWqFSpWrIitW7fi/fffl9Z58OABHB0dX7uv4pDtVp6cnBxERETkq3Y7d+6MY8eOFfia48eP51u/S5cuOH36NHJzcwt8TXZ2NtLS0tR+9CElIxtn4x8BAD5olf+sExGVPfXq1cOxY8fUvtSPHTsGGxsbuLq6wsvLC6ampjh58qS0PC0tDdeuXXvldi0tLdGjRw/MmzcPBw4cwPHjx6UvUzMzM7W7SwrSqFEj/P3330V+H6ampvjyyy/x1VdfITMzE/Xq1YO5uTni4uJQs2ZNtZ9n/SPq1KmDU6dOqW3n9OnTr91X06ZNoVQqkZSUlG/bL15qc3NzQ2BgIMLDw/HZZ59h2bJl0jJHR0cMGTIEv/76K0JDQ7F06dIC9/W6z0dbTZs2xeXLl9W2u3PnTqSnpyMyMhJRUVHSz+bNm7Ft2zakpKQAeHrcLly4gKysLLVtnjp1Co6OjqhYsSKAp3csXb16Fdu3b8+3fyEEUlNTC8334v4L+lm+fHmhr/Xz80Nqaqrav9l///0XqamphZ4he1FYWBh8fHzyXQYtjBAi3yMILly4IJ0h1BfZipvk5GQolcp8laKzs3Oh1/0SExMLXD8vLw/JyckFviYkJESqJO3s7PJ1bNIlc5Onh3NYa3YkJipLUlNT831BxMXFYfTo0YiPj8cnn3yCy5cvY/v27QgODkZQUBCMjIxgY2ODwYMHY8KECdi/fz8uXryIYcOGwcjIqNBT/KtWrUJYWBguXLiAGzduYO3atbC0tIS7uzuAp31MDh06hDt37hT6ey04OBgbNmxAcHAwLl26hPPnz2PWrFmvfI/9+/eHQqHAwoULYWNjg88//xzjx4/H6tWrERMTg8jISCxYsEDqJPrhhx/i8uXL+OKLL3D16lX89ttvWLVqFYBXj//j7e2NAQMGYNCgQQgPD0dsbCxOnTqFH374ATt37gQAjBs3Dnv27EFsbCzOnDmDf/75B3Xr1gUATJ06Fdu3b8f169dx8eJF/O9//5OWvex1n4+22rdvj4yMDFy8+PyxHmFhYejevTsaN26MBg0aSD/vvvsuHB0d8euvvwJ4elnGxMQEAwcOxOnTpxETE4Nff/0VISEhmDBhgrS9Pn36oG/fvnj//fcREhKC06dP49atW/jf//6Hjh07Yv/+/YXme7lofPnnVYVd3bp10bVrV4wcORInTpzAiRMnMHLkSPz3v/9Vu5xap04dbN26Ve21aWlp2Lx5c4G3yN+4cQMhISGIiIhAXFwcjh8/jj59+sDS0hIBAQHSepmZmQWe2NC5195PpSd37twRAMSxY8fU2r/77jtRu3btAl9Tq1YtMXPmTLW2I0eOCAAiISGhwNdkZWWJ1NRU6Sc+Pl6vt4ITlUevun2ztBs8eHC+268BiMGDBwshtLsVvEWLFmLSpEnSOi/eCr5161bRsmVLYWtrK6ytrUWrVq3EX3/9Ja17/Phx0ahRI2Fubv7KW8G3bNkimjRpIszMzISDg4N45513pGUF3QouhBAzZswQjo6OIj09XahUKjF37lxRu3ZtYWpqKhwdHUWXLl3EwYMHpfWf3Qpubm4u2rVrJxYtWiQASJ9zQbmEECInJ0dMnTpVeHh4CFNTU1GlShXRq1cvce7cOSGEEB9//LHw8vIS5ubmwtHRUQwcOFAkJycLIYT49ttvRd26dYWlpaWoVKmS6Nmzp7hx44YQQvtbwV8+Fj179pQ+38L069dP+gwTExOFiYmJ+O233wpc95NPPhENGzaU5q9duybeffdd4erqKqytrUXDhg3F/PnzhVKpVHudUqkUixYtEs2bNxdWVlbC1tZW+Pj4iLlz54rMzMxX5iuOlJQUMWDAAGFjYyNsbGzEgAEDpNv7nwEgVq5cqda2ZMkSYWlpWeBt6nfu3BHdunUTTk5OwtTUVFSrVk30798/3+3l69evL/Q7Xgjd3QouW3GTnZ0tjI2NRXh4uFr72LFjxRtvvFHga9q0aSPGjh2r1hYeHi5MTExETk5Okfar7+fcEJVHZbm40bXHjx8LOzs7sXz5crmj6Nx3330nqlWrJneMEnHu3Dnh5OQk0tLS5I5iUJo3by7WrVtX6HJdFTeyXZYyMzODj48P9u3bp9a+b9++Qq/7+fn55Vt/79698PX1LXJHPCIiXYqMjMSGDRsQExODM2fOYMCAAQBQ4PNLypqFCxfi1KlT0uWzH3/8EYMHD5Y7Volo2LAhZs2apTZcAhVPUlISevfurda5WG+KVYIV08aNG4WpqakICwsT0dHRYty4ccLa2lrcvHlTCCHEpEmTxMCBA6X1b9y4IaysrMT48eNFdHS0CAsLE6ampuL3338v8j555oZI98rzmZszZ86IZs2aCWtra1GxYkXRsWNH6fJLWTdu3DhRtWpVYW5uLmrVqiW++eYbtUs+RLpmEE8o7tu3L1JSUvDNN99Ig6Ht3LlT6liXkJCAuLg4aX1PT0/s3LkT48ePx4IFC+Di4oJ58+bJfhs4EZVfTZs2RUREhNwx9GLOnDmYM2eO3DGINKYQ4hUPZDBAaWlpsLOzQ2pqaoGP1iYizWVlZSE2Nhaenp4FPtiLiKgoXvW7RJPvbw5ZTUQ6U87+ViIiHdPV7xAWN0RUbM869Ov7kepEZNhycp6Oz/jy0601VbafjU5EpYKxsTHs7e2l8XqsrKy0HjeHiMonlUqF+/fvw8rKSqOx1grC4oaIdOLZo/VfNSAhEdGrGBkZoXr16sX+44jFDRHphEKhQNWqVeHk5FToWG9ERK9iZmZWrKEznmFxQ0Q6ZWxsXOzr5URExcEOxURERGRQWNwQERGRQWFxQ0RERAal3PW5efaAoLS0NJmTEBERUVE9+94uyoP+yl1xk56eDgBwc3OTOQkRERFpKj09HXZ2dq9cp9yNLaVSqXD37l3Y2Njo/CFjaWlpcHNzQ3x8PMet0iMe55LB41wyeJxLDo91ydDXcRZCID09HS4uLq+9XbzcnbkxMjJCtWrV9LoPW1tb/o9TAnicSwaPc8ngcS45PNYlQx/H+XVnbJ5hh2IiIiIyKCxuiIiIyKCwuNEhc3NzBAcHw9zcXO4oBo3HuWTwOJcMHueSw2NdMkrDcS53HYqJiIjIsPHMDRERERkUFjdERERkUFjcEBERkUFhcUNEREQGhcWNhhYuXAhPT09YWFjAx8cHhw8ffuX6Bw8ehI+PDywsLFCjRg0sXry4hJKWbZoc5/DwcHTq1AmOjo6wtbWFn58f9uzZU4Jpyy5N/z0/c/ToUZiYmKBJkyb6DWggND3O2dnZmDJlCtzd3WFubg4vLy+sWLGihNKWXZoe53Xr1qFx48awsrJC1apVMXToUKSkpJRQ2rLp0KFDeOutt+Di4gKFQoFt27a99jWyfA8KKrKNGzcKU1NTsWzZMhEdHS0+/fRTYW1tLW7dulXg+jdu3BBWVlbi008/FdHR0WLZsmXC1NRU/P777yWcvGzR9Dh/+umn4ocffhAnT54UV69eFZMnTxampqbizJkzJZy8bNH0OD/z6NEjUaNGDdG5c2fRuHHjkglbhmlznHv06CFatmwp9u3bJ2JjY8W///4rjh49WoKpyx5Nj/Phw4eFkZGRmDt3rrhx44Y4fPiwqF+/vnj77bdLOHnZsnPnTjFlyhSxZcsWAUBs3br1levL9T3I4kYDLVq0EIGBgWptderUEZMmTSpw/YkTJ4o6deqotX344YeiVatWestoCDQ9zgWpV6+emD59uq6jGRRtj3Pfvn3FV199JYKDg1ncFIGmx3nXrl3Czs5OpKSklEQ8g6Hpcf7xxx9FjRo11NrmzZsnqlWrpreMhqYoxY1c34O8LFVEOTk5iIiIQOfOndXaO3fujGPHjhX4muPHj+dbv0uXLjh9+jRyc3P1lrUs0+Y4v0ylUiE9PR2VKlXSR0SDoO1xXrlyJWJiYhAcHKzviAZBm+P8xx9/wNfXF7NmzYKrqyu8vb3x+eef48mTJyURuUzS5jj7+/vj9u3b2LlzJ4QQuHfvHn7//Xd07969JCKXG3J9D5a7gTO1lZycDKVSCWdnZ7V2Z2dnJCYmFviaxMTEAtfPy8tDcnIyqlatqre8ZZU2x/llP//8MzIyMtCnTx99RDQI2hzna9euYdKkSTh8+DBMTPiroyi0Oc43btzAkSNHYGFhga1btyI5ORmjR4/GgwcP2O+mENocZ39/f6xbtw59+/ZFVlYW8vLy0KNHD/zyyy8lEbnckOt7kGduNKRQKNTmhRD52l63fkHtpE7T4/zMhg0bMG3aNGzatAlOTk76imcwinqclUol+vfvj+nTp8Pb27uk4hkMTf49q1QqKBQKrFu3Di1atEBAQABmz56NVatW8ezNa2hynKOjozF27FhMnToVERER2L17N2JjYxEYGFgSUcsVOb4H+edXETk4OMDY2DjfXwFJSUn5qtJnqlSpUuD6JiYmqFy5st6ylmXaHOdnNm3ahOHDh2Pz5s3o2LGjPmOWeZoe5/T0dJw+fRqRkZH4+OOPATz9EhZCwMTEBHv37kWHDh1KJHtZos2/56pVq8LV1RV2dnZSW926dSGEwO3bt1GrVi29Zi6LtDnOISEhaN26NSZMmAAAaNSoEaytrdGmTRt89913PLOuI3J9D/LMTRGZmZnBx8cH+/btU2vft28f/P39C3yNn59fvvX37t0LX19fmJqa6i1rWabNcQaenrEZMmQI1q9fz2vmRaDpcba1tcX58+cRFRUl/QQGBqJ27dqIiopCy5YtSyp6maLNv+fWrVvj7t27ePz4sdR29epVGBkZoVq1anrNW1Zpc5wzMzNhZKT+FWhsbAzg+ZkFKj7Zvgf12l3ZwDy71TAsLExER0eLcePGCWtra3Hz5k0hhBCTJk0SAwcOlNZ/dgvc+PHjRXR0tAgLC+Ot4EWg6XFev369MDExEQsWLBAJCQnSz6NHj+R6C2WCpsf5Zbxbqmg0Pc7p6emiWrVqonfv3uLixYvi4MGDolatWmLEiBFyvYUyQdPjvHLlSmFiYiIWLlwoYmJixJEjR4Svr69o0aKFXG+hTEhPTxeRkZEiMjJSABCzZ88WkZGR0i33peV7kMWNhhYsWCDc3d2FmZmZaNasmTh48KC0bPDgwaJt27Zq6x84cEA0bdpUmJmZCQ8PD7Fo0aISTlw2aXKc27ZtKwDk+xk8eHDJBy9jNP33/CIWN0Wn6XG+dOmS6Nixo7C0tBTVqlUTQUFBIjMzs4RTlz2aHud58+aJevXqCUtLS1G1alUxYMAAcfv27RJOXbbs37//lb9vS8v3oEIInn8jIiIiw8E+N0RERGRQWNwQERGRQWFxQ0RERAaFxQ0REREZFBY3REREZFBY3BAREZFBYXFDREREBoXFDRERERkUFjdUrqxatQr29vZyx9Cah4cHQkNDX7nOtGnT0KRJkxLJU9r8888/qFOnDlQqVYnsr7R8HtrsQ6FQYNu2bcXa75AhQ/D2228XaxslISkpCY6Ojrhz547cUaiEsLihMmfIkCFQKBT5fq5fvy53NKxatUotU9WqVdGnTx/ExsbqZPunTp3CqFGjpPmCvqA+//xz/P333zrZX2Fefp/Ozs546623cPHiRY23o8tic+LEiZgyZYo0IGJ5+TzKioL+v33xZ8iQIXrZr5OTEwYOHIjg4GC9bJ9KHxY3VCZ17doVCQkJaj+enp5yxwLwdATthIQE3L17F+vXr0dUVBR69OgBpVJZ7G07OjrCysrqletUqFABlStXLva+XufF97ljxw5kZGSge/fuyMnJ0fu+C3Ls2DFcu3YN7733XqE5DfnzKAte/P81NDRU+mye/cydO1dt/dzcXJ3te+jQoVi3bh0ePnyos21S6cXihsokc3NzVKlSRe3H2NgYs2fPRsOGDWFtbQ03NzeMHj0ajx8/LnQ7Z8+eRfv27WFjYwNbW1v4+Pjg9OnT0vJjx47hjTfegKWlJdzc3DB27FhkZGS8MptCoUCVKlVQtWpVtG/fHsHBwbhw4YJ0ZmnRokXw8vKCmZkZateujbVr16q9ftq0aahevTrMzc3h4uKCsWPHSstevAzi4eEBAOjVqxcUCoU0/+Ilij179sDCwgKPHj1S28fYsWPRtm1bnb1PX19fjB8/Hrdu3cKVK1ekdV71eRw4cABDhw5Famqq9Jf7tGnTAAA5OTmYOHEiXF1dYW1tjZYtW+LAgQOvzLNx40Z07twZFhYWheY05M/jRadOnUKnTp3g4OAAOzs7tG3bFmfOnMm3XkJCArp16wZLS0t4enpi8+bNasvv3LmDvn37omLFiqhcuTJ69uyJmzdvFjnHy178/9XOzk76bKpUqYKsrCzY29vjt99+Q7t27WBhYYFff/21wEtuoaGh0vF9ZuXKlahbty4sLCxQp04dLFy4UG15w4YNUaVKFWzdulXr/FR2sLghg2JkZIR58+bhwoULWL16Nf755x9MnDix0PUHDBiAatWq4dSpU4iIiMCkSZNgamoKADh//jy6dOmCd955B+fOncOmTZtw5MgRfPzxxxplsrS0BPD0r9CtW7fi008/xWeffYYLFy7gww8/xNChQ7F//34AwO+//445c+ZgyZIluHbtGrZt24aGDRsWuN1Tp04BePpLPSEhQZp/UceOHWFvb48tW7ZIbUqlEr/99hsGDBigs/f56NEjrF+/HgCk4we8+vPw9/fP99f7559/DuDpX9lHjx7Fxo0bce7cObz33nvo2rUrrl27VmiGQ4cOwdfX97VZy8PnkZ6ejsGDB+Pw4cM4ceIEatWqhYCAAKSnp6ut9/XXX+Pdd9/F2bNn8cEHH+D999/HpUuXAACZmZlo3749KlSogEOHDuHIkSOoUKECunbtWujZuWeXAYvjiy++wNixY3Hp0iV06dKlSK9ZtmwZpkyZghkzZuDSpUuYOXMmvv76a6xevVptvRYtWuDw4cPFykdlhN7HHSfSscGDBwtjY2NhbW0t/fTu3bvAdX/77TdRuXJlaX7lypXCzs5OmrexsRGrVq0q8LUDBw4Uo0aNUms7fPiwMDIyEk+ePCnwNS9vPz4+XrRq1UpUq1ZNZGdnC39/fzFy5Ei117z33nsiICBACCHEzz//LLy9vUVOTk6B23d3dxdz5syR5gGIrVu3qq0THBwsGjduLM2PHTtWdOjQQZrfs2ePMDMzEw8ePCjW+wQgrK2thZWVlQAgAIgePXoUuP4zr/s8hBDi+vXrQqFQiDt37qi1v/nmm2Ly5MmFbtvOzk6sWbMmX87y8Hm8vI+X5eXlCRsbG/Hnn3+qZQ0MDFRbr2XLluKjjz4SQggRFhYmateuLVQqlbQ8OztbWFpaij179gghnv6/2LNnT2l5eHi4qF27dqE5XvTyZxMbGysAiNDQ0Ne+tzlz5gh3d3dp3s3NTaxfv15tnW+//Vb4+fmptY0fP160a9euSPmobOOZGyqT2rdvj6ioKOln3rx5AID9+/ejU6dOcHV1hY2NDQYNGoSUlJRCT+kHBQVhxIgR6NixI77//nvExMRIyyIiIrBq1SpUqFBB+unSpQtUKtUrO6SmpqaiQoUK0qWYnJwchIeHw8zMDJcuXULr1q3V1m/durX01/J7772HJ0+eoEaNGhg5ciS2bt2KvLy8Yh2rAQMG4MCBA7h79y4AYN26dQgICEDFihWL9T5tbGwQFRWFiIgILF68GF5eXli8eLHaOpp+HgBw5swZCCHg7e2tlungwYNqn8/Lnjx5ku+SFFB+Po8XJSUlITAwEN7e3rCzs4OdnR0eP36MuLg4tfX8/PzyzT977xEREbh+/TpsbGykHJUqVUJWVlahn0OvXr1w+fJljY7Hy4py9u1F9+/fR3x8PIYPH652zL777rt8OS0tLZGZmVmsfFQ2mMgdgEgb1tbWqFmzplrbrVu3EBAQgMDAQHz77beoVKkSjhw5guHDhxfaMXHatGno378/duzYgV27diE4OBgbN25Er169oFKp8OGHH6r1sXimevXqhWazsbHBmTNnYGRkBGdnZ1hbW6stf/m0vRBCanNzc8OVK1ewb98+/PXXXxg9ejR+/PFHHDx4UO1yjyZatGgBLy8vbNy4ER999BG2bt2KlStXSsu1fZ9GRkbSZ1CnTh0kJiaib9++OHToEADtPo9neYyNjREREQFjY2O1ZRUqVCj0dQ4ODgV2Fi0vn8eLhgwZgvv37yM0NBTu7u4wNzeHn59fkTp7P3vvKpUKPj4+WLduXb51HB0di5RDGy9/PkZGRhBCqLW9+O/n2W3/y5YtQ8uWLdXWe/nfz4MHD/SanUoPFjdkME6fPo28vDz8/PPP0q3Av/3222tf5+3tDW9vb4wfPx7vv/8+Vq5ciV69eqFZs2a4ePFiviLqdV780n9Z3bp1ceTIEQwaNEhqO3bsGOrWrSvNW1paokePHujRowfGjBmDOnXq4Pz582jWrFm+7Zmamhbprp/+/ftj3bp1qFatGoyMjNC9e3dpmbbv82Xjx4/H7NmzsXXrVvTq1atIn4eZmVm+/E2bNoVSqURSUhLatGlT5P03bdoU0dHR+drL4+dx+PBhLFy4EAEBAQCA+Ph4JCcn51vvxIkTau/9xIkTaNq0qZRj06ZNcHJygq2trdZZisvR0RGJiYlqRWdUVJS03NnZGa6urrhx44bUb6kwFy5cQLt27fSYlkoLXpYig+Hl5YW8vDz88ssvuHHjBtauXZvvMsmLnjx5go8//hgHDhzArVu3cPToUZw6dUr6Yvviiy9w/PhxjBkzBlFRUbh27Rr++OMPfPLJJ1pnnDBhAlatWoXFixfj2rVrmD17NsLDw6WOtKtWrUJYWBguXLggvQdLS0u4u7sXuD0PDw/8/fffSExMfOUtrgMGDMCZM2cwY8YM9O7dW+3yja7ep62tLUaMGIHg4GAIIYr0eXh4eODx48f4+++/kZycjMzMTHh7e2PAgAEYNGgQwsPDERsbi1OnTuGHH37Azp07C91/ly5dcOTIEY0yG+rnUbNmTaxduxaXLl3Cv//+iwEDBkgdqV+0efNmrFixAlevXkVwcDBOnjwpdVweMGAAHBwc0LNnTxw+fBixsbE4ePAgPv30U9y+fbvA/W7duhV16tQpcs6iaNeuHe7fv49Zs2YhJiYGCxYswK5du9TWmTZtGkJCQjB37lxcvXoV58+fx8qVKzF79mxpnczMTERERKBz5846zUellJwdfoi08XInxhfNnj1bVK1aVVhaWoouXbqINWvWCADi4cOHQgj1TozZ2dmiX79+ws3NTZiZmQkXFxfx8ccfq3XaPHnypOjUqZOoUKGCsLa2Fo0aNRIzZswoNFtBHWRftnDhQlGjRg1hamoqvL291TrBbt26VbRs2VLY2toKa2tr0apVK/HXX39Jy1/uwPrHH3+ImjVrChMTE6mDZWGdS5s3by4AiH/++SffMl29z1u3bgkTExOxadMmIcTrPw8hhAgMDBSVK1cWAERwcLAQQoicnBwxdepU4eHhIUxNTUWVKlVEr169xLlz5wrN9ODBA2FpaSkuX7782pwvMoTP4+V9nDlzRvj6+gpzc3NRq1YtsXnz5gI7Py9YsEB06tRJmJubC3d3d7Fhwwa17SYkJIhBgwYJBwcHYW5uLmrUqCFGjhwpUlNThRD5/1981tG8KArrUBwZGZlv3UWLFgk3NzdhbW0tBg0aJGbMmKHWoVgIIdatWyeaNGkizMzMRMWKFcUbb7whwsPDpeXr168vcmdnKvsUQrx0MZOIqIyaOHEiUlNTsWTJErmjUCnTokULjBs3Dv3795c7CpUAXpYiIoMxZcoUuLu76+Tpw2Q4kpKS0Lt3b7z//vtyR6ESwjM3REREZFB45oaIiIgMCosbIiIiMigsboiIiMigsLghIiIig8LihoiIiAwKixsiIiIyKCxuiIiIyKCwuCEiIiKDwuKGiIiIDMr/Afs2k9XztAsqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "roc_display = RocCurveDisplay.from_estimator(glm_fit_1d, X_1b_test, y_1b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fd071-e433-4b7d-82a6-f7ae9ac67ba0",
   "metadata": {},
   "source": [
    "Unfortunately, the plotted ROC curve does not indicate the threshold value leading to a particular point on the curve. Fortunately, the information contained in a ROC curve can be obtained using the `roc_curve()` function in the sklearn metrics module.\n",
    "\n",
    "The inputs to  `roc_curve()` are true outputs and the predicted probabilities for class 1. Use the corresponding series from Part 1 of this assignment as function inputs and save the result as `roc_fpr_2d`, `roc_tpr_2d` and `roc_thresholds_2d`.\n",
    "\n",
    "Next, find the the threshold that balances class-specific performance best. You could do that by finding the index at which TPR and (1-FPR) are as close are possible. Then you simply return the value of `roc_thresholds_2d` at this index. State your chosen threshold in the  variable `opt_threshold_2d`. Round to two decimals, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c7cd9f1e-9478-4064-a260-92ede98f9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15956585056346786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# 1.\n",
    "roc_fpr_2d, roc_tpr_2d, roc_thresholds_2d = roc_curve(y_1b_test, glm_prob_1e)\n",
    "\n",
    "# 2.\n",
    "opt_threshold_2d = roc_thresholds_2d[np.argmax(roc_tpr_2d - roc_fpr_2d)]\n",
    "\n",
    "print(opt_threshold_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3741fe7-a150-4ad2-b775-96e28809e13a",
   "metadata": {},
   "source": [
    "### Task 2e)\n",
    "To what extend does our chosen threshold from Task 2d compromise overall accuracy? To find this out, use your `performancemetrics()` function from Task 2b and save the output that you get with the threshold chosen in Task 2d as `metrics_2e`.\n",
    "\n",
    "Then, use the string variable `is_accuracy_compromised_2e` to discuss whether overall accuracy is noteably affected if we switch from a threshold probability of 50% to a value that balances class-specific prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f00e8b82-ac50-40fa-a2ff-0087412ff7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix':       TN    FN\n",
      "FP  2965  2249\n",
      "TP   222   729, 'FPR': 0.4313387034906022, 'TPR': 0.7665615141955836, 'classification_error': 0.4008110300081103}\n"
     ]
    }
   ],
   "source": [
    "metrics_2e = performancemetrics(y_1b_test, glm_prob_1e, opt_threshold_2d)\n",
    "print(metrics_2e)\n",
    "\n",
    "is_accuracy_compromised_2e = \"??\" #Comment on the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d68ce5-28c7-472b-a1e3-07f1dc5dfc79",
   "metadata": {},
   "source": [
    "## Part 3: Multiclass logistic regression with `sklearn`\n",
    "\n",
    "We can use the `LogisticRegression` function from `sklearn` to learn models whose output variable has more than two categories. The data we are using to learn a multiclass logistic regression is drug consumption data. Our response variable is usage of drugs (Cocaine , Crack, Ecstasy, and Heroin) and we have three possible responses, \"never used\", \"used more than a year ago\", and \"used within a year\". As explanatory variables we have personality test data, demographic data, and consumption of chocolate, alcohol, and nicotine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c0298-afb6-4b7d-b32d-15610d67931f",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "Conduct the following steps to prepare your data:\n",
    "\n",
    "1. Load `drug_train.RDS` using the `read_r()` function in the `pyreadr` package and save it as a data frame `drug_data`. \n",
    "2. Extract the variable *drugs.usage* into a Pandas series called `y_3a`\n",
    "3. Create a Pandas dataframe `X_3a` containing all other variables in `drug_data`.  Note that you will need to transform categorical variables into indicator variables, e.g. using the `get_dummies()` function in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "27776fed-bcd9-4b01-8ba2-ec14d9d4b846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chocolate.consumption</th>\n",
       "      <th>Alcohol.consumption</th>\n",
       "      <th>Nicotine.consumption</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Neuroticism</th>\n",
       "      <th>Extraversion</th>\n",
       "      <th>Openness.to.experience</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Impulsiveness</th>\n",
       "      <th>Sensation.seeking</th>\n",
       "      <th>Fictitious.drug.Semeron.consumption</th>\n",
       "      <th>drugs.usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>White</td>\n",
       "      <td>0.13606</td>\n",
       "      <td>-0.94779</td>\n",
       "      <td>1.06238</td>\n",
       "      <td>-1.92595</td>\n",
       "      <td>0.75830</td>\n",
       "      <td>-0.21712</td>\n",
       "      <td>-0.52593</td>\n",
       "      <td>never used</td>\n",
       "      <td>never used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>White</td>\n",
       "      <td>0.62967</td>\n",
       "      <td>0.63779</td>\n",
       "      <td>2.90161</td>\n",
       "      <td>-1.07533</td>\n",
       "      <td>0.93949</td>\n",
       "      <td>0.88113</td>\n",
       "      <td>1.22470</td>\n",
       "      <td>never used</td>\n",
       "      <td>more than a year ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.61113</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>White</td>\n",
       "      <td>-1.32828</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>-1.27553</td>\n",
       "      <td>-0.60633</td>\n",
       "      <td>0.12331</td>\n",
       "      <td>0.88113</td>\n",
       "      <td>0.76540</td>\n",
       "      <td>never used</td>\n",
       "      <td>within a year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>never used</td>\n",
       "      <td>1.09449</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.79151</td>\n",
       "      <td>0.96248</td>\n",
       "      <td>-1.11902</td>\n",
       "      <td>0.94156</td>\n",
       "      <td>0.41594</td>\n",
       "      <td>0.19268</td>\n",
       "      <td>0.40148</td>\n",
       "      <td>never used</td>\n",
       "      <td>never used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>more than a year ago</td>\n",
       "      <td>never used</td>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.45468</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>1.06238</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>2.04506</td>\n",
       "      <td>-0.71126</td>\n",
       "      <td>-0.52593</td>\n",
       "      <td>never used</td>\n",
       "      <td>never used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Chocolate.consumption   Alcohol.consumption  Nicotine.consumption      Age  \\\n",
       "0  more than a year ago  more than a year ago  more than a year ago -0.95197   \n",
       "1  more than a year ago  more than a year ago  more than a year ago -0.95197   \n",
       "2  more than a year ago  more than a year ago  more than a year ago  0.49788   \n",
       "3  more than a year ago  more than a year ago            never used  1.09449   \n",
       "4  more than a year ago  more than a year ago            never used -0.95197   \n",
       "\n",
       "    Gender  Education  Country Ethnicity  Neuroticism  Extraversion  \\\n",
       "0 -0.48246    0.45468 -0.57009     White      0.13606      -0.94779   \n",
       "1  0.48246   -0.61113 -0.57009     White      0.62967       0.63779   \n",
       "2 -0.48246   -0.61113  0.96082     White     -1.32828       0.00332   \n",
       "3  0.48246    1.16365  0.96082     White     -0.79151       0.96248   \n",
       "4  0.48246    0.45468  0.96082     White     -0.24649       0.00332   \n",
       "\n",
       "   Openness.to.experience  Agreeableness  Conscientiousness  Impulsiveness  \\\n",
       "0                 1.06238       -1.92595            0.75830       -0.21712   \n",
       "1                 2.90161       -1.07533            0.93949        0.88113   \n",
       "2                -1.27553       -0.60633            0.12331        0.88113   \n",
       "3                -1.11902        0.94156            0.41594        0.19268   \n",
       "4                 1.06238        0.59042            2.04506       -0.71126   \n",
       "\n",
       "   Sensation.seeking Fictitious.drug.Semeron.consumption           drugs.usage  \n",
       "0           -0.52593                          never used            never used  \n",
       "1            1.22470                          never used  more than a year ago  \n",
       "2            0.76540                          never used         within a year  \n",
       "3            0.40148                          never used            never used  \n",
       "4           -0.52593                          never used            never used  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "drug_data = prdr.read_r('drug_train.RDS')\n",
    "drug_data = drug_data[None]\n",
    "\n",
    "# 2.\n",
    "y_3a = drug_data['drugs.usage']\n",
    "\n",
    "# 3.\n",
    "X_3a = pd.get_dummies(drug_data.drop(columns = ['drugs.usage']))\n",
    "\n",
    "drug_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f2eb6-7279-431a-8341-a052389e69d5",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Specify and learn fit a multinomial logistic regression using `LogisticRegression()`. In the specification, choose no penalty and make sure an intercept is added. Additionally, set the `multi_class` argument to `'multinomial'`. Lastly, increase the number of iterations if this is needed for the learning algorithm to converge.\n",
    "\n",
    "Save the resulting object as `mlogit_fit_3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d1be9547-16aa-4004-9282-c14e267f3909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, penalty=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, penalty=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial', penalty=None)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlogit_fit_3b = LogisticRegression(penalty = None,fit_intercept = True, multi_class='multinomial', max_iter=1000)\n",
    "\n",
    "mlogit_fit_3b.fit(X_3a, y_3a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430dd66-b606-40fd-abae-a24c19ef96f8",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "We now skip the entire data splitting step that we discussed in Part 1. Instead, we say that model `mlogit_fit_3b` is our result of the entire modeling process and it is implemented in practice. We now get new test data for which we obtain output predictions. To prepare this, conduct the following steps:\n",
    "\n",
    "1. Load *drug_test.RDS* as object `drug_data_test`.\n",
    "2. Create an input variable matrix `X_3c_test` in the same way as in Task 3a, but using `drug_data_test` instead of `drug_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "333d2f7d-b881-42f2-a9d5-090762b3f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_data_test = prdr.read_r('drug_test.RDS')\n",
    "drug_data_test = drug_data_test[None]\n",
    "\n",
    "X_3c_test = pd.get_dummies(drug_data_test.drop(columns = ['drugs.usage']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f1abd-e837-4994-bd43-a3d696534fb3",
   "metadata": {},
   "source": [
    "### Task 3d)\n",
    "\n",
    "Get predictions from model `mlogit_fit_3b` for your test data using the `predict_proba()` and `predict()` methods. Save your predictions as `mlogit_prob_3c` and `mlogit_pred_3c`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b95ef853-029d-4121-ae35-a576c33a219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlogit_prob_3c = mlogit_fit_3b.predict_proba(X_3c_test)\n",
    "mlogit_pred_3c = mlogit_fit_3b.predict(X_3c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fcdca-e2af-426c-b9fe-c7b266f9a02b",
   "metadata": {},
   "source": [
    "As time passes, you also observe outputs for your first batch of test data. This allows you to evaluate the accuracy of your prediction model during deployment. The variable *drugs.usage* in `drug.data.test` contains these test outputs. Extract them into a vector `ytest_3d`.\n",
    "\n",
    "Then, use the `confusion_matrix` function in sklearn to get a confusion matrix. Save it as `confumat_3d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "be1acef3-5c16-49bd-aa10-0064c0ee2be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>more than a year ago</th>\n",
       "      <th>never used</th>\n",
       "      <th>within a year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>more than a year ago</th>\n",
       "      <td>131</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never used</th>\n",
       "      <td>43</td>\n",
       "      <td>186</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>within a year</th>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      more than a year ago  never used  within a year\n",
       "more than a year ago                   131          50             17\n",
       "never used                              43         186             22\n",
       "within a year                           34          48             34"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_3d    = drug_data_test['drugs.usage']\n",
    "confumat_3d = confusion_matrix(ytest_3d, mlogit_pred_3c)\n",
    "\n",
    "class_names = sorted(ytest_3d.unique())  \n",
    "\n",
    "confumat_df = pd.DataFrame(confumat_3d, columns=class_names, index=class_names)\n",
    "\n",
    "confumat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b795bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates along the direction vector: [2.21034343 4.42068686 4.18154055]\n",
      "Reconstructed x1: [1.27614237 2.55228475 2.41421356]\n",
      "Reconstructed x2: [1.80473785 3.60947571 3.41421356]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Direction vector\n",
    "direction = np.array([np.sqrt(1/3), np.sqrt(2/3)])\n",
    "\n",
    "# Datapoints\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = np.array([2, 4, 3])\n",
    "\n",
    "# Calculating the coordinates\n",
    "coordinates = np.dot(np.vstack((x1, x2)).T, direction)\n",
    "\n",
    "# Conversion to the position on the chosen direction in terms of the original variables x_1 and x_2\n",
    "reconstructed_x1 = coordinates * direction[0]\n",
    "reconstructed_x2 = coordinates * direction[1]\n",
    "\n",
    "print(\"Coordinates along the direction vector:\", coordinates)\n",
    "print(\"Reconstructed x1:\", reconstructed_x1)\n",
    "print(\"Reconstructed x2:\", reconstructed_x2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
